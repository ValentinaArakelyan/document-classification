paper_1	 Through Social media, people are able to write short messages on their walls to express their sentiments using various social media like Twitter and Facebook. These numerous amounts of information can be extracted, processed and properly utilized in areas like marketing and electronic learning. This paper reports on the successful development of a way of searching, filtering, organizing and storing the information from social media so that it can be put to some good use in an electronic learning environment. This helps in solving the problem of losing vital information that is generated from the social media. It addresses this limitation by using the data from twitter to cluster students and by so doing support group electronic learning. Good clustering methods produce high quality clusters with either a high intra-class similarity within clusters or a low inter-class similarity between clusters. In incremental prototyping, the whole requirements are broken down into building blocks which are incremented each time a new component is integrated based on an overall design solution. The solution is complete when all the components are in place. The second task involved identifying the right kind of data to use for training the expected prototype as well as testing it. The third step involved using the data already preprocessed above to train the prototype. From this, we identified the groups that students' fall that were turned into discussion groups. The prototype was then subjected to testing using the test data. They are part of the data that was used to train the system but its results are already known. The first task was to determine if the prototype achieved its overall goal which is grouping students through social media for electronic learning. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . This is summarized in the chart below. A total of 200 tweets were extracted and used for this test which was summarized in a confusion matrix. This matrix consists of the following parameters: TP, TN, FP and FN, which are defined below. The results below illustrate a summary of what was obtained when 200 tweets were used to test the Naïve Bayes Classifier. Precision and recall were however average. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. Through social media a better and more efficient way can be used to enable online learning generally and group formation specifically. It was able to extract tweets from various social media accounts based on a given hash tag (task) and then pass them to a Naïve Bayes classifier as input. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. This paper addresses the limitation of social media of not being properly utilized as a platform for supporting learning activities like group formation. Most of the information that passes through social media was being used majorly for social interaction. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_2	 This creates unwanted congestion during peak hours, loss of man-hours and eventually decline in productivity. The density of the vehicles is measured in three zones i.e., low, medium and high based on which timings were allotted accordingly. These include loss of man-hours, accident, missed opportunities, noise pollution, air-pollution, increased fuel consumption, increased tendency to violate traffic rules, and in some cases extortion by corrupt traffic control officials. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Sometimes higher traffic density at one side of the junction demands longer green time as compared to the standard allotted time. The suggested case study, Jakpa junction is a typical example of a traffic congested area. The junction is a link to three roads which include; Effurun-Sapele road, Jakpa road, and PTI road. In order to overcome the aforementioned problem, this research adopted a density based approach in controlling vehicular traffic. The signal timing changes automatically on sensing the traffic density at the junction. The sensors used in this project are infra-red (IR) and photodiodes. The density of the vehicles is measured in three zones i.e., low, medium, high based on which timings were allotted accordingly. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Raspberry pi is used as a microcontroller which provides the signal timing based on the traffic. A major drawback of this system is that it may not provide a reliable count of the vehicles upon which density is based. Fuzzy Logic offers the possibility to 'compute with words', by using a mechanism for representing linguistic constructs common on real world problems. This is very important when the complexity of a task (problem) exceeds a certain threshold. Real world complex problems such as human controlled systems involve a certain degree of uncertainty, which cannot be handled by traditional binary set theory. The three junctions at Railways, Haile Salessie and General Post Office were used to collect data through observations of traffic behavior at the intersection points. This is converted by a bridge rectifier to a dc voltage. To isolate the output voltage of +5V from noise further filtering by a 220uF capacitor is done. Since the peak inverse voltage of the diodes has to be greater than the peak secondary voltage of the transformer, the 1N4007 silicon diode with peak inverse voltage (PIV) of 1000 Volts was used in the circuit. For this circuit, V peak rectified = 16.67-2(0.7) =15.27v dc This voltage is the input voltage of the capacitor. The Transformer steps down the 220 v AC supply to 12 v AC. The time delay in the traffic signal is set based on the density of vehicles on the roads. According to the IR count, microcontroller takes appropriate decisions as to which road is to be given the highest priority and the longest time delay for the corresponding traffic light. The Vero board is also called a strip board. It is a widely used type of electronic prototyping board characterized by a 0.1-inch rectangular grid holes with parallel strips of copper cladding running in one direction all the way across one side of the board. The leads were then soldered to the copper tracks on the other side of the board to make the desired connections. The PIC Microcontroller was programmed using Embedded C language. The codes are as shown in the Appendix. Simulation was done via Proteus software. Jakpa Junction, in Effurun, Delta State, Nigeria is increasingly becoming chaotic by reason of the recurring traffic gridlock it experiences. The sensors help in keeping count of vehicles entering roads and the microcontroller subsequently allots time delay thereby giving accurate priority to each road. Although the aims and objectives of the project were achieved satisfactorily, it could be further improved upon. There should also be a two -way traffic light between Effurun -Sapale road and Jakpa road to pass vehicles coming from Effurun -Sapale road to Jakpa road. (2) Solar energy should be used to support the mains power supply because of the highly erratic nature of power supply from (PHCN).
paper_3	 Also, the formative evaluation of AEHS MATHEMA by students of the Department of Informatics and Telecommunications of the University of Athens, Greece, has shown, with the exception of other things, that all its functions are useful and easy to use. Adaptive Hypermedia Education Systems (AEHSs) can be considered as the answer to the problems of traditional hypermedia systems. They build a personalized model of the learner and apply this model to adapt the content and/or the appearance of the hypermedia, according to the learner's specific characteristics, such as the cognitive goal, level of knowledge, pre-existing or prior knowledge, interests, preferences, stereotypes, cognitive preferences and cognitive or learning style  [4] . In the Web-based AEHSs, several adaptive and intelligent techniques have been applied to introduce adaptation, such as  [4] : (a) Curriculum Sequencing: It helps the learner to follow an optimal path through the learning material. (b) Adaptive Presentation: It adapts the content presented in each hypermedia node according to specific characteristics of the learner. (e) Interactive Problem Solving Support: It provides the learner with intelligent help on every step of problem solving, by giving a hint to executing the next step for the learner. (f) Intelligent Analysis of Learner's Solutions: It uses intelligent analysers that not only tell the learner whether the solution is correct but also it tells him/her what exactly is wrong or incomplete. (g) Example-based Problem Solving Support: It helps the learners in solving new problems, not by articulating their errors, but by suggesting them relevant successful problem solving cases, chosen from their earlier experience. (h) Adaptive Collaboration Support; Adaptive Group Formation and/or Peer Help: These techniques support the collaboration process either just like the interactive problem solving support systems assist an individual learner in solving a problem, or they use knowledge about possible collaborating peers in order to form a matching group relatively to the kind of the collaborative task. AEHS MATHEMA (Meta-Adaptation Technology Hypermedia for Electro-Magnetism Approach) combines the constructivist, socio-cultural and meta-cognitive teaching model and supports personalized and collaborative learning. The general purpose of the AEHS MATHEMA is to support learners through an interactive and constructivist educational environment in order to construct their knowledge in Physics and Technology, individually and/or collaboratively, and to overcome their misconceptions and/or learning difficulties AEHS MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and meta-adaptive navigation, interactive problem solving, and adaptive group formation. More specifically it supports: (1) adaptive presentation according to learner's learning style; (2) four adaptive navigation techniques which are direct guidance (uses a "next" button), link hiding (hide nonrelevant links), link annotation (use colors to indicate relevance), and link sorting (like in search engines); (3) a meta-adaptive navigation technique in order for the learner selects the more appropriate navigation technique that best suits him or her; (4) an interactive problem-solving method through personalized and collaborative problem-solving activities that exploits the teaching approaches of experimentation through simulations, guided inquiry and discovery, investigations and peer collaboration; and (5) a method of adaptive group formation for peer matching. The AEHSs fit so nicely in this model. For each type of relationship (e.g., prerequisites) some generic adaptation rules can be defined. Today almost all AEHSs follow the general architecture of the AHAM reference model (but possibly with a different kind of rule or reasoning language and engine). The ADAPT project has identified high level design dimensions for AEHSs, which are: (1) context of use (CU), (2) content domain (DM), (3) instructional strategy (IS), (4) instructional view (IV), (5) learner model (LM), (6) adaptation model (AM), and (7) detection mechanism (DE). Proper's architecture is a combined architecture of SCORM LMS and AEHS. Thus the prototype involves four main modules: (1) The Domain Model (DM) that represents the domain knowledge of the system. (2) The User Model (UM) that represents the particular user's knowledge of the domain as well as his individual characteristics; both these models comprise the Data Model of the system. 3 The Adaptation Module (AM) which interacts with DM and UM in order to provide adaptive navigation to the course, and (4) The RTE Sequencer that is triggered by course navigating controls and interacts with DM and delivers the appropriate educational content to the learner. Educational content can be either SCO or Asset. Their system derives from SCORM 2004 Sample RTE Version 1.3.3 that is based on SCORM 2004 specification. Thus its architecture is a typical of a SCORM compliant LMS. They use the Apache Tomcat 5.5 as Web and application server and the MySQL 5 as database server. The adaptive navigation techniques have been applied to it via Java servlets and Java Server Pages (JSP). These adaptation rules involve the use of Learning Object (LO) metadata, which are independent of any learning style. This dynamic adaptation mechanism reduces the work-load of authors, who only need to annotate their LOs with standard metadata and do not need to be pedagogical experts (neither for associating LOs with learning styles, nor for devising adaptation strategies). 4 ) based on JSP, Java servlets, and Javabeans. The Presentation Generator requests a composition of the presentation to the JavaServer Pages. It enables to recognize the student's learning style automatically in real time by means of Multi Layer Feed-Forward Neural network (MLFF). For supporting features, it uses the (JSP) pages, Java servlets, JavaBeans, JavaScripts, and Apache Struts technologies. A servlet is a Java class that implements the Java servlet API, a protocol to respond to HTTP requests. JavaBeans technology comes into play. (3) Protects your intellectual property by keeping source code secure. The Apache Struts is a formalized framework for the architecture style design pattern Model View Controller (MVC). Figure 5  shows the architecture of the AEHS MATHEMA. The didactic design of the AEHS MATHEMA supports learners in constructing knowledge, choosing and achieving their learning goals, recognizing what they have already learned and what they are capable to do, and judging personal progress of their learning. The domain knowledge is structured in a way that supports the ability of the system to choose the educational material, depending on the learner's requirements and current status. It also defines the relationships between the concepts (prerequisites) and the level of difficulty of the concepts (see part of them in the  Table 1 ). The model that supports the AEHS MATHEMA is the overlay model. The characteristics that the system maintains in learner model for each learner are: name, gender, username, password, learning style, level of knowledge for each cognitive goal (general performance), level of knowledge for each basic meaning of the cognitive goal (quantitative and qualitative characterization), concepts that have been successfully evaluated for each cognitive goal, current page, current teaching method, current navigation technique, navigation history, meta-adaptive navigation status, initial Web experience and knowledge level (pre-existing knowledge) in the current cognitive goal, ways of presenting feedback messages, preference for assistance by Advisor in terms of its navigation, course sequence, and phases of the guided dialogues in interactive problem solving process in which each learner participates. The Student Model is transparent to the learner (open learner model) and is verifiable by the learner, that is, the learner can access his/her model and change the content of some characteristics. The learner during the study selects the cognitive goal he or she wishes to study, following his or her personal course in the educational content, exploits the system's assistance, navigating and studying, is informed about the information that the system maintains, who can change them by intervening and directing the system's adaptation in this way, in terms of curriculum sequencing, adaptive presentation and navigation and adaptive group formation, selects his or her learning style, selects the navigation technique, activates and deactivates the curriculum sequencing, selects the way that the feedback messages are presented, changes his or her knowledge level for each basic concept of the cognitive goal, and activates-deactivates the Advisor. The system, during the learner's study, monitors his or her interactions with the system and his or her assessment and accordingly updates the links to the course material provided and/or adapts learning material to learner according to his or her learning style. The adaptive navigation techniques that it supports are: direct guidance, link hiding, link annotation, and link sorting. This is an innovation on the architecture of AEHSs. It is the second level meta-adaptation engine that is responsible for monitoring the cognitive improvement of the learner in the cognitive goal that he or she studies, after n successful evaluations of his or her knowledge in basic concepts, showing him or her advantages and disadvantages of the four navigation techniques that it supports. In AEHS MATHEMA, meta-adaptive navigation works as follows: The first time that the learner enters the system, he or she is asked to state his or her Web experience and level of knowledge in the cognitive goal he chose to study (preexisting knowledge). Following the learner's statement, the system suggests the most appropriate navigation technique to him or her, taking into account the level of his or her Web experience and his or her level of knowledge in the subject he or she chose to study as follows: (1) Direct guidance: Little or no Web experience and little or no knowledge of the cognitive goal. knowledge of the cognitive goal. The meta-adaptation engine proposes to the learner to change the navigation technique if he or she wishes after n successful assessments of the basic concepts of a cognitive goal he or she studied, showing him or her the pros and cons and additional information on each of four navigation techniques, and the learner decides whether to change it or not. Figure 7 . A snapshot of a meta-adaptation result. In this module there is the basic servlet, which deals with the presentation of the training material and other JSP pages and servlets, such as the page and the corresponding servlet to inform the learner about the characteristics of all learning styles, the servlet for the curriculum sequencing, the servlet for selecting and presenting the concept that the student selects to study according to his or her learning style, etc. More specifically it: (1) initializes and updates the Student Model; (2) collects and stores the data that the learner answers to open-ended questions, entries in tables, registers in notes, etc; (3) monitors the interactions between the learner and the system to get information about his or her choices in terms of the questionnaires, the phases of guided dialogs, the links, the options for meta-adaptive navigation, the visited pages, and more. Following a study  [13]  conducted on the formation adaptive groups in the MATHEMA, it was decided that: when the system creates a priority list of candidate peers for an interested learner, in the first and second position of the priority list, the algorithm will place the candidate collaborators with a concrete or abstract style and with the same active or reflective dimension of their learning style as follows: If the learner has a concrete learning style, then the algorithm will place the candidate collaborators with a concrete learning style in the first position, and in the second position, the candidate collaborators with an abstract learning style. If the learner has an abstract learning style, then the algorithm will place the candidate collaborators with an abstract learning style in the first position, and in the second position, the candidate collaborators with a concrete learning style. In candidate collaborators belonging in the same position, the classification is according to their level of knowledge in the current cognitive goal up to that moment. The collaboration protocol that uses the synchronous communication tool is as follows: (1) The learner declares willingness to collaborate either by selecting his or her partner from the priority list of candidate collaborators or by declaring a desire for collaboration so that others who would like to work with him or her can choose it, while activating the synchronous communication tool. (2) Learners negotiate for collaboration and, if they come in agreement with each other, state it in the system to not include them in the priority list of other learners who would like to work with them. (3) After negotiating and updating the system that they come in agreement with each other to collaborate, learners can start collaboration. The  Figure 11  shows a snapshot of a dialog between the learners Giannis and Mary taking place via the Synchronous Communication Tool (chat-tool). Generally, the students felt that almost all of the system's functions are quite useful (83.7 to 100 percent) and easy to use (58.1 to 93 percent). Regarding adaptive and intelligent techniques of the AEHS MATHEMA mentioned above, the students having evaluated them consider that the adaptive presentation of different teaching strategies is quite useful (93.0 percent) and fairly easy to use (86.0 percent), the different ways of navigating are quite useful (100.0 percent) and handy (90.7 percent), the meta-adaptive navigation is quite useful (88.4 percent) and fairly easy to use (86.0 percent), the adaptive group formation of collaborative teams to assist them in collaborative problem solving is useful (83.7 percent), but less user-friendly (58.1 percent). This article presents the trends of architecture of AEHSs so far and then an extensive description of the architecture of the AEHS MATHEMA. Most of AEHSs presented in this paper are based on JSP, Java servlets, and Javabeans technologies to implement their functions, as implemented by AEHS MATHEMA. Furthermore, the presented AEHSs above have at least three main modules, such as Student Model, Domain Model, and Adaptation Model as the AEHS MATHEMA also has. MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and metaadaptive navigation, interactive problem solving, and adaptive group formation.
paper_21	 In this paper, we present the powerful scheme ZSISMP (Zimmermann Self Invertible Stabilizer Multiplier Permutation) to attack the hardness of the minimum distance search problem of BCH codes. The proposed scheme ZSISMP is validated on all BCH codes of known minimum distance. A comparison with several known powerful techniques proves its efficiency in giving more accurate results in short time. The use of this efficient local search had yield to determine the error correcting capability of many BCH codes of length 1023 and 4095. An efficient solution of this problem is the use of error correcting codes. The remainder of this paper is organized as follows: The next section presents the main related works. The section 3 presents the proposed scheme ZSISMP. The section 4 presents the main results. The conclusion and possible future directions of this research are outlined in section 5. For this reason, many researchers have explored several ways to attack the difficulty of the minimum distance search problem for large BCH Codes. In  [9] , Augot, Charpin, and Sendrier presented an algebraic system constructed from Newton's identities. The existence of solutions to this system is a necessary condition to the existence of codewords of weight w in the code. The use of this method has finished the table of BCH codes of length 255. [15] . This improvement has yield to a fast convergence of the Simulated Annealing by reducing the number of iterations, as well as obtaining good results in comparison with the previous works presented in  [17-18-19-21] . From these stabilizers, we take only a Self Invertible stabilizer if it exist and by using a mathematical tool, we find the sub code fixed by this involution and then we evaluate the minimum distance by using the famous Zimmermann algorithm. The proposed scheme works as follows: Inputs: formula_0 Step 1: Find the sub code SC fixed by σ Step 2: Find the estimated minimum distance d of SC by using the Zimmermann algorithm. This section presents also a comparison between the proposed scheme and previous work on minimum distance for BCH codes. It is well known that the smallest primitive narrow-sense BCH code whose minimum distance is greater than its designed distance is BCH (127, 43, 29) and it is the only one for this length. In order to find the minimum distance of some large BCH codes, the proposed scheme is applied by using a simple machine of the configuration given above. The obtained results are given in the table 4 so that d f represent the minimum distance found by our scheme. In this paper, we have proposed a new efficient scheme to find the minimum distance for large BCH codes.
paper_31	 Chronic diseases have long duration, difficulty in treatment and high cost. From 1990 to 2017, the morbidity of chronic diseases with high incidence in China had been increasing continuously  [1] . According to China National Health and Nutrition Big Data Report 2018, 20% of Chinese suffer from chronic diseases, and chronic disease mortality was 86%. Therefore, it is crucial to effectively prevent and control chronic diseases. With the development of Internet, people's life has been profoundly changed. Due to the late development of information technology in China, our Wise Information Technology of med (WITMED) starts later than foreign countries. For example, the General Hospital of the People's Liberation Army (PLAGH) communicated and discussed some diseases with foreign experts through satellites. And some scholars combined Bluetooth technology with GSM short message to realize the collection and remote monitoring of physiological parameters such as blood pressure  [12]  and pulse. Chronic diseases can easily cause damage to patients' vital organs such as the brain, heart, and kidneys  [3]  to seriously affects working ability and quality of life, and its high medical expenses increases the burden of economy on society and families. Due to the hazards and characteristics of chronic diseases, researchers all over the world have presented various methods for preventing, monitoring and treating it from different perspectives. Until now, the development of foreign WITMED (Wise Information Technology of med) is divided into three stages: The first stage refers to the middle of last century when Network technology was just emerging. This is the second stage of the development of WITMED. Most data were transferred by satellite and ISDN (integrated service digital network). In recent years, with the popularization of mobile terminals (mobile phones, tablets) and the rapid development of the Internet, Telemedicine starts to combine with big data, artificial intelligence, cloud computing, cloud services and other technologies. This is the third stage of the development of WITMED. At this stage, telemedicine has gradually transitioned to WITMED, and developed from precision medicine to disease prevention  [5] . Energy-efficient Zigbee-based wireless sensor network (WSN) occupies a major role in emergency-based applications  [6] . Kumar establishes an enhanced shortcut tree routing-based geographic location (ESTRBGL) protocol  [7] . Haji Bagheri Fard present methods based on deep learning algorithms to achieve high classification accuracy  [8] . Zigbee has been implemented on the Health Care Profile. As an alternative for aiding healthcare systems, sensors and wearable devices are used for monitoring patient physiological data to help guide health services or the self-care of patients  [9] . Detecting physiological signals in the daily life of the population can effectively grasp the health status of the people in the area, and provide an important reference for the diagnosis and prevention of chronic diseases  [10] . In this telemedicine stage, it gradually focused more at community, families and individualized care  [11] . There are some defects of the current WITMED (Wise Information Technology of med) system as follows: (1) The inadequate reliability of data transmission. (2) The problem of power consumption. Chronic diseases patients have to be long-term monitored, however, whether using a WIFI or GPRS, high power consumption will be arouse. (3) The problems of unitary monitoring data. (4) The problem of data processing. As the speedy development of the escalation of people's living standard, China has gradually entered the aging society with an increasing number of patients with chronic diseases and a descent trend of age  [14] . According to the statistics of Professor Yang Hongying, there are 212 million patients with chronic disease in China at present. The traditional prevention and treatment greatly increase the economic cost and time cost of patients. Attracted by the broad prospect of smart medicine, BAT and other Internet enterprises have participated in the smart medical industry. The data acquisition structure charts  After receiving the relevant data, the data analysis platform needs to analyze the corresponding data and feedback the processed results to relevant users. Suppose there are N pieces of information to be sent according to the messages' arriving order, divide each message into many words, and mark each word as Nm, , . With the definitions above, f x, y can turn to be: f x, y ax 1 a y ε , namely f x, y y a x y ε . Definition 2: let {X1, Y1}, {X2, Y2} is concept form of {X, Y}, {X2, Y2b {X1, Y1}. Therefore, it is crucial to guarantee the security of data transmission in the process of information transmission. This system adopts AES (Advanced Encryption Standard) encryption algorithm which is widely used at present. The state matrix after row shift is multiplied by the fixed matrix to obtain the confused state matrix. The process of key plus calculation can be regarded as the result of Bitwise XOR or byte level or bit level operation. The intelligent system realizes data acquisition, data encryption and processing, and big data analysis.
paper_38	 A purposive sample of nine hundred and forty-three (943) first-year students constituted the population for the study were drawn from Computer Science, Mathematics and Physics undergraduate degree programmes from the Faculty of Science of the university who were admitted from the 2010/2011 to 2014/2015 academic sessions. It is a progressive development of knowledge and skills of examinees through stages of teaching and learning at various levels  [1] . Students admitted into any of the Faculty of Science undergraduate degree programmes in the Kaduna State University must have been subjected to serious academic scrutiny. In order to accomplish and improve the value of education, it is necessary to find other ways to enhance the academic performance of students. Over the years in Nigerian tertiary institutions, there has been rife with complaints about students' poor academic performance. Alternatively, they could be withdrawn from the University due to the lapse of the given probation period for those who had a CGPA of less than 1.0 in two consecutive academic sessions. The following research questions directed the study: 1. 2. How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? 3. What is the relationship between OL results, UTME scores and Post-UTME scores of students and their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? 4. How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? Performance as defined by  [7] , is an observable or measurable behaviour of a person or an animal in a particular or experimental situation in which the authors further explained that performance measures the behaviours within a specific period. Reference  [9]  stated that students' performance could be obtained by measuring the learning assessment and co-curriculum. Academic performance or sometimes known as an academic achievement is defined by  [10]  as "Knowledge attained or skill developed in the school subjects, usually designated by test scores or by marks assigned by teachers". A student's academic performance usually is measured in either examinations or continuous assessment tests, and this is expressed in various ways depending on what the scores should be used for. The numerous ways of reporting academic performance include raw scores, percentages, transformed scores, or even as categorical variables such as Excellent, Merit, Very Good, Pass, First Class, Distinction, A1, B2, C4, F9, and others. However, the following review of literature examined the relationships between UTME and PUTME scores as a predictor for the academic performance of students have revealed contradictions in their findings. The authors in  [8]  comparatively analysed the academic performance of graduates admitted through UTME and preliminary programmes (Certificate, Basic Studies and School of Science Laboratory Technology [SSLT]) in the University of Port Harcourt. The records of students who graduated in the 2009/2010 and 2010/2011 academic sessions from seven faculties were obtained using the stratified random sampling technique. Graduates with the best academic performance from the preliminary programmes were those admitted through the certificate programme. Their results revealed that the correlations coefficient between PUTME and CGPA for the four departments were negative/low, positive/low and positive/moderate coefficients. The author's study found that the use UTME score was a very poor predictor of students' final grades and thereby recommended that less emphasis should be placed on UTME scores as a criterion for admission of candidates into universities. Their results suggested that the JAMB UTME had positive but low indices of predictive validity, which varied across the academic sessions from 2005/2006 to 2013/2014 and all programmes of study except for four departments. In contrast to the studies from the earlier mentioned authors,  [15]  investigated the relationship between 276 students' performance in the entrance examination and their performance in Mathematics in two selected Colleges of Education (CoE) in Osun and Oyo states each. Furthermore,  [16]  used the PPMCC analysis to investigate to which extent the scores of UTME and PUTME predicted the academic performance of university undergraduates. The author further recommended the need for the PUTME exercise to be strengthened to have a fruitful admission of candidates. For instance,  [12]    This study aimed to investigate which of the University's entry requirements used for the admission process best predicts the academic performance of students in the 100 level CGPA examinations. He explained that there are two types of ex-post facto research designs namely the correlational and the casual comparative. The design adopted in the study is the correlational ex-post facto, which is used to measure the degree of association between two or more variables or sets of scores. The correlational design is also sub-divided into explanatory and predictive research designs. The Faculty of Science consists of nine undergraduate B. Sc. Full-time degree programmes: Biochemistry, Biological Sciences, Chemistry, Computer Science, Geography, Industrial Chemistry, Microbiology, Mathematical Sciences, and Physics. The population of the study was limited to consist of all students admitted into three Faculty of Science undergraduate degree programmes of Kaduna State University for five academic sessions from 2010/2011 to 2014/2015 using the OL, UTME and CGPA results. The instrument used to derive the data for this study were: the JAMB UTME scores from 2010 to 2014 and OL grades in the five relevant subjects which were the pre-admission criteria and first-year CGPA results obtained from the semester examinations from 2010/2011 to 2014/2015 academic sessions. The data used in the study are the OL grades, and JAMB UTME scores obtained from the University's central database, and the academic standing of first-year CGPA results collected from the various departmental examination officers (DEOs) with the approval from the Dean of the Faculty of Science. However, the OL results obtained from the database was for students admitted during 2010/2011 to 2014/2015 academic sessions. Stanine (STAndard NINE) was defined as "a nine-point scoring system with a mean of five and a standard deviation of two"  [18]  and is used in education to compare student performance for each subject. It was used in this research study. In other words, MLR is used to predict a nominal dependent variable given one or more independent variables. The results of the data analysis are presented in tables according to the research questions that guided the study in this section. How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? Table 6  is the likelihood ratio test results for the Computer Science, Mathematics and Physics degree programmes. In  Table 6 , the likelihood ratio Chi-Square of 18.723, 17.661 and 12.401 for Computer Science, Mathematics and Physics programmes with a significant value of 0.227, 0.281 and 0.414 tells us that the model as a whole does not predict the dependent variable, i.e., CGPA. For Computer Science, Mathematics and Physics programmes, the first equation intercept is the log of the ratio of the likelihood of a student having a 'pass' degree to the likelihood of that student having a 'Fail' degree. These showed that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. What is the relationship between OL results, UTME scores and Post-UTME scores of students and their firstyear CGPA across the academic session, 2010/2011 to 2014/2015? Table 8  shows the summary of correlations coefficient between OL-CGPA, UTME-CGPA and PUTME-CGPA aimed at all the degree programmes for the academic sessions ranging from 2010/2011 to 2014/2015. PPMC was used to analyse the data for this research question. The summary of  Table 8  shows the correlations of OL and CGPA scores, UTME and CGPA scores and PUTME and CGPA scores for all the programmes used for the analysis. How well do OL results, UTME scores and Post-UTME Sa'adatu Abdulkadir and Francisca Nonyelum Ogwueleka: Predicting Students' First-Year Academic Performance Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria scores of students predict their first-year CGPA across the academic session, 2010/2011 to 2014/2015? Table 9  is the likelihood ratio test results for all academic sessions ranging from 2010/2011 to 2014/2015. These show that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. The primary purpose of this study is to investigate if OL results, UTME and PUTME scores do predict the academic performance among first-year undergraduate students in the Faculty of Science. This predictor holds true especially for students who are in the CGPA category of '2 nd Class Lower' and '1 st Class' respectively. The authors in  [21] , in their study also recommended the need of potential researchers to compare the OL, UTME and Post UTME terms and scores across Nigerian Universities for standardisation and a model for educational development in the twenty first century.
paper_57	 Crossover and mutation operations have been adapted for use with ant generated strings which still have to provide feasible solutions. Extensive simulation tests were made in order to determine influence of genetic operation on algorithm performance. [2]  as Ant system (AS) algorithm. On their way back they use the same way from which abundant loops has been removed, but the amount of pheromone (1) ∆τ k ij (t) they produced is inversely proportional to the tour length L k (t). formula_0 formula_1 T k (t) is the tour generated by ant k, Q is a constant and tuple (i, j) denotes beginning and termination node of an arc. Let's mention ant colony system (ACS) with pseudo-random proportional rule  [3]  in which random uniformly distributed variable q ∈ (0,1) is compared with a tunable parameter formula_5 else the probability selection rule (3) is applied; random selection applied to AS rank  [4]  where random selection rate r is the probability of random selection and represents a user parameter which adjusts the balance between exploration and exploitation; prevention of quick convergence (i) and stagnation avoidance (ii) mechanisms applied to AS  [5] . The stagnation avoidance mechanism (ii) is based on the comparison of a randomly generated quantity q ∈ (0,1) with probability p k ij (t) of selected arc. If q ≥ p k ij (t), then choose the next node randomly. The original GA is known as simple genetic algorithm (SGA). In ACO algorithms representation (i) of genotype space is sequence of nodes: formula_6 where gene n is graph node and L is path length. 1 . 2) . If more such nodes occur, random selection is applied. If no such node exists, another gene is randomly picked up from the list. 3 ). If more of such nodes exist, random selection is applied. Since optimization process is primary done by ants cooperative behavior, the selection process has purely random concept and genetic operations serve just for selection pressure decrease. The above described genetic operations have been applied to one of the best performing ACO algorithms of Kumar, Tiwari and Shankar (ACO KTS )  [5] . At the end of each cycle t, when all the ants finish their tours T k (t), genetic operations are applied on the T k (t) strings which represents the list of nodes. Prior the genetic operations all loops are removed from the tours. If mutation is not feasible, another node is chosen. If crossover operation is not feasible, another second string is selected. After all genetic operations are executed fitness evaluation and pheromone update are scheduled. Feasibility of genetic operations depends on the graph and generated tours. For this purpose ACO GO algorithm has embedded user feedback which represents a ratio between accomplished and required genetic operations. 4) . Variable parameters were set to determine the influence of the genetic operations quantity on algorithm performance and effect of distribution of mutation operations between paths. The reference value of n [%] was received without any genetic operation and is 5.6 (Table 2, row 1). It can be seen that the higher number of mutation operations, the better the performance is (Tables 2). Results received without crossover operation have higher values along with the Mutation paths axis  (Fig. 5 ). However, results received with two crossover pairs have higher values along with the Mutations per path axis  (Fig. 7) . I.e. with no crossover pair certain amount of mutation operations should be spread out among more paths, but with 2 crossover pairs concentration of mutation operation on less paths tends to perform better. As the crossover rate increases, ratio accomplished / required operation decreases  (Fig. 9 ). It has been proved that genetic operations increase ACO algorithm performance. Limit of crossover is 60% of crossover rate. The higher the crossover rate, the lower the accomplished / required ratio. Mutation operation causes better results than crossover operation. Without crossover operation distributed mutation operation has better performance, but with two crossover pairs concentrated mutation operation on less paths tends to perform better. Further research and more experiments are needed to determine the distribution and optimal amount of mutation operation with respect to the number of ants and length of the paths.
paper_78	 A hundred plots were classified into 12 vegetation formations by fuzzy C-means clustering. These formations were main communities dominated by Glycyrrhiza uralensis in North China. The results suggest that fuzzy C-means clustering is an useful technique for classification of plant community. It is applicable in ecology because the description of ecological systems is not always possible in terms of a binary approach  [4] . The importance value was calculated by the formulas  [7] :  formula_8 Based on the investigation, we determined C equal to twelve. I. Form. Its disturbance intensity is medium and heavy. The average cover of Glycyrrhiza uralensis in this community is 25% with a density of 3600 ha -1 . Form. Its disturbance intensity is medium and heavy. The average cover of Glycyrrhiza uralensis in this community is 35% with a density of 5700 ha -1 . Form. It is distributed from 300 to 500 m in hills with slope 20 -30° in sunny， semi-sunny and semi-shady slope and sandy chestnut soil. Its disturbance intensity is medium and heavy. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 59500 ha -1 . Form. Its disturbance intensity is medium and heavy. The average cover of Glycyrrhiza uralensis in this community is 37% with a density of 4900 ha -1 . IX. Form. Its disturbance intensity is heavy. The average cover of Glycyrrhiza uralensis in this community is 33% with a density of 5500 ha -1 . X. Form. Its disturbance intensity is heavy. The community has a total cover of 75%, a shrub layer cover of 10% and an herb layer cover of 70%. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 6000 ha -1 . XI. Form. It is distributed from 400 to 750 m in hills with slope 15 -35° in sunny and semi-sunny slope and sandy soil. Its disturbance intensity is heavy. The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 65%. The average cover of Glycyrrhiza uralensis in this community is 29% with a density of 4100 ha -1 . XII. Form. Its disturbance intensity is heavy. The average cover of Glycyrrhiza uralensis in this community is 30% with a density of 4000 ha -1 . Fuzzy C-means clustering successfully classified 100 plots into 12 communities dominated by Glycyrrhiza uralensis. This study proved that fuzzy C-means clustering is fully usable in classification analysis of plant communities  [1, 19] . Glycyrrhiza uralensis communities recognizing by fuzzy C-means clustering varied greatly in species composition, structure and distribution area  [20] . These twelve formations represent the general vegetation of Glycyrrhiza uralensis in northern China  [21, 22] . These communities are important not only for protection of medicinal plant species but also for conservation of ecosystems and their environments in semi-arid and arid regions in North China  [23, 24] . These further confirm that fuzzy C-means clustering is an effective technique in vegetation analysis  [28, 29] .
paper_96	 With the rapid development of China's transportation, the frequency of traffic accidents is also high. Such warning system of obstacle avoidance of embedded electronic guide dog can effectively eliminate the impact of the traffic environment and the subjective factors of the blind, warning in advance for the travelling blind in time, effectively avoiding obstacles such as vehicles, to reduce traffic accidents caused by the their blindness. This humane technology innovation is the specific embodiment of environmental science and technology aesthetic theory in the field of scientific and technological innovation. It has a positive and promoting role to the development of transportation and blind-man welfare in China. I think that if electronic guide dogs guide them to avoid obstacles such as vehicles, the blind people are able to avoid such accidents, aren't they? So I had an idea that I use the scientific knowledge of embedded system grasped by me to invent a kind of "electronic guide dog". After that I began to design the structure of the entire system, which made the overall implementation of "electronic guide dog" take a welcome step. Currently, some of the international anti-collision avoidance system development has made some achievements, and the more successful countries are Japan, Germany, and the United States. The research on "warning system of embedded electronic guide dog avoiding obstacles" in this paper is an important part of intelligent electronic guide dog system, the study of which is mainly based on the embedded system, anti-collision warning system, and the USB technology is applied to warning system of embedded electronic guide dog avoiding obstacles. In this way, the design of obstacle-avoiding early warning system of embedded electronic guide dog has the following advantages. [3]  The alarm system of guide dog adopts the combination of light and sound. The hardware of the obstacle-avoiding early warning system of embedded electronic guide dog can reference the Figure 1:  Figure 1 : Schematic diagram of hardware system structure  [4]   In the anti-collision early warning system, referenced in the design of obstacle-avoiding early warning system of embedded electronic guide dog, there mainly exits four steps, that is, information collection, information processing, information judgment, and warning information. Presently, the types of the generally used anti-collision warning systems mainly are radar anti-collision warning system, ultrasonic anti-collision warning system, laser anti-collision warning system, infrared anti-collision warning system, machine vision anti-collision warning system, and interactive intelligent anti-collision warning system. We should optimize the choice of the obstacle-avoiding early warning system technique of embedded electronic guide dog, based on China's national situation. Therefore, in the design process of the obstacle-avoiding early warning system of embedded electronic guide dog, after the comprehensive and comparative analysis of the anti-collision warning techniques, we have compared the optimization principles of the anti-collision warning techniques, and finally adopt the approach of millimeter wave radar sensor + ARM microprocessor. The design of obstacle-avoiding early warning system of embedded electronic guide dog also adopts the generally used embedded processor. Presently, the generally used embedded processors mainly are ARM, Power PC, MIPS, Motorola 68000 series, etc. The full name of ARM is Advanced RISC Machines. The ARM architecture follows the principle of reduced instruction set computer (RISC). At present, the embedded RTOS (Real Time Operate System) mainly includes two categories--commercial and free. Thereinto, the commercial RTOS includes WINCE and VxWotks; while the free RTOS has Linux (uCLinux and RT Linux included) and uC/OS -II. These are the most remarkable features that uCLinux owned. Also, it supports a large number of other network protocols and various file systems, including NFS, EXT2, ROMFS, JFFS, ms-dos, FAT16/32, etc. In addition, the design of obstacle-avoiding early warning system of embedded electronic guide dog also makes a full use of USB. The warning system of embedded electronic guide dog avoiding obstacles studied in the project can effectively eliminate the influence of traffic environment and blind subjective factors, send the pre-trip alarm for the blind in time, effectively avoid obstacles such vehicles, to reduce traffic accidents caused due to the blindness.
paper_134	 Olive oil is one of the most important agricultural crops due to its digestive properties and economic status. However, olive oil production is a costly process which causes an expensive price of the final product. The most jobbery ways during olive oil production consist of mixing other oils such as maize, sunflower, Canola and corn into the olive oil. So, the aim of this study was to develop a dielectric-based system to Authenticate in olive oil using cylindrical capacitive sensor. A set of 16 samples of olive oil, sunflower, canola and corn oil which mixed with different ratio of Authentication, were used for calibration and evaluation of developed system. New information and communication technologies, as well as decision support technologies, can be very effective in providing timely, accurate, and relevant information to users by collecting, storing, evaluating, interpreting, analyzing, retrieving and disseminating information to specific users  [1] . Detecting the purity of different materials can be done in a variety of ways  [3] . Dielectric properties are one of the most important physical properties of agricultural and food products. The latest statistics have shown that every Iranian consumes 100 grams of olive oil per year. One of the main reasons for this low consumption is the high price of this oil. When in a country where milk is eaten up by 30 percent as a result of a rise in the price of a few hundred tons, what else would you expect from a 60 to 80 thousand Riels per liter consumption of lubricants  [9] ? Knowing the original oil is also not an easy task that anyone can handle. Lizhi et al. (2010) conducted a research on olive oil for detecting adulteration using a dielectric spectrum. They used a 4V sine voltage in the range of 10Hz to 1 MHz to determine the dielectric properties of a binary mixture of olive oil. They also used the partial least squares model (PLS) to detect oil falsification. The results of the PLS calibration model showed good results for the adulterated oils with soybean oil as a counterfeit oil. Also, the charts sorted by the method showed clear performance for all oil samples and easily categorize them in different clusters. Soltani et al (2010) used a non-destructive capacitive method to detect the amount of banana fruit juice. The results of the prediction were satisfactory and the correlation coefficient between the actual yield and the expected value of the fruit was 0.85  [13] . Reggie et al. (2006) predicted egg quality parameters using its capacitive properties. Soltani et al. (2015) reported a research about egg grading using image and sensor processing. 70% of the data was for network training, 15% for validation and 15% for the network testing. Samples of olive oil provided from Khorramshahr Oil Company and produced at Rudbar oil plant located in Manjil. Due to the high flow of data, the ch340g chip on the Arduino board is used to measure dielectric parameters as well as a device that can detect the purity of olive oil. Arduino has a variety of ports, controls, side panels, and screen connectivity, while it may be smaller, less expensive, and possibly faster than a microcomputer system. (Figure 1 .) Using this technique, the amount of RMSE for the samples were equal to 3.56, 2.45, 3.05 and the amount R 2 was equal to 0.92, 96 and 94, respectively. Figure 5  shows the results of adulterated oil boosted tree regression. Also, in regard to the amount of RMSE, result has shown 4.81, 2.54, and 4.38 for Olive-Sunflower, Olive-Canola and Olive-corn, respectively. In the current research, three different techniques were applied to predict olive oil adulterated. Interestingly, olive-Canola oil samples predicted with high accuracy in all techniques.
paper_139	 Much effort made towards in investigates mechanism with identical detection algorithms, still the retrieved web documents with outmodedlink. In this proposed system, we are successfully identifying and minimize the redundant information and like link in web documents. We introduce the correct graph theory based KTMIN-JAK-MAXAM algorithm filters out the redundant link. Web content mining is the way toward extracting the applicable information, data and learning from World Wide Web. Utilizing customary data recovery  [5]  and information mining systems it get to the known and obscure data from the Web content. Web mining is categorized into three group Web Content mining  [6] , Web structure mining, Web usage mining. After applying the above steps the entire vertex without redundant information available in the set U. Pseudo Code forThe Proposed Algorithm KTMIN-JAK-MAXAM Step1: Compute degree measure for all vertices in the setup. Step2: Pick the minimum degree vertex 'v' in the set-up and include in the set U. Step3:While U doesn't include all vertices Step3A: Include the entire isolated vertex which is adjacent to the vertex 'v' to U. Step3B: Find the adjacent vertex 'u' to 'v' which is not in U and has maximumdegree. Update 'u' to U. Step3C: Update the value of degree for all adjacent vertices of 'u'. Repeat step 3 till all nodes are included in the set U. Step4: Finally network U contains no cyclic information. We have 3 nodes (B, 1 and 5) and we can pick the minimum degree node. Here all nodes have an equal degree  (3) . So pick any one of the node among the 3 adjacent nodes. Now the set U consists of the nodes A, B, 2, 1, 3. We propose a graph theoretical based algorithm for detecting and eliminating redundant links. Also we observed derived linked graph need not be unique but this approach will provide the optimized cost analysis report in future in data science field. Future work aims to create a finite automata tool to produce only relevant and without redundant information of web documents in data mining.
paper_145	 With the increase in age significant increase in prevalence rate of obesity was observed. The most common medical morbidities associated with obesity include impaired glucose tolerance and metabolic syndrome  [11, 12] . The important factors for obesity and overweight were identified by factor analysis, where largest factor weight indicated the most important variables  [14, 15]  responsible for obesity. To study the variability of socioeconomic variables for diabetic and nondiabetic people, some respondents were also investigated as a control group. However, among this latter group of respondents also there were 91 diabetic patients. Data have also been collected from parents/guardians of 200 randomly selected students of different disciplines of the university, on the assumption that the respondents would be of normal group of people. The variables included for factor analysis were residence of the respondents, their age, gender, marital status, religion, level of education, occupation, type of work, monthly income and smoking habit. The analysis was done by using SPSS [version 20.0]. The level of obesity was measured by BMI [weight in kg /(height in m)  2  ] and it is a most commonly used measure of level of obesity  [18] . The respondents were classified as underweight  [BMI < 20] , normal [BMI,  20 -25] , overweight [BMI < 30] and obese [BMI 30+]. In performing factor analysis the inclusion of variables was decided by calculating square of the multiple correlation coefficient  [19] . Besides the factor analysis, the association of different socioeconomic variables with level of obesity were investigated. The differences in proportions of level of obesity according to residential area were not significant [P (χ2 ≥ 5.128) = 0.528] which indicated that respondents for different levels of obesity were similar for both urban and rural areas. The levels of obesity were significantly different among males and females [  Table 2 , P (χ2 ≥ 27.546) = 0.000]. There were 58.9 percent males among the respondents and 47.2 percent of them were normal. Significant differences in proportions of obesity among the two religious groups were noted [P (χ2 ≥ 10.82)= 0.012]. Among the investigated respondents 92.6 percent were currently married and 43.1 percent of them were normal  [Table  4 ]. Similar normal group was noted among the other group of respondents. However, there was significant differences in proportions of different levels of obesity among the two marital groups of respondents [P (χ2 ≥ 22.933) = 0.028]. Majority (52.9%) of the respondents were of age 50 years and above and 48.5 percent of them were normal [  Table 5 ]. The percentages of normal groups among the respondents of ages 25 -40 and 40 -50 were 35.5 and 36.6, respectively. Levels of obesity was significantly associated with levels of ages [P (χ2 ≥ 18.34) = 0.008]. But if classification of respondents was done into two groups, one group of ages <40 years and another group of ages 40 years and above, both the groups were almost similarly exposed to overweight and obesity [O. R.= 0.81]. The proportions of different levels of obesity according to professional variations were significant [P (χ2 ≥ 46.472) = 0.000]. But, all other professional groups of respondents had the similar risk of obesity compared to servicemen [O. R. = 1.04]. It was noted that [  Table 9 ] 50 percent respondents were involved in official work with or without physical labor. There was no significant association between level obesity and prevalence of diabetes [P(χ2 ≥ 0.851) = 0.837]. The present study also indicated similar result [  Table 11 ]. Among the smokers 47.2 percent were normal and 37.2 percent were overweight. The corresponding figures among non-smokers were 41.3 and 32.5, respectively. The association between smoking habit and level of obesity was significant [P(χ2 ≥ 20.189) = 0.0.002]. Table 11  Among the smokers 47.2 percent were normal and 37.2 percent were overweight. The corresponding figures among non-smokers were 41.3 and 32.5, respectively. The association between smoking habit and level of obesity was significant [P(χ2 ≥ 20.189) = 0.0.002] and smokers were 21 percent more exposed to overweight and obesity compared to non-smokers [O. R. = 1.21]. Thus, we were in search of identification of most important variables to explain the variation in the levels of obesity in the present data  [14] . This was done by factor analysis. The analysis helps to identify the important variables to explain the variation in the data set  [15, 21] . The inclusion of the variables in the analysis was justified as communality of no variable was less than 0.4  [22] . From the analytical results it was seen that all the p-values were less than 0.05 which indicated that the inclusion of variables for factor analysis were justified. Moreover, no communality for a variable was less than 0.4  [22] . From the results of the communality of the variables it could be concluded that 85.6 percent variation of the variable marital status would be explained by the extracted factors  [22] . From the factor analysis it was noted that the coefficients of the variables gender was highest followed by occupation, education and type of work. This information were noted from the characteristic roots of the correlation matrix of the variables, where the roots were 2.573, 1.616, 1.086, 1.053, and 1.003. The second component showed that the most important variables to explain the variation in the data of obesity were residence followed by religion and income. Similar finding was also noted in another study  [21] . The prevalence of overweight and obesity were significantly associated with age, religion, education, occupation, marital status, income and smoking habit. Similar results were also noted in separate studies  [5-9, 15, 20 -21] . This result was also similar as was observed in another study  [21] . The obesity is one of the risk factor of prevalence of non-communicable diseases  [NCD]  and it enhances the arterial hypertension, diabetes renal failure etc. [3] . This finding is similar to that observed in both home and abroad  [23] [24] [25] . This is for the in service, private or government, people.
paper_212	 Mathematical modeling of disease has been an indispensable tool in accounting for disease transmission dynamics as well as disease spread. Epidemiological disease models have been used to explain the dynamics of HIV/AIDS in the population from the early 1900s. Despite the spread of HIV/AIDS having been explored widely, not much literature is available on the Gillespie Algorithm based SIR model. This algorithm is able to give a statistically correct of the course of a disease with initial conditions to begin with and propensity functions to update the system. The purpose of this paper is to build on the concept of Gillespie's Algorithm based SIR models by developing a stochastic SIR model to simulate disease evolution in the population setting. This virus, being highly transmittable is analyzed and capturing how it is transmitted is crucial in attempting to model the disease. There are several channels in which the virus can be transmitted such as inter-species transmission, vector transmission, direct transmission or environmental transmission. HIV is transferred from one individual in three modes: through blood, sexual intercourse and mother-tochild. However, this is not always the case as there exist individual differences in the ability to transmit and acquire HIV that remain unexplained  [2] . The following year, 26 new cases of HIV were recorded from sex workers and the NAC was established. 170 of 1999. Among other countries in the world, Kenya is among the twenty two that account for 90% of expectant women living with HIV. This accounts for 4% of new pediatric infections worldwide. They applied these dynamic models to forecast the transmission of HIV for the Chinese population. The average number of partners was different at different ages in the HIV to AIDS cycle. The group most affected would be the 31-40 years group. In several concentrated epidemics, HIV prevalence estimates do not match reported cases and mortality estimates do not match reported deaths, even after adjusting. Furthermore, it provides inaccurate estimates where an epidemic has not gone beyond its peak  [6] . Despite the fact that a lot of research has been done on modeling disease trajectory, not much literature is available on the use of Gillespie based SIR models to simulate the trajectory of a disease in the population. Mathematical models generated as deterministic have been used in the past and they offer a lot to be studied and concluded from statistically. This will allow us to derive new insight from the analysis of the simulation of this SIR model. Several execution options have been suggested for the SIR model such as Gillespie's algorithm and agent-based models but they have not been extensively explored in literature. This paper will contribute and build on to the existing literature on modeling disease dynamics in the population with the model tested on HIV/AIDS data 1985-2018 to investigate if the simulated values would reflect results that are close to reality. In some instances, these deterministic models do not capture some model characteristics and this could lead to biases. They set the transmission and infection rates as invariant for all ages and this allowed the inclusion of an infectives class. The reliability of the simulated values would set the precedent for the valued to be predicted based on the model is also explored. The stochastic SIR model. Assuming that the resulting state is . A propensity function is the probability of one event occurring in the time interval ! , ". Continuous-time Markov chains are the basic tool for building discrete population epidemic models. The Markov property lets us specify a model by giving the transition probabilities-defined as rates-on a small interval between the compartments. Considering the fact that the propensity functions require to be in probability form, we explore this assumption further by defining and interpreting it. A Markov chain model is one where the probability of the next event depends on the probability of the present state. Continuoustime Markov chains are the basic tool for building discrete population epidemic models. The Markov property lets us specify a model by giving the transition probabilities-defined as rates-on a small interval between the compartments. The transition probabilities assigned are defined on an open interval (t, t + ), such that the probability an individual moves from the susceptible compartment to the infectives compartment is  [ 1 ] . The wait times between events can either assume an exponentially distributed wait time or the rate of flow between compartments can assume any of the following distributions depending on the results. Considering the hypotheses, 4 = If the mean and variance of the simulated and natural data are equal, the simulated mean does not fit the data. Curves produced are illustrated below. Mathematical modeling of disease trajectory using Gillespie based algorithms is yet to be explored extensively in literature. The simulated curves were found to resemble the data available in reality. Therefore, the implementation of a stochastic factor to an epidemiological model is a useful contribution to mathematical modeling. Recommendation for research would be to explore other variations of the SIR model such as SI, SEIR under Gillespie's algorithm. Furthermore, making parameter values time-varying under the Gillespie algorithm and comparing it with the version where parameters are invariant to see which performs better is another recommendation.
paper_214	 Objectives of decision-making process aimed at adopting the best solution from many possible alternatives. Among the methods most commonly used are decision analysis and mathematical programming  [1, 2] . Tabelelele decision highlights a possible alternative schematic characteristic information. Among the advantages are: support the simulation model provides a functional form of expression of the links between the phenomena studied. simulation models have a procedural nature, their solution involving the processing of experiments created within the system. data model can be used in the construction of real observations (numerical values) or knowledge. The solution offered is one spot that has no counterpart in the real system. always taking into account the unique factors of a problem specific results can not be transferred to other issues; results are difficult to interpret, being dependent on random factors; no matter how powerful your computer is, the optimal solution is difficult to obtain a model that has many equations and a significant number of parameters. Tested values of different variables and decision highlights the consequences of decisions on the result of values  [1, 3] . In the first case, the user is provided customized views of data stored by performing a diverse set of operations on transactional data. The outputs from the process of decision making, represented by analytical indicators reflecting the performance of the system analyzed variables results the evaluation criteria or implementation plans of the decisions  [2, 3] . The decident system uses a dialog interface with the key users of the company, enabling connectivity and communication between networks with different topologies and areas. After analyzing the results achieved and the objectives of their reporting, signaled differences and after identify problems it was reveal the need to take action. The information is selected factors that have caused the deviation from the desired result and appreciate the importance they have in context. The result of the information stage is a formal description of the problem identified the category to which it belongs and responsibilities involved. For choosing the solution which takes the results of the previous stages, the action is chosen according to the criterion of selection and decision-making model. Structural problems use mathematical formulas and analytical method to achieve an optimum solution. In order to improve efficiency the best solution search algorithms are used. Time and memory space limit searches, in most situations the decision maker stopping at the best of the tested solution to a certain moment. Implementation is the phase that involves the integration model chosen solution in context and simulating the real system. Assisting decision states that the decision is the responsibility of the user. It receives relevant and substantiated elements on activity in the real system and builds models for solving future decision making on the basis of current assessments.Of the foregoing that a decision support system provides a filtering of information provided to decision makers and indicates certain restrictions. The database contains no internal data, external data and personal data.Internal data consist from the current activities of the organization and operations of various functional departments image. Personal data is data that relates to the behavioral aspects of decisionmakers in making decisions. The data source, internal or external, data is extracted and managed by a management database. In most cases there is SGBS transactional relational data system and a management database for multidimensional data warehouses created. The data dictionary is a catalog of all data from the database. It is the component that differentiates interactive decision support systems to other systems. The models are domain-specific and models can be classified into strategic, tactical and operational models models. Tactical models are applied to the organizational subsystems and assist the user in taking decisions for allocation and management subsystem resources available  [3, 6] . The models are used currently in operational and transactional system that aims of the organization. Manage in a logical manner a variety of models to consistency of the data model and provides integration of application systems components maker. In building a data warehouse is based on the analysis of data. Extract information in order to obtain information for decision making. Design is the stage where the data warehouse model is chosen, depending on the complexity of the system real user requirements and data structure existing in the company (databases, Excel spreadsheets...) Building a data warehouse there are three models: type star, snowflake patterns type and constellation type models. Conceptual models are multidimensional and designed to organize data necessary decision-making process on issues. Facts associated table. The star is the type of aggregation criteria when codes are explained in type tables list. In the business area is crucial to store data into a warehouse such as a star modell and create many queries and reports so the decision maker can see the trends of the economical indicators and helps him to make a decision that can improve those numbers or it can be useful to implement a strategy for a further development  [1, 3] . By simply analysing these reports and graphs helps the decision maker to create different scenarious by changing target indicators and corelate with the economical indicators that can't be changed  [2, 5] . It is very important to store data into a warehouse model and then to create future strategies that can help a company to expand or to grow the profit in a realistic way based on complex analyses and alternatives.
paper_216	 The Black-Scholes model is a well-known model for hedging and pricing derivative securities. However, it exhibits some systematic biases or unrealistic assumptions like the log-normality of asset returns and constant volatility. The objective of this study is to value a European call option using a non-parametric model and a parametric model. The MSE and RMSE of Wavelet model is 0.208546 and 0.456669 respectively which is much lower than that of Black-Scholes model and therefore in conclusion, Wavelet model outperforms the other model. The study was carried out using simulated stock prices of 1024 observations. The financial contracts or instruments which derive their value from some other variables are called Derivatives. Derivatives are instruments whose value depend on an underlying asset. Equity, commodity, bond or currency, stocks, interest rate, exchange rate or any other financial variables of interest to the researcher could be the underlying asset. Derivatives includes; Forwards, futures, options and swaps. An option is a type of derivative that gives the option holder the right but not obligation to buy or sell an underlying security at a specified price, at any time on or before a given date in the future as agreed on. This study therefore prices a European option using two nonparametric methods and a parametric method. The Black Scholes model has been developed under some assumptions such as the risk-free rate and volatility of the returns are known and constant, the returns of the underlying security are normally distributed, markets are efficient (market movements cannot be predicted). The Black Scholes model therefore belongs to the parametric continuous time models with a closed form solution family. The implied risk neutral moment-generating functions (MGF) is estimated by the wavelet method. The following are some examples of these advantages; when there is presence of jumps in the underlying process MGF is more tractable, the option's obtained MGF is a continuous function, statistical moments of the asset distributions can be obtained using MGF and the risk neutral MGF can be used to estimate the out of sample options that has maturity dates which are different. The following are some of the applications of wavelet method in finance and economics as pointed out in  [6]  and  [7] ; Wavelets can be used in multi-scaling analysis. For example, analyzed the relationship between economic variables at different scale by using the wavelet method and they found out that over a different time horizon, the relationship changes  [8] . Employing the wavelet method to de-seasonalize prices of electricity  [9] . The second use of wavelet method is that, they can be used to de-noise raw data. The de-noising ability of wavelets was also recognized in  [7] . Evidence have been provided to support that wavelet based local linear approximation  [11] . European options can also be priced using the Shannon wavelet  [14] . Another example is the application of the wavelets in nonparametrically estimation of a diffusion function  [15] . Where;  formula_0 From the Black-Scholes formula above, we see that the strike price (K), risk free interest rate (r), time to maturity (T), the volatility (") and the current price of the stock (S0) are required. is the cumulative distribution function. Based on some general assumptions like independent and identically distribution (iid) for asset returns and that the moment generating function is defined well, the wavelet based pricing model can be expressed as follows; formula_1 Where, t is the current time, is the underlying asset price at time t, # $ is the time t price for a European call option written on asset, K is the strike price and T is the future maturity date. ʆ ! The underlying asset dynamics and investor expectation in option prices is captured by the MGF Θ !$ + of the logarithmic returns /0 7 . When T -t = 1, Θ (v) is the risk-neutral MGF for the rate of return per unit of time. This needs to be approximated by wavelets. This function also emulates the probability density function of asset returns. In this study Monte Carlo simulation was used to generate 1024 stock prices. These data is divided into three, In-The-Money options, At-The-Money options and Out-of -The money options. The focus on this study was on pricing of a European Call option using two models, one parametric model (BS Model) and one non-parametric models (Wavelet Pricing Model). In order to estimate the risk neutral Moment Generating Function of the underlying security from option prices, we applied the methodology of wavelet. The superiority of the wavelet method comes from the ability of the wavelets to estimate the risk neutral MGF. We recommend more further investigation on the nonparametric models since most of the studies have focused on the parametric model especially Black-Scholes model. A lot of focus should be put on the Wavelet model because it is a new method in the field of finance. Other complex options include; Bermuda options and exotic options. Moreover, hedging performance may be evaluated with respect to each of the two models.
paper_219	 This paper introduces a framework to determine data quality on enterprise networks for net-centric and net-ready initiatives as introduced by the US Department of Defense (DoD). Traditionally quality of data delivered to an enterprise user focuses on network performance, i.e. quality of service (QoS). The QoE is assessed for each of the framework's attributes using the best practices from survey statistics in sampling and estimation. The overall value of data quality on enterprise networks is decided using a minimax decision model consisting of the three attributes. The resultant minimax value correlates to the lowest performing attributes of the framework. The presented framework offers decision support tools to enable agencies to allocate limited resources towards improving the performance of their net-centric service offerings to the enterprise network. A fundamental shift in paradigm is required to ensure the stressors of conducting military operations are supported through data (of both high-quality and high-relevance) and, not burdened by attempts to manage its excess. The US Department of Defense (DoD) introduced the terms net-centric and net-ready to describe the mechanisms by which operators can search and discover information within the bounds of data. Successful policy implementation amongst other factors requires system analysis methods that assist acquisition agencies in targeting limited developmental resources to areas of greatest impact to the overall mission objectives. This paper introduces such a method in support of achieving maximum data quality for military enterprise networks: a quantitative mechanism by which the value of different net-ready implementation options can be evaluated and graded. Section 2 of this paper provides a brief overview of the origins of net-centric and net-ready policies. The NR KPP specifies the attributes required of data sharing Information Technology (IT) introduced into the net-centric operational environment: (1) IT must be able to support military operations (SMO), IT must be able to be entered and managed on the network (EMN), and (3) IT must effectively exchange information (EI)  [3] . Making data visible is achieved via deployment of discovery capabilities that access and search data asset catalogs and registries in the enterprise  [6] . Alignment can be achieved via direct negotiation or-more practically-via the adoption of commonly referenced standards such as those listed indicated by the global information grid (GIG) Technical Guidance Federation  [6] . To satisfy the attribute of support to military operations, IT deployed to the operational environment must support identifiable net-centric operational tasks and mission objectives  [3] . The performance of the IT must be quantifiable with threshold and objective values that are traceable to measures of effectiveness (MOEs)  [3] . To satisfy the attribute of entered and managed on the network, the IT connections to external networks required to perform net-centric operational tasks must be identified  [3] . The specific data elements and assets exchanged with external networks as part of executing net-centric operational tasks are specified by the exchange information attribute  [3] . Fig. 1  illustrates the mapping relationship for each enterprise attribute to various DoD objectives of net-centric, net-ready and cyber-security. The subjective measure of overall user-satisfaction of a service or application is referred to as quality of experience (QoE)  [9] . The question arises how to get the users' opinions. But the arithmetic mean assumes that all user groups are of equal size which could lead to biased estimates of the MOS. One possible split is to consider groups of users based on their shared mission or objectives. where n ≤ N and s = {1, 2,…, n}. Where it is not easy to design a stratified sampling plan a stratification post collection of n samples without replacement can be constructed. QDS is a subjective rating from the perspective of the end user  [11] . But cost and time to use panels of experts to assess MOS has resulted in seeking alternative approaches. One alternative approach is using models formed from objective perceptual measurements to predict subjective ratings of QoE  [12] . In the video standards of  [14]  and  [15]  they present a series of methods of modelling with objective measurements to predict the subjective ratings. The reason to use the full reference is to capture environmental conditions resulting in the most accurate predictions of ratings. The prevailing method for assessing the quality of a still image is based on the ability to perform certain levels of object recognition with scoring defined by the national imagery interpretability rating scale (NIIRS)  [17] . QoS is in essence an engineering optimization problem where the objective is to maximize users' satisfaction while minimizing cost of delivery of the supporting network services. User satisfaction is traditionally associated with network metrics: delay, jitter, throughput, packet loss, order preservation. And the service level agreement (SLA) is the users' agreement with network provider(s) on acceptable ranges for the metrics. The most widely deployed QoS architecture used to deliver a SLA on an enterprise IP/MPLS network is referred to as a differentiated service (Diffserv)  [11] . These models of QoE based on QoS objective measures are often referred to as "QoE\QoS" correlators  [21]  and  [12] . For both models  [21]  showed QoE had an exponential model in terms of their QoS objective metric(s). 1 ). A scalar measure is required to support evaluating the degree of relevance on the enterprise performance. 2 . QoE) that the data would provide to the consumer assuming perfect discovery and delivery. overhead, side-view, rear-view, distant, near). target while in port, target while in open ocean, target when first detected, target after engagement). The processing of tagged relevance involves the review and analysis of the data product to assess its key features followed by the application of specific metadata values. The goal of the tagging and discovery process is to connect highly relevant data with an authorized consumer. The measure of discovered data relevance is an indication of how well the enterprise system enables a consumer to differentiate the data product offerings accessible via the network according to the level of relevance and value to the mission objectives. To support a high level of discovered relevance, The taxonomy available to the consumer must be sufficient to explicitly discriminate the desired data features from the undesired. The processing element of discovered relevance is a measure of how well the search methods of the system match the descriptive words of the consumer (i.e. metadata) with those of the producer. Just as the performance of tagged relevance is improved by an understanding of the consumer by the producer, discovered relevance benefits from awareness that the consumer has of the full range of data offerings of the enterprise and how they may be described. Without a strong mutual awareness, a consumer may prematurely end their discovery process with the first piece of seemingly relevant data believing that it is the best or perhaps the only product available to them. We first discuss the design philosophy for the overall assessment of the data quality for the enterprise: (1) The enterprise data quality is determined by the attribute with the lowest user satisfaction (QoE). Alternatively the data relevance user satisfaction to match the other attributes can be improved. Formal minimax definition  [24]  is formula_4 where a(i) denotes the actions of the i-th player of n players, a(-i) is actions of all other players except the i-th, and v i is the value function of player i. Player one has three action options U, D, N to choose from. The minimax model of Wald produces a decision with an outcome (payoff value) which is the worst chosen amongst the best outcomes of all decisions. formula_5 Thus, for each d in D, starting with d=QDS, a state vector s max (d) is found that produces max s in S(d) f d (s) where f d (s) is a discrete function formed by a predictive model for QoE or in the absence of a model formed from direct user sampled MOS value calculated using (3), i.e. f d (s) = u HT . The x-axis of the chart in  Fig. This paper introduced a framework to determine data quality on enterprise networks for DoD net-centric and net-ready initiatives.
paper_241	 So the use of transformers protection in electric power systems is very crucial and critical as they are required for efficient transportation of electricity to consumers for long term. In A. These faults, as in the case of phase to phase or phase ground faults could cause an imbalance of phase current (i.e. differential current) and can be prevented using differential protection and microcontroller based relay protection. The Differential current protection scheme is based on the principle that the input power to the transformer under normal condition is equal to the output power and is concerned with having a differential or balanced current between the primary and secondary side which would be digitally displayed  [2] . The protection techniques employed differential relay mechanism with Arduino. [4] Ochieng designed a microcontroller based power transformer protection system which uses a current sensor as the interfacing instrument between the current transformer and the pic16f690 microcontroller. The current sensor acs712x series was used in the project as the interfacing instrument between the power transformer and the pic16f690 microcontroller. A power transformer functions as a node to connect two different voltage levels  [3] . The fundamental idea of the differential current protection scheme is to provide protection to the transformer if any fault occurs in the protected zone that will cause an imbalance in the differential currents. The output power in a transformer is equal to that of the input power hence, for differential current protection of the current transformers reduces the currents at the primary and secondary sides to a measureable value and in such a way that they are equal  [5] . When there is an occurrence of internal fault an imbalanced or non-zero differential current will flow through the operating coil of the differential relay which would be substituted with a microcontroller that sends a trip signal to the relays needed to open the circuit. The differential protection scheme is concerned with the faults that arise from associated difficulties such as the magnetizing inrush current and saturation. The power transformer primary and secondary currents are measured by the current transformers, which are to be protected by the Arduino and the circuit breaker. This 12V is regulated to 5V (using L7805CV voltage regulator) which is the required voltage for the Arduino. The current are first connected in series and then in parallel to the secondary side of the step-up transformer to display. The LCD display is used to display the differential current values of the transformer and to display the voltage and current level of the system as well. For the load with both the 200W and 60W bulbs the current values and difference were larger than with each connected separately. At No load the secondary current is very close to zero as a result of open circuit at the secondary. The microcontroller based relay is a technological advancement or development as compared to the use of conventional relays which improves the efficiency and reliability of the system which could be beneficial to both the society and its economic growth.
paper_251	 In this paper we propose a concept of multi agent based batch scheduling and monitoring in cloud computing environment, where the number of agent are more than or equal to two with reducing the complexity of accessing and responding time. Heterogeneity in the computational requirement, dynamic choice and infrequent usages types of resources of the users in modern era has main challenge for service provider (application developer and hardware manufacturer). Various devices (Computing) and application has been developed and developing to fulfill the common users need. Hence demand of users is heterogeneous in nature so that varieties of application (hardware & software) have been developed to achieve the highest user satisfaction. Specialization has more promising than generalization due to expertise in specific job/function but it also has dark sides. Various requirements require numerous specialized devices (CPU, storage etc.) Purchasing or licensing of all such required items (devices & applications) is not feasible to the organization or individuals in terms of the cost and installation. Modern era is reflection of human creative thinking and application of optimize solution for the problems mapped and simulated into the machines using technological skills and advancement on it. Cloud computing is another example of technological advancement which offers dynamic provisioning of the utility on rent basis to the subscriber. A cloud service provider has deployed and manages to sufficient number of resource that has been shared to the entire subscriber as per the load requirement of the individuals. To achieve this task cloud service provider required highly efficient scheduling approach and the proper monitoring of the services provisioned or will be provision to subscriber. Outcome the results shows that the provisioning of SaaS (Software as a service) and its monitoring using agent has gives better result which is more efficient than existing approach. Integration of the agent in the propose system provides the cost effective and reliable with dynamic pace, solution for efficient scheduling (elasticity of the resource and services) and proposer monitoring of the cloud computing systems. They are following with respective functionality in the proposed system 1. 3. i.e. monitoring and scheduling using software agent New Relic service has been subscribed. Propose system has surveyed and identified the problem domain that must be addressed in context of the cloud computing and consequently present the idea of agent integration. Better Provisioning of the SaaS 4. The main lacking point in the article  [1]  and  [2]  is validation of the proposed mechanism. Additionally the requirements for such fast provisioning of the cloud has been discuss in the recent year in the article  [3] . To Evaluate and delivered the cloud computing services (SaaS) using agent (for better and fast delivery) using public cloud such as "New Relic and cloud bees". 4. Summary MAP of the SaaS using Agent -  Figure 1  has shows the overall MAP of the proposed SaaS with the help of the customized new relic agent. Availability 4. All the matrices of the performance checking has been same meaning as our proposed system generated like 1. Table 2  shows the response time of the deployed SaaS obtained results and has been compared with existing agent based method (in seconds) . Average user satisfaction is same as to availability and scalability of the proposed system. Table 3  shows the user satisfaction of the deployed SaaS obtained results and has been compared with existing malty agent based method  Rather than performing analytical calculation, proposed system has been compared with the author's MABOCCF and NMABOCCF method. Our response time and scalability availability and CPU show that the obtained result has closest to 100 (97-99%). Hence proposed system performs better than  [2]  and any other methods as shown in figure 3. This paper proposed and developed an agent based enhanced method for better scheduling (for resource like CPU, memory etc. Analytical analysis is to collect statistics to check the required number of resources needs or used and provides dynamic indication to better elasticity achievement. Proposed agent based methods obtained result has been found satisfactory and performs better than existing available solution. Develop a security perimeter based on anomaly detection using Application Process Management by integrating the mobile Agent on them for the cloud computing paradigm.
paper_272	 K is Boltzmann's constant (1.37x10-23 Joules/K) The constant A is approximately 6x10 5 for most metals According to the Harris model and classification who was classified "the positive ions initiated from ionization process in an Anode plate and generates joule heating to a cathode plate surface during switching process"; i. Ionization zone ii. The total above processing time was measured by Harris approximately (20-250µs)  [15] . If the high frequency accompanying re-ignitions and voltage escalation in one phase couple into other two phases, the process of virtual current chopping can occur  [6] . Virtual current chopping involves the load current in the other two phases being forced to zero by superimposed highfrequency re ignition coupled current. It is important to appreciate that while current chopping and voltage escalation can occur in a single -phase circuit, virtual current chopping is specifically a 3-phase characteristic: the effects of normal current chopping, multiple re ignition and over voltage escalation in one phase can generate surge over voltages in the second and third phases. A high frequency current governed by the circuit parameters flows. Although the current in the vacuum interrupter can chop to zero almost instantaneously (fraction of a microsecond), the current in the load inductance-3ph coils in the power transformer cannot attain zero value instantaneously. The second transient ends up in a negative loop of current that changes the polarity to positive at just about 480 µs time scale indicates in the figure. formula_0 From equation  (4)  express in dimensionless from the current in the inductor of any parallel RLC circuit, with any degree of damping. So that a family of generalized curves can be drawn from equation  4 for different values of η with dimensionless quantity -t`, as abscissa. This has been done in Figure. By calculating the inductor current in parallel RLC circuit under conditions of a subsidence transient, but have a far wider application. Suppose Vo =13.8√2 KV So. Zo = √L/C = 5X104 ohms η. Further point has also to be consider, namely the frequency f(o) was measured. The electrostatic charges -static charged -parasitism are almost depleted in the a mount current begin to decrease in the same time of first peak value which generates chopping currents, moreover the formula of the transition processing rate of du/dt or di/dt have to be considered. 1. The ionization electroplates zone will be evaluated between (20us -250 us) respectively. 2. The real factor for switching transition rating interrupter depends on the values du/dt & di/dt that will be ranging from (700us -1500us) for VD4. 3. Chopping current times (400us-800us). Transient over-voltages (450us -480us-500us). 5. 6. 7.
paper_294	 It was therefore, recommended that concerned policy makers should make policies that will allow public libraries to acquire store and preserve audiovisual materials on various indigenous knowledge for development to ensue; and for posterity; and that governments and other stakeholders should ensure that the public library sector is adequately funded and manpower is adequately provided as well. Abrigo and Abrigo (2010)  assert that, these media are jealously guarded and relayed, shown and played back for the younger generations. The Tiv had different management strategies of postharvest losses of cassava (Akom or Alogo) that includes dried cassava chips (kpor), imyorun or imorun (gari) processing, fermented cassava (akpu) and production of base (mtuhem) among others, which were potent managing postharvest losses of cassava. Public libraries are supposed to be conscious of ensuring that these postharvest losses management strategies are captured, acquired stored in audiovisual format. Essentially, all these were part of their management strategies for postharvest losses of cassava. In addition, these indigenous postharvest losses management strategies seem to be facing total extinction due to lack of documentation and storage of audiovisual materials on them. Identify Tiv management strategies of postharvest losses of cassava 2. 1. What are the Tiv management strategies for postharvest losses of cassava? 2. 3. Public library is a library that is established and managed with public funds. In other words, it is called an omnibus for the single fact that its users and readership is not restricted. Public libraries serve as intellectual centers as well as recreational centers. Information processing and retrieval are the core aims and objectives of librarianship, which warrants adequate coverage at all levels of education and above all service to all users both learned and non-learned. Apparently, in order to achieve this mission, public libraries preserve a variety of information materials including print materials such as books, monographs, serials and periodicals; and non-print materials such as audio or audiovisual materials such as cassettes, microfiche and films among others. This is their most crucial function of all. Importantly, among the various formats of information materials the public library acquires, audiovisual materials are most suitable for meeting the needs of users in Africa particularly in Benue State. This is in line with Agber and Mngutyô (2013); Agber, Ugbagir, Mngutyo and Amaakaven (2014) who reiterated that for developing countries such as Nigeria, audio and audio-visual media are appropriate information formats for capturing information for the people. Hence these farmers cannot read nor write, the print materials acquired by the public library cannot meet their information needs. Therefore, it will be appropriate only if the public library will acquire audiovisual materials, which will be most suitable in meeting the needs of the indigenous Benue farmers. Moreover, other varieties of cassava in Tiv land of Benue State include: Akpu from which akpu are processed, Yakpe, Genyi, Wari and Gyo-Akom among others. Cassava plays an essential food security role because its matured edible roots can be left in the ground for up to 36 months. The variety of foods that are made from the roots and the nutritious leaves are reasons why cassava cultivation is expanding worldwide  (Lebot, 2009) . Fresh cassava has a very short postharvest storage life  (Karuri, Mbugua, Karugia, Wanda & Jagwe, 2001 ). The food need not be removed any great distance from the harvest site, but it must be separated from the medium that produced it by a deliberate human act with the intention of starting it on its way to the table. Essentially, cassava postharvest losses can be defined as both the physical losses such as weight and quality suffered during postharvest handling operations of cassava and the loss of opportunities as a result of inability of producers to access markets or only lower value markets. These include: 1. Peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips) 2. Slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base) and 5. These are the Benue state Library Board Headquarters in Makurdi and its branches in Gboko, Vandeikya, Katsina-Ala, Gungur, Aliade and Adikpo with about 33 workers  (Mngutyô & Amaakaven, 2013) . The sample size of 680 out of the population of 20,000 was drawn using the sample size table, (Emaikwu, 2015). Section A of the questionnaire contained respondents' bio-data, which included sex and occupation. Section B consisted of 7 variables of forms or Tiv management strategies for postharvest losses of cassava. The 19 item questionnaire adapted a 4 point rating scale and respondents were asked to respond by ticking the correct or applicable responses (SA) strongly agree, (A) agree, (D) disagree and (SD) strongly disagree. The collected data were analyzed using mean and standard deviation. Apparently, any item of the instrument whose mean rating scores was 2.50 and above was considered significant and any item with the mean rating scores below 2.50 was not considered significant. What are the Tiv management strategies for postharvest losses of cassava? To answer the research question, data were collected on Tiv management strategies for postharvest losses of cassava. The collected data were analyzed and presented in  Table 1 . From  Table 1 , it can be seen that the mean rating scores of all the items are above 2.50. In order to answer the research question, data were collected relating to the research question, analyzed and presented in  Table 2 . Table 2  shows the mean rating scores of all the items are below 2.50, which implies that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). To answer the research question, data were collected relating to challenges faced by public libraries on acquisition and storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. The collected data were analyzed and presented in  Table 3 . Table 3 , it can be seen that the mean rating scores of all the items are above 2.50. This implies that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. 1. Table 2 , findings of the study revealed that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). 3. Findings from the study based on the result in  Table 3  showed that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. The research discovered that the Tiv people had various strategies they adopted in managing postharvest losses of cassava. Therefore, it was recommended that concerned policy makers should make policies that will allow public libraries to acquire store and preserve audiovisual materials on various indigenous knowledge for development to ensue; and for posterity. Management of public libraries should also ensure that initiatives on going from one community to another to record and shot films on indigenous knowledge are in place. The study concludes that public library must make it a point of duty to start acquiring, storing and preserving information materials on indigenous knowledge, particularly in audiovisual format, since this is the only way the library will be useful to the host communities.
paper_298	 The study investigated the awareness and use of library catalogue for accessing information resources by users of Federal University of Kashere (FUK) Library. Majority of the respondents used the library for research work rather than for leisure and recreational purposes. University library provides well stocked information resources and trained personnel to organize available information materials and assist faculty members and student users in the retrieval and use of these resources. The library as a service oriented organization must provide the bibliographic resources and services channeled towards the fulfillment of its parent institution's goals and objectives. Library catalogue is considered as an interface of information retrieval system which assists information searchers to access resources of libraries using several access points. Like the card catalogues, sheaf catalogues, books, machine readable catalogues (MARCs) and online public access catalogues (OPACs). Following advancement in ICT and subsequent development of Online Public Access Catalogue (OPAC), the traditional concept of access to library resources which many scholars identified to be prone to numerous challenges has changed. OPAC has brought a changing relationship between libraries and their users in the changing ICT environment. Overall majority of respondents 80% satisfied with OPAC functionality  [4] . Students' skills in search catalogue can be created through library tours, orientations, seminars, lectures, library staff, handbooks and use of library studies. The purpose of using OPAC majority of the respondents 63.2% stated that they use OPAC to know the availability of required document. Kumar revealed that 26.99% respondents use the OPAC fortnightly and 22.84% weekly. Aguolu andAguolu observed that students use catalogue mostly for educational purposes and have really helped in conducting and disseminating information resources in the library  [10] . b. To find out methods employ by students to consult library catalogue to search for information resources. c. To ascertain the extent of use of the library catalogue by students to access information resources. The stratification sampling technique was employed to sample the entire registered population of undergraduate library users from each level (100-500) in Federal University of Kashere respectively. The gender breakdown is presented on  Table 1 . It shows that majority of the respondents 188 (69%) were male, followed by females 84 (31%). This shows that majority of library users in the Federal University of Kashere Library were male. Librarians in discharging their responsibilities should inform library users by communicating the availability of new technology and the way it operates to them. shows that majority of the respondents 178 (68%) were aware of the card catalogues as a access point / retrieval tool for searching for information resources in the library. With 64% response rate it is obvious that the respondents are aware of the existence of the card catalogues as a retrieval tool for searching for information materials. 26 (10%) of the respondents were aware of the existence of OPAC as a retrieval tools in the library. Their responses were presented in the  Table 4  below. The result proves that the management of the university library provides awareness opportunity to users for retrieval and utilization of information resources, except that they need to put more emphasis during library orientation to add to existing ones. Again, university website could be used to provide access to library resources especially e-resources, therefore library management need to make it as one of the major access points and retrieval of information resources. 24 (9%) of the respondents never used the catalogue but they were aware of it existence in the library as indicated on table 2 above. This shows that these few users preferred browsing the shelves to search for information resources. Total  F  %  F  %  F  %  F  %  F  %  F  %  100  10  20  0  0  6  12  28  56  6  12  0  0  50  200  10  23  6  14  10  23  16  36  2  5  0  0  44  300  10  19  6  11  14  26  12  22  6  11  6  11  54  400  14  25  0  0  36  64  6  11  0  0  0  0  56  500  2  3  0  0  54  93  2  3  0  0  0  0  58  Total  46  18  12  5  120  46  64  24  14  5  6  2  262  The finding indicated poor use of OPAC 12 (5%) at the university library. It could be concluded that majority of the respondents used the library for research work rather than for leisure and recreational purposes. This agrees with the findings of Oghenekaro that 94 (37.5%) of the respondents used it for research purpose, 66 (26.3) used it for enquiry, while others used it for bibliographic record supply, document delivery and others  [1] . The results are presented in  Table 8 . Higher number of the respondents indicated poor computer skills hindering navigation when searching OPAC 96 (37%) and inability to locate materials on the shelves, indicated in the catalogue as being available 58 (22%) were the challenges affecting the use of catalogue as an information retrieval tools. This finding corroborates with that of Ogbole & Morayo which revealed that (41.1%) of the respondents rarely use OPAC. The difficult interface of OPAC poses a big challenge to undergraduates who are the target users of OPAC in any academic library. The study revealed that majority of the university library users were male. It is disturbing to discover from the study, that most of the respondents were aware of the card catalogues as access and retrieval tool for searching for information resources in the library. It could be concluded that majority of the respondents used the library for research work rather than for leisure and recreational purposes. The library management should organize a periodic user education programmes for the undergraduate users. This will enable them gain the needed skills to use the library catalogue maximally when searching for information resources. Because most challenges associated with the use of the OPAC often have to do with lack of ICT skills on how to make use of it.
paper_305	 Mitigation of credit risk is a key aspect of portfolio management in any financial institution. The performance of loan contracts in good standing guarantees profitability and stability of a bank. Further, the classifier labels every classification instance with a level of confidence value. This makes credit control one of the key concerns in a bank's financial management  [1] . Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced  [4] [5] [6] . For many classification algorithms, this innovative strategy results in dramatic improvements in performance  [8] . One such algorithm that effectively addresses these issues is the LogitBoost Meta classifier -based on the log of the odds ratio for the dependent variable  [7, 8] . In the quest to find solutions to loan approval problem  [9] , the authors proposed a neural network banking model for the Jordanian banks. There have been various other attempts to deal with the loan appraisal problem using various techniques  [10]  to varied degrees of success. The reported technique derives from the intuitive understanding that instead of putting all the effort on finding highly accurate base classifiers, it becomes sufficient or even desirable to use a set of weaker hypotheses. In majority voting, to predict the class of a new item, each base classifier got to vote for either the 'accept' or the 'reject' class. It can be proven (as discussed here-under), that under the assumption that all individual classifiers have the same prediction rate and that the distribution of the data correctly classified by each base classifier is independent and random, this is the best possible strategy. Logistic regression models these probabilities using linear functions in x while at the same time ensuring they sum to one and remain in [0,1]. The model was specified in terms of K −1 log-odds that separate each class from the base class K. a) With K attributes , there are K different decision stumps to choose from b) At each stage of boosting i. given reweighted data from previous stage ii. Repeat until you have T classifiers selected The development platform used for this project mainly included the following open source software products: The Java Development Kit (JDK) which is a Sun Microsystems product released under the GNU General Public License (GPL) was one of the packages used especially for the compilation of the source files. i. Separate data into fixed number of partitions (or folds) ii. Select the first fold for testing, whilst the remaining folds are used for training. iii. iv. vi. This strategy relies on two separate files, one for training and the other for testing. This strategy is similar to the use of two files as discussed earlier but relies on the learner to automatically partition a given data set into two given a split percentage Options: -F -R -I 15 Number of performed iterations: 15 Time taken to build model: 0.06 seconds Time taken to test model on training data: 0.01 seconds The results were interpreted along the following parameters for all the various training and testing strategies. The accuracy returned by the training set is 19 correctly classified instances out of 20 instances. This gives an accuracy of 19/20=95%  Class =Accept: The number of correctly classified instances is 12 and that of instances classified as belong to the class is 13. This gives a precision value of 12/13=0.92 Class =Reject: The number of correctly classified instances is 7 and that of instances classified as belong to the class is 7. This gives a precision value of 7/7=1 Class =Accept: The number of correctly classified instances is 12 and the number of instances belonging to the class is 12. This gives a recall value of 12/12=1 Class =Reject: The number of correctly classified instances is 7 and the number of instances belonging to the class is 8. The ROC graph is a plot of two measures: Sensitivity: The probability of true classifications given true instances i.e. P(true | true) calculated as a/a+b from a standard confusion matrix 1-Specificity: The probability of true classifications given false instances i.e. P(true | false) calculated as 1-d/c+d The ROC area has the following indicators: . 1.0. 0.9. Excellent prediction . 0.8. Good prediction . 0.7. 0.6. 0.5. Random prediction . Indicates something is wrong with the classifier The ROC produced for the described strategy was as shown in figure 2.The ROC graph is regular with an area of 0.96. The value converted to 1 decimal place, these values indicate a perfect classification After a successful implementation of the stated system, the following were the key outcomes:  Three options were investigated for training the algorithm namely: a) The use of single file both for training and testing the model through stratified cross validation. The model accuracy using this procedure was 86.86% making it a fairly reliable strategy b) The use of separate training and testing data sets returned an accuracy of 95% making it a relatively better strategy c) The use of a ratio to determine the size of the training and testing files from one data set returned an accuracy of 88.64% Therefore, it implies from these findings that the use of separate files for training and testing of the model returns the best model accuracy and hence should be adopted. The trained model was subjected to 20 instances of unclassified data which had been carefully selected from a portion of the training and through analysis returned 19 correctly classified instances resulting in a predictive accuracy of 95%  Three suggestions are likely improve the model and hence the predictive accuracy of the learner: The training and testing procedures can be done severally with different input parameters and file sizes to settle on the most effective set for different learning processes. A cost matrix can be fined as part of the training procedure that penalizes wrong classifications especially the true negatives for this study. This is a good basis for manually investigating such cases whose levels of confidence go below a certain threshold.
paper_310	 Android application based smart phones are becoming each time more powerful and equipped with several accessories that are useful for Robots. We derived simple solutions to provide a framework for building robots with very low cost but with high computation and sensing capabilities provided by the smart phone that is used as a control device. The data received by the Bluetooth module from Android smart phone is fed as input to the controller. Related reference articles implementing wireless control of robots have been studied as mentioned in  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] . f) Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. The system consists of following parts: a) Arduino UNO (ATMEGA 328P) b) Bluetooth module (HC-05) c) Smart phone d) Motor driver (l293D) e) Arduino software (version 1.8.1) The basic building blocks of the project have been described below: Microcontroller will act as the brain of the robot. The Bluetooth module will act as an interface between Smartphone and microcontroller. Generally our transmitter will be smart-phone and receiver will be Bluetooth module (  Figure  2 ). Bluetooth module will give the commands given by smart-phone to the microcontroller. It sends the data to microcontroller through Bluetooth module. It also helps to send the instruction of forward, backward, left, right to the microcontroller. Here we the Bluetooth RC Controller application (  Figure 3 ) as the operating remote of this system. The advantage of this project is that the application software designed for android phones is kept simple but attractive with all necessary built-in functions. It is also interfaced with the microcontroller  (Figure 4 (a) ) and with circuit connections  (Figure 4 (b) ). Here we use programming language 'C' for coding. In android application when we press a button, a corresponding signal is sent through the Bluetooth to Bluetooth module (HC-05) which is connected with the Arduino board. When signal data arrives the Arduino the pin which corresponds to the particular input is set to high. There are two steps of the programming. As seen from the  Figure 6 . The Bluetooth module receives the signal sent from an android smart-phone, where the application software coded in C language is installed. The microcontroller, thereby, sends instructions, which when executed, helps in functioning of the motor driver. The movement and functioning of the motor can be controlled by using the android based application software. Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. In this method user must be present within in range (< 15 meters) to control the system. In future we would try to extend the range using Internet of Things (IoT)  [12] . When user sends any data to the Arduino board then the corresponding pin of Arduino goes to high state and switches the motor driver ic in the on mode. The corresponding motor moves as per the input data. Here in this project the user (android application) is the input section. This device is connected with the Arduino board (microcontroller section) by the means wirelessly i.e. Bluetooth module. The system can now be connected with the motors (output section) to be controlled via wireless connectivity. These commands help the microcontroller to interface with the Bluetooth module HC-05 and also with the motor driver IC L293D. Here the Bluetooth module act as a receiver which receives the instruction from the smart phone (remote or transmitter). Then the microcontroller decides the operation for the instruction which is coming from the smart phone. When any input is given then the motors moves as per the preloaded functions in the microcontroller. In future we can interface sensors to this robot so that it can monitor some parameters and we can improve the efficiency using Internet of Things (IoT) technology.
paper_333	 The diagnosis of diseases on the plant is a very important to provide large quantity and good qualitative agricultural products. Enset is an important food crops produced in Southern parts of the Ethiopia with great role in food security. There are several issues and diseases which try to decline the yield with quality. The aim of this study is to design a model for Enset diseases diagnosis using Image processing and Multiclass SVM techniques. This study presented a general process model to classify a given Enset leaf image as normal or infected. Bacterial Wilt and Fusarium Wilt disease and collected 430 Enset leaf images from Areka agricultural research center and some selected areas in SNNPR. It is also the sector that is given an overriding focus in the government's plan for growth of the economy as a whole. This is commonly known as false banana, the Ethiopian banana or the Abyssinian banana. Cheesman. There are several issues and diseases which tries to decline the yield with quality. Particularly, diagnosis of potential diseases on Ethiopian banana is based on traditional ways and due to limited research attention given to Enset crop production. There is a potential need for technology supported alternative systems to support the manual identification of Enset crop diseases so as to optimize the accuracy for remedial action. If the Enset crop diseases can be minimized then definitely the production will significantly be increased to contribute to the country's economy towards a new boost. The general field of image processing was started before a year and now it is used in different areas. Experimental results are reported in Section IV. The feature of normal and diseased Enset image features was extracted to train kernel support vector machine. As a result a defined Enset image feature repositories was created. Figure 2  shows the architecture of the proposed system  A. Dataset Partitioning The images of the dataset are collected from different parts of southern Ethiopia. From all those disease category a total of 460 Enset leaf images are collected from which 368 is used for training and 92 images are used for testing. B. From the total dataset 20% is used for testing and the distribution of each disease category is shown in table 2. Different results were found by using different multiclass support vector machine classifiers. In order to measure the performance of the classifiers K-fold (in this case k is 10) cross validation method was used. From the figure it is shown that the RBF kernel classifier has classified the given enset image in its correct class category by 94.04% and 92.44% respectively. If there is a shortage of experts in the area of those disease identification the system is going to be a replacement for the experts. Better results is achieved if the system is used by large number of dataset.
paper_389	 This paper systematically describes the definition, model structure, parameter estimation and corpus selection of the conditional random field model, and applies the conditional random field to the Chinese word segmentation and the Chinese word segmentation method. In this paper, a large number of experiments have been carried out using conditional random fields. The experimental corpus has been tested by Changjiang Daily for many years. Experiments are carried out to analyze the influence of the choice of conditional random field model parameters and the selection of Chinese character annotation sets on the experimental results. Peng F establishes a Chinese character segmentation model based on CRF. In addition to using some common features, But also used a lot of domain knowledge. Common Chinese character tagging method is based on the Chinese characters appear in the words of different locations marked different labels. Enter the sentence as "This is Wuhan." The feature that appears on each node is then calculated, using the feature weights to compute the most probable of all paths from "BOS" to "EOS". According to the meaning of "O", "B" and "I", the rules are summarized as follows: 1, the sentence of the first Chinese character tag can not be I, the last character of the mark can not be B. Because the mark B must be followed by a mark and only the mark I, and must be marked in front of the mark I, and marked as B or I. Finally, according to the mark of each Chinese character, the mark result is "OOBIO", so the result is "这 / 是 / 武汉 /." The probability of the i-character of the position as the prefix: pY> 95% 昨狭抨第竞耽 3 Z The probability of the i-character as the suffix: pZ> 95% 丸役袄侣丸瞰 4 R The probability of the i-character individual word of the position: 85% <= pR <= 95% 吧枚刘磊躺却 5 U Position of the i-character as a prefix of the probability: formula_0 The probability of the i-character as the suffix: 85% <= pV<= 95% 貌型胁帘午岸 7 D The probability of the i-character position of the individual word: pD <= 5% 言辽改信申仪 8 E The probability of the i-character of the position as the prefix: pE <= 5% 业络王场姆杨 9 F The probability of the i-character as the suffix: pF<= 5% 增隐晋香浓伊 In order to deal with the long-distance information, this paper takes the context distance as 2. The probabilities of the position probabilities in  Table 3  are extracted from the training corpus and the probability of each position is calculated according to the following probability formula: P (individual word) = number of occurrences of the individual idiom of the word / total number of occurrences of the word * 100% (1) P (prefix) = number of occurrences of the word as a prefix / total number of occurrences of the word * 100% (2) P (suffix) = number of occurrences of the word as a suffix / total number of occurrences of the word * 100%  3 After many experiments and comparisons, the probability of more than 85% or probability of less than 5% of the word as a location probability feature, but also on the selected word for some filtering, the elimination of some unnecessary words, such as At the same time, select the probability of greater than or equal to 85% of the word is subdivided into the probability of 85% <p <95% and p> 95% of the two sets. Appropriate increase of p> 95% of the number of occurrences is in order to improve the characteristics of the sample expectations, and achieved good word effect. Using the CRF model generated by the training, the test corpus is tested and a prediction result is obtained. The main corpus used in this paper is the training corpus and test corpus of Changjiang Daily. The coding method is GB code. The corpus content mainly comes from newspaper news. The format consists of a sentence segment consisting of words marked with spaces. The formula is as follows: Correct rate P = number of words correctly recognized / total number of system output words * 100% Recall rate IP correctly identify the number of words / test words in the total number of words * 100% F value F=*P*R/(P+R)*100％ The speed of word segmentation is another important index of word segmentation performance. The query speed of the word dictionary depends on the organization structure of the dictionary. The results of "+ feature template 3" model are obviously better than that of "+ feature template 2" model, that is, under the condition of adding feature template 3, F-score is 4.4% higher than that of feature template 3, Played a better effect. The experimental results show that conditional random field is an efficient segmentation method. This chapter first briefly introduces the CRF tools, experiment corpus and standard of experimental evaluation in Chinese word segmentation experiments. And then use these tools and the corpus carried out a number of experiments. The experiment not only demonstrates the influence of the choice of conditional random field model parameters and Chinese character annotation set on the experimental results, but also verifies the validity of the new features and the feasibility of the new method. As the Yangtze River Daily corpus is from the newspaper news, for the news corpus in the special format, such as title, poetry, weather forecasting, etc., to our model training has a certain impact, so if the corpus in the handling of these disturbances, Our model should have better performance.
paper_391	 Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. But, in the class attribute, it is 0.72. As a conclusion, the best algorithm based on the TT vaccination data is multilayer perceptron classifier with an accuracy of 67.28% and the total time taken to build the model is at 0.01 seconds. Multilayer perceptron classifier has the lowest average error at 32.72% compared to others. These results suggest that among the machine learning algorithm tested, multilayer perceptron classifier has the potential to significantly improve the conventional classification methods for use in EDHS data of Tetanus toxoid. Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. And the major cause of early infant deaths in many developing countries is often due to failure to observe hygienic procedures during delivery. WHO estimates that only 5% of Neonatal Tetanus (NNT) cases are reported, even from countries with well-developed surveillance systems  [2] . As of December 2012, maternal and neonatal tetanus persist as a public health problem in 30 countries, mainly in Africa and Asia  [2] . In sub-Saharan Africa, up to an estimated 70,000 newborns die each year in the first four weeks of life due to neonatal tetanus  [5] . Ethiopia's Expanded Program on Immunization (EPI) started in 1980 and remains the single most important component of primary health care supported by the Ministry of Health. The vaccine to prevent Maternal Neonatal Tetanus (MNT) introduced as part of routine immunization programs in over 100 countries by the end of 2011. This problem caused the loss of time and effort in the healthcare system environment and spending lots of efforts and costs without having evidence-based information for planning and intervention. The EDHS 2011 was conducted by the Central Statistical Agency (CSA) in collaboration with Measure DHS and ICF International under the auspices of the Ethiopian Ministry of Health (EMoH). The data used in this investigation are the TT immunization data. csv" file formats and stored as an ". The classification has numerous applications, including fraud detection, target marketing, performance prediction, manufacturing, and medical diagnosis. How does this classification work? The approaches are; (a). Decision tree (J48) approach It is a flowchart-like a tree structure. And the topmost node in a tree is the root node. [8, 9]  (b). This method of classifiers is based on learning by analogy, by comparing a given test tuple with training tuples that are similar to it. In this way, all the training tuples are stored in an m-dimensional pattern space. When given an unknown tuple, a k-nearest-neighbor classifier searches the pattern space for the k training tuples that are closest to the unknown tuple. (c). They can predict class membership probabilities like, the probability that a given tuple belongs to a particular class  [10, 11] . It is made to simplify the computations involved and, in this sense, is considered "Naïve"  [7] . (d). This is for assessing how "accurate" your classifier is at predicting the class label of tuples. Confusion matrix; is used in this study, include accuracy, sensitivity, specificity, and precision. There are four additional terms we need to know that are the "building blocks" used in computing many evaluation measures. Thus, in this study, we have used five clusters. "How does the k-means ( ) algorithm work?" From the selected 7037 mothers, 3351 of mothers received TT Immunizations. The 5680 of mothers were from rural Ethiopia, and more of them (3484) were in the age range from 25-34. Using evaluation with cross-validation (10 folds) correctly classified best are by naïve Bayesian 63.30% and the least accurate were by K-nearest neighbor 60.52%. (Table 3)  Simple K-Means preferred the method of clustering for this project we have adjusted the attributes of our cluster algorithm by clicking Simple K-Means. The only attribute of the algorithm we are interested in adjusting here is the "Num clusters field", which tells us how many clusters into five based on the instruction given. Briefly described as follows: Cluster 0-This group have 1534 (27%) instances of which consist of mothers who live in the rural, no access to radio, no access to TV, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 1-This group consists of 1004 (18%) instances of which mothers who live in the rural, no access to radio, access to TV yes, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, head of household male, tt-vaccinated yes. Cluster 3-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 4-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, orthodox in religion, Amhara in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education no education, women age 35-49, mother marital status married, hh_head Female, tt-vaccinated yes. As explained in the    The literacy status of the mother has high information gain (0.046) and followed by the literacy status of the mother (0.041) and the least information gained was head of household (0.00000147). As a conclusion, the best algorithm based on the TT vaccination data is multilayer perceptron classifier with an accuracy of 67.28% and the total time taken to build the model is at 0.01 seconds. Multilayer perceptron classifier has the lowest average error at 32.72% compared to others. These results suggest that among the machine learning algorithm tested, multilayer perceptron classifier has the potential to significantly improve the conventional classification methods for use in medical data.
paper_402	 The BPNN and SFS were used interchangeably to identify the relevant features that contributed with the response variable. As a result, the BPNN with the selected features was able to interpret more accurate results (r 2 = 0.991) than the model with all the features (r 2 = 0.816). The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. It is concluded that the usage of ANN with SFS provided an improvement to the prediction model's accuracy, making it a viable tool for machine learning approaches in civil engineering case studies. Therefore, an artificially intelligent (AI) selection algorithm is required to overcome this shortcoming and identify the underlying parameters that improve the model's accuracy and simplify the computational complexity. In this study ANN is employed with other machine learning techniques to identify the parameters that capture the compressive strength of UHPC using data collected from the literature. [24] . Ghafari et al. As a result, BPNN proved to be more accurate than SMD in the prediction of compressive strength and slump flow of UHPC. Meaning, the model does not produce any analytical model with a mathematical structure that can be studied. Therefore, ANN should be utilized in detecting the dominant input parameters that have direct association with the ANN model. This technique makes use of ANN's complex computation and allows the SFS tool to select and remove the influential and redundant parameters, respectively. Four sets of open human motion data and two types of machine learning algorithms were used. The total number of features was reduced rapidly, where this reduction helped the algorithm demonstrate better recognition accuracy than traditional methods. Moreover, Rodriguez-Galiano et al. Four types of machine learning algorithms were used as wrappers for the SFS. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. Table 1  presents the initial input variables together with their range (maximum and minimum values) and symbols for identifying them in this experimental program. In this study, the feed backward ANN is used, where it is composed of input neurons, hidden neurons, bias units, wires containing randomly generated weights, and output neurons. SFS reduces the dimensionality of data by selecting only a subset of measured features to create a prediction model. In this study, four variables (Cement, Sillica Fume, Flyash, and Water) were selected as the most relevant features for the prediction model. As a result, the model that used the selected features showed stronger agreement with the experimental results in contrast with that prior to the selection. As a result, the ANN model with the relevant features was capable of predicting 89.6% of its values within the aforementioned boundaries, as opposed to the ANN model with all the features which predicted 58.7% of its values within the boundaries. The LSR model is a linear function and its form is shown in  (2) . It is observed from  Figure 6  that there is noticeable increase in the compressive strength of UHPC with the increase in Flay Ash and more noticeable with the increase in Silica Fume. This study was conducted to detect the correlation between the material constituents of UHPC and its compressive strength. It can be concluded from this study that: 1) The use of ANN with SFS reduced the number of input parameters needed to accurately predict the compressive strength of UHPC mix for the prediction of compressive strength, making it less computationally expensive. 2) The use of ANN with selected input parameters improved the accuracy of prediction of compressive strength of UHPC and reduced the computational effort. 4) LSR was implemented using the selected input parameters to develop an analytical model that can be used to accurately predict the compressive strength of UHPC.
paper_418	 For short series, the cyclical is embedded in the trend  [2] . In particular, this means that the fluctuations overlapping the trend-cycle are not dependent on the series level. They do not depend on the level of the trend  [3] . The emphasis is to compare the row, column and overall means and variances of the Buys-Ballot table for additive and mixed model when trend-cycle component of time series is linear. This is an indication that the seasonal variation equals a certain percentage of the level of the time series. Sometimes the seasonal component is a proportion of the underlying trend value. The seasonal or cyclical variation may virtually be wiped off by very sharp and rising or declining trend. According to them, the seasonal fluctuation exhibits constant amplitude with respect to the trend in additive case while amplitude of the seasonal fluctuation is a function of the trend in multiplicative seasonality. This is an improvement over what is in existence. The method adopted in this study is the Buys-Ballot procedure in descriptive time series. 2 (a) The column means mimic the shape of the trending parameters and contain seasonal indices for additive model. (b) For mixed model, the column means mimic the shape of the trending curves of the original series and contain the seasonal indices. 3. The row and overall variances contain both trending parameters and seasonal indices for additive and mixed models. 4. That is when 0 b =  (Table 1)  For mixed model, we obtain using the expression in  Table  1  ( ) ( )  formula_8 when there is no trend. formula_10 When there is no trend ( 0 b = ) we obtain from  (Table 1)  . . Table 3  that when there is no trend i.e. (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. The data of 120 observations has been arranged in a Buys-Ballot table as monthly data (s = 12) and for 10 years (m = 10). Estimates of trend parameters and seasonal indices are discussed. Also, when there is no trend ( ) 0 b = , the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models.
paper_428	 The present article examines the influence of thermal radiation on two-dimensional incompressible magnetohydrodynamic (MHD) mixed convective heat transfer flow of Williamson fluid flowing past a porous wedge. The solutions of the resulting nonlinear system are obtained numerically using the fifth order numerical scheme the Runge-Kutta-Fehlberg method. Hayat et al. Peristaltic motion of Williamson fluid through a channel enclosed by permeable wall is significant in Biology and medicine; in this regard Vajravelu et al. By considering the approximation of long wave length and small Reynolds number the peristaltic pumping of Williamson fluid in a planar channel was investigated by Vasudev et al. Nadeem et al. found numerically convergent solutions of two dimensional flows of non-Newtonian fluids along chemically reactive species  [11] . Hayat et al. examined MHD motion of nanofluid owing to rotating disk with partial slip  [20] . Srinivasacharya et al. Jackson et al. Fu et al. Jafari et al. Ashraf et al. studied boundary layer flow of fluid in a porous wedge subject to Newtonian heating along heat generation or absorption  [69] . The nondimensional form of the given system of partial differential equations is obtained by introducing the following stream function and the similarity variables  [76] . To examine accuracy of our work a comparison has been made with the available works of Ishak et al. 1. Figures (1-2)  illustrate the influence of wedge angle parameter < with on velocity and temperature profile. It is observed that velocity increases by increasing the wedge angle parameter < , but the thermal boundary layer thickness is decreased. Since the wedge angle parameter < is a dependent over the pressure gradient, and its values may be positive or negative. Figure 3  displays the velocity profile for various values of the magnetic parameter M, the ratio of electromagnetic force to the viscous force that quantifies the intensity of applied magnetic field. It is observed from graph that with an increase in magnetic parameter M, there is a decline in the velocity distribution. The influence of thermal radiation is to enhance the amount of heat, while in other hand an increase in values of Prandtl number causes to decline the temperature distribution. The steady, incompressible two dimensional boundary layer flow of Williamson fluid past a porous wedge is analyzed numerically using the 5 th order Fehlberg technique. The non-dimensional velocity profile increases by increasing the wedge angle parameter <.
paper_432	 This paper aims to study the Cotangent Bundles Hamiltonian Tubes theorem and its applications in reduction theory. The total space of a cotangent bundles naturally has the structure of a symplectic manifold. Let be dimensional differentiable manifold of class ∞ and * the cotangent bundles over . If are local coordinates in neighborhood of a point ∈ . Time -dependent smooth Hamiltonian on Τ * , the cotangent bundles of . We can introduce the Legendre transformation we need some basic facts about the structure of the cotangent bundles * of a nd dimensional differentiable manifold . Given any , manifold , of dimensionn, with -. 1, for any 0 ∈ , the tangent space at 0, denoted , is the space of linear derivations on 1 2, , that vanish on 3 2, , . < H , J = >= < = . c) Recall that the n-dimensional real projective space 0 is defined by 0 =3 /o, where ~ , for ∈ 3 ⊂ Š> . For the mapping.∅: & → | to have a lift ‹ satisfying the condition ( ‚ ) =J ‚ , where ‚ ∈ &,J ‚ ∈ H, 0 (J ‚ ) =d 0 =( ‚ ), it is necessary that∅ (π n (&, ‚ )) ⊆0 (a n (H,J ‚ )) (3. 5). For cOEOE . 1. [18]   Let be a lie group• ∈ o⋃ ~∞•. , for K ∈ , n, ' ∈ . [6]  Let be a Lie subgroup of a Iie group , and 3 a manifold on which acts. (n, ) = ( nℎ -1 , ℎ. ), the left multiplication of : n`. The twisted product × ° is the quotient of × 3 by the twist. Now consider a G on a manifold. šJ% ± ∈ , with isotropy subgroup = ² . A tube for the action at ± is a −equivariant diffeomorphism from some twisted product × ° to onopen neighborhood of ±in , that maps [J, 0] H to ±. The space o may be embedded in × ° as {[J, ] : ∈ 3}; the image of the latter by the tube is called a slice theorem. [12] . This model is known as the Hamiltonian tubes; it the basis of almost all the local studies concerning Hamiltonian of Lie groups on symplectic manifolds. [17]   Let act freely and properly by cotangent lifts on * Q, and let Jbe the momentum map of the G action (with respect to the canonical symplectic form on * Q. Leta ³ : → / is projection. Define the map, ∅ : ´s > (0) → * ( / ) by, for every 0 ∈ µ * Q. [12]   The first result of the theory in cotangent bundles reduction, the theory developed for the problem with a single or bit type playing an important role in the solution to the general problem of a singular cotangent bundles reduction for base manifolds, Hamiltonian tubes when the symplectic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models and the first work studying symplectic normal forms in the specific case of cotangent bundles.
paper_444	 Resource constrained project scheduling problem is to make a schedule for minimizing of the completion time or total cost subject to precedence rules and resource constraints. Traditional resource constrained project scheduling problem research takes into account achieve management goal in certain environment. In this paper, for better described the uncertain resource constrained project scheduling problem, we firstly consider the uncertain resource availability project scheduling problem based on uncertainty theory. Then, an uncertain resource constrained model is built. The other constraint is the resource constraint in which the demand of resource shall not exceed the total supply of resource for each resource type at any time. In real-word projects, there may be some uncertainty phenomena, for example, overdue materials, the decrease in the number of workers at Grain in Ear season, etc, which result in the resource availability may be changed. In this case, many scholars begin to consider the uncertain resource constrained project scheduling problem. Xie  [9]  supposed that the increased quantities of resource were real-value variables, and built a multi-mode resource constrained project scheduling model for minimizing both the project makespan and cost. Lambrechts  [10]  established a stochastic project scheduling model in which the resource availability was a random variable in order to increase robustness. Uncertainty theory based on uncertain measure founded by Liu  [12] , and its a branch of axiomatic mathematics for modeling human uncertainty. Liu  [13]  firstly established an uncertain project scheduling model, aiming to minimize the total cost under the constraint that the completion time does not exceed the deadline. Up to now, we have not yet found uncertain resource availability constrained RCPSP in uncertain environment, which is not either randomness or fuzziness. In this paper, we consider uncertain resource availability project scheduling problem based on uncertainty theory, in which the increased quantities of resource are uncertain variables, and the finish time of each activity is a decision variable. In Section 2, an uncertain resource constrained project scheduling model will be built and transformed into a crisp form. A multi-objective programming model is built under the resource restricts and precedence rules of activities constrains in order to balance the completion time and the total cost of the project. The uncertain resource constrained time-cost trade-off problem can be described as following optimization model: formula_0 In the above model, objective ○ 1 is to minimize the project total completion time; Objective ○ 2 is to minimize the expected project total cost which consists of the activity cost and the additional resource cost. As to a pair of activity and 2, activity 2 start after its predecessor activity is finished. Theorem 1. [16]  Let G A , G P , ⋯ , G ) be independent uncertain variables with regular uncertainty distributions H A , H P , ⋯ , H ) , respectively. If the function Q(R A , R P , ⋯ , R ) ) is strictly increase with respect to R A , R P , ⋯ , R S and strictly decreaseing with respect to R STA , R STP , ⋯ , R ) , then formula_2 holds if and only if formula_3 To solve the model (1), according to the operational law of uncertain variables, we transformed it into its equivalent form, as shown below. Theorem 3. formula_4 Proof: Because is an uncertain variable with regular uncertainty distribution Φ , and formula_5 by the definition of uncertain variable  [16] , we know that ? is an uncertain variable, and the inverse uncertainty distribution of ? is 280 yuan. The constrains are recourse constraint and precedence constraint. Table 1 . The information of the activities. By describing the increased quantities of resource as uncertain variables, an uncertain resource constrained project scheduling problem is discussed in this paper. Then an uncertain optimal model was built with objective of minimizing the completion time and the cost with resource constrained based on uncertainty theory. In future research, We can also focus on more types of project scheduling problems based on uncertainty theory.
paper_462	 The goal of postgraduate education for master's degree in clinical medicine is to cultivate high-level medical and health professionals. Since 1998, the postgraduate education for master's degree in clinical medicine has been piloted in China. Chongqing Medical University was founded in 1956. Since 1979, it has enrolled doctoral and master's degree students. In 1981, it was approved as the first authorized unit of doctoral and master's degree in China. In 1998, it became the first batch of doctoral and master's degree in clinical medicine in China. Pilot units for professional degrees. Since 1998, the school has vigorously developed professional degree postgraduate education  [1] . In view of the main problems existing in clinical master education, the school has carried out a series of reforms and explorations on the organic link between clinical master education and standardized resident training (hereinafter referred to as standard training). After more than five years of exploration and practice, the reforms have achieved remarkable results and have provided a training model for clinical master in China. It not only guarantees the quality of professional degree postgraduate training, but also supplies a large number of high-quality talents for related industries. The development of postgraduate education of clinical medicine degree in China is faced with the characteristics of short time, complex training objects, multiple administrative departments involved in medical degree education, and diversified training channels  [2] . How to reform the training mode of clinical master, improve the quality of training, and bring up a large number of high-level applied medical talents is the main problem for graduate educators. Educational development and other key problems of clinical master training. The organic cohesion of syndromes has effectively improved the quality of clinical master training. To vigorously develop professional degree postgraduate education and improve the quality of training high-level applied talents is the focus of the current degree and postgraduate education work. The current single tutorial responsibility system can no longer meet the requirements of clinical rotation training, which is not conducive to the management and supervision of postgraduates. The tutor who applied for the postgraduate examination is the first tutor (defense tutor). Effective management during the transition period. Aiming at the characteristics of professional degree postgraduate education and adapting to the innovation of its training mode, the school has established a funding system different from academic degree postgraduates, and continuously improves the standards and coverage of grants. Excluding very few phenomena, the school grants cover 100% of the total, and the living allowance for clinical master's degree has also been greatly increased. Our school has made corresponding reforms in the curriculum system of clinical master's degree. Secondly, the curriculum of Master of Clinical Science is adapted to the requirement of training students' theoretical knowledge and foreign language. Thirdly, according to the requirements of standardized resident training, and with the cooperation of training bases, starting from medical ethics, medical ethics, laws and regulations, professional ethics and basic clinical skills, we will offer lectures on medical law, applied psychology, humanistic literacy and doctor-patient communication to comprehensively improve the comprehensive quality of clinical master. System The secondary schools awarded professional degrees in clinical medicine in our university are all the standardized training bases for residents in Chongqing. The rotation requirement not only meets the requirements of the state for clinical master, but also closely combines with the regular training. Requirements for the first stage of training. For non-graduate students, we adopt the "fill-in" training method, that is, accurately record the clinical rotation time of clinical master, and add up the previous training time to meet the requirements of training time. The main reason for this phenomenon is that since the trial implementation of clinical medical professional degree, the state has not evaluated the quality of the pilot units and lacked an effective monitoring mechanism. The clinical master emphasizes on examining the clinical competence of postgraduates. At the same time, the cost of training clinical masters has increased substantially. The school revises the standard of clinical master's degree award, which reduces the requirement of publishing articles. Second, in-depth research. Thirdly, we should formulate an enrollment system conducive to the development of professional degrees. Qualification certificate students are not allowed to practise medicine in other places in accordance with the Law of Licensed Physicians, so it is difficult to carry out clinical training  [14] . This model is based on practice in an all-round way, does not need the national single enrollment index, does not need the national special allocation, has the characteristics of innovation, practicability, commonality, feasibility, and has the realistic basis for comprehensive promotion and implementation. It fully guarantees the quality of clinical master training and brings up a large number of high-level applied medical talents for Chongqing and even the whole central and Western regions, which has produced remarkable economic and social effects. Benefit. Since 2009, the school began to explore the reform of the training mode of clinical master in an all-round way. Dozens of brothers such as Fudan University learn from our experience. In the past five years, a total of 2063 clinical masters enrolled in the school have been integrated into the training. They have achieved the ultimate goal of training "doctors who can really see a doctor". They have trained a large number of high-level applied medical talents for Chongqing and the whole central and Western regions, promoted the development of health undertakings in the central and Western regions, and produced remarkable economic and social benefits  [16] . In the 2011 "Forum on Reform and Development of Medical Education", the principal of the school presented the reform experience to the conference and won the unanimous praise of the broad masses of colleagues. In December 2013, the school sponsored the National Symposium on the Reform of the Training Model of Clinical Master's Degree Postgraduates. At present, the school has completed more than 10 research reports and published more than 30 academic papers on the sub-project "Construction and Practice of Quality Assurance System for Medical Degree Postgraduates" of the Ministry of Education Innovation Project. Since the reform of training mode, the enrollment of clinical masters in our university has been increasing rapidly. In the past five years, a total of 2063 clinical masters have been enrolled in the standardized resident training system. Professor Xie Peng, Professor Huang Ailong and Professor Wang Zhibiao were appointed experts of the Discipline Review Group of the Academic Degree Committee of the State Council. The school assists in formulating the standardized training policy for general practitioners and residents in Chongqing, the construction of bases, the training and assessment system, and teacher training. The "5 + 3" reform of clinical medicine master's degree in Chongqing Medical University has established a "one goal", that is, to train "doctors who really can see a doctor", and established the basic principles of the reform of training mode of clinical medicine master's degree and the training of clinicians. The implementation of "three combinations", namely, the combination of enrollment and enrollment, training and training, degree award and physician access. The current management system and mechanism are not suited to the key issues of clinical medicine master training, such as the development of professional degree postgraduate education  [18] .
paper_476	 Under drastic competition, major express companies have increased their daily delivery frequency to improve customer satisfaction and market share. This paper uses JD Logistics as an example to quantitatively analyze the relationship mentioned above. The results show that: (1) The cost and resources operational efficiency are closely related to the order splitting ratio under the same delivery frequency; (2) The delivery frequency has different effects on the operational efficiency of the resources in different links. In order to reduce the operating costs of logistics enterprises on the basis of ensuring service levels, one should first properly allocate the amount of orders and splitting ratios to achieve an economic increase in the delivery frequency. at various stages in the delivery system, which will more likely to result in uneconomical performance. The transportation system is a very complex system with many different feedbacks and lagged responses between policy makers. System dynamics models not only offer a different Operational Efficiency: A Case Study of Jingdong Logistics perspective while whole system approach to transport planning, but also demonstrate to policy makers the importance of these feedbacks and lagged responses  [1] . There are various of applications of system dynamics in the area of transportation  [2, 3] . Fan Xuemei et al. Jesus et al. Some other researches tried to improve the delivery efficiency and reduce the delivery cost through delivery center location optimization  [6] [7] [8] , delivery vehicle route optimization and scheduling  [9] [10] [11] [12] , and delivery resource integration  [13] [14] [15] . The system dynamics has a good applicability in analyzing the delivery efficiency, and some researches have achieved a series of results. Hongtao Yang and Jianbang Du  [19] [20] [21]  uses system dynamics to analyze the problems of supply chain and delivery efficiency from the perspectives of transportation routes, drivers and cooperation agreements, and its sensitivity analysis provides some reference to this paper for the corporate in this case to improve the delivery efficiency. This paper uses Jingdong Logistics (JDL for short) as research objects. Then, it establishes a simulation model for the operation of the delivery system on the base of system dynamics. As the first step, this paper defines the research boundary of JDL delivery system. Based on field surveys of JDL delivery systems and interviews with operational personnel, on a route daily business JDL delivers orders within the region based on a fixed frequency. The equipment usage costs can be divided into two parts: the fixed costs and the variable costs. The employees include direct employees who are vehicle drivers and indirect employees who are managers. The on-site logistics personnel is the person who puts the sorted goods into a mail packages and places them on a pallet and then sends it to the transportation vehicles. This article considers the utilization efficiency for the facilities, personnel, and technology such as turnover rate of transportation vehicles, the utilization rate of delivery site, and the delivery personnel loading rate, etc., which are put into the sorting, transportation, and terminal delivery links. The usable area of the site consists of public area and working area. The personnel load rate in the process is calculated by dividing the actual workload by the authorized workload. Based on the surveys of JDL and interviews with related professionals, this paper summarizes 55 influencing factors on delivery frequency. These factors present a complex, nonlinear, and inverse relationship between each other. Therefore, this paper uses the causal loop method of system dynamics to analyze the relationship between the factors. As is shown in  Figure 2 , 58 causal loops are formed. According to the influencing factors above, a simulation model of the system dynamics has been built as shown in in  Figure  3 . This model passed the mechanical error checking and dimension consistency testing by VENSIM. The main parameters involved in the model are as follows: (   Table 1 , and the explanations are shown in  Table 2 . In this paper, four different schemes of delivery frequency were set up for simulation analysis. Second  Third  Fourth   1  60  30  40  20  2  30  30  20  20  3  10  30  30  20  4  -10  10  30  5  ---10  Table 4 . Setting reason for each scheme. The reason is that in scenario 3 the sorting center dispatched more vehicles in advance, which increased the cost of transportation. In this situation, the delivery frequency should be increased accordingly. Regarding on the sorting efficiency equipment, the operation time of sorting equipment became longer and the sorting cost increased. In addition, scenario 4 has the lowest transportation cost among the three scenarios as it has a higher order volume. This is because the sorting center needs more vehicles to carry the corresponding batch of goods. When the volume of orders increased to a certain value, the transportation cost of scenario 1 became the highest among the three scenarios. In the aspect of unit average transportation cost, similar trends were also emerged among the scenarios mentioned above. The main reason is that the increase in the delivery frequency improved the demand for the number of delivery personnel and caused the delivery time of delivery personnel to be shorter in a single batch. The reason is that the proportions of the sorting orders taken by the three sorting centers were different. Thus the number of additional vehicles under different order quantities was different. Figures 19, 20, 21  and 22show that the delivery frequency had different effects on terminal delivery operations. In most conditions, the area utilization efficiencies of the delivery site in scenario 1 were higher than that in scenarios 2 and 3. In some cases, the area utilization efficiencies in Scenario 1 and Scenario 3 were the same. In addition, scenario 4 can effectively ease the utilization of area and personnel in the delivery site. Recommendation I: one should properly allocate the amount of orders and splitting ratios to achieve an economic increase in the delivery frequency. With different delivery frequency, JDL's delivery resources and consumer service quality are different. In the same delivery link the demand for delivery resources can be different due to different delivery operational capacities. Therefore, JDL needs to consider different constraints when making delivery frequency decisions. In this case, if scenario 2, 3, or 4 is adopted, one can increase the total number of order shipments by 2,400, 1,400, and 4,450 units respectively without increasing the number of transport vehicles. Therefore, JDL should combine the simulation results with the actual situation before making the delivery frequency decision. Therefore, according to the actual situation, it is necessary to increase the delivery frequency or expand the area of delivery sites.
paper_479	 Inability to model an individual's handwriting over time has made estimating a full likelihood ratio for comparative handwriting analysis impossible thereby employing nuisance parameters and subjectivity in computation of L R that is not full. From the research carried out, it can be concluded that modeling an individual's handwriting is a crucial factor in achieving a full likelihood ratio, little/or no inconclusiveness in result reporting and a less degree of disagreements for handwriting identification in a forensic environment. It has been established that there are varieties of handwritten documents ranging from forgeries, counterfeiting, identity theft, fraud, suicidal note, contested wills. For the handwriting evidence, previous literatures reported many factors and the inability of the forensic examiner to model the writing profile of a writer because it is often believed that an individual's writing profile is a latent characteristic that cannot be observed directly, and is not easily modeled because an individual's writing profile cannot have static characteristics and may change over time  [9] [10] [11]  which has resulted to adopt ad-hoc methods to compute likelihood  [9, 12, 13]  when full likelihood ratio is not achievable. The purpose of this paper is to illustrate how neural network approach to comparative handwriting analysis can greatly affect the outcome of a forensic investigation and make full likelihood ratio achievable. The novelty of our proposed implementation relies on natural handwriting samples over a period of six months from known individuals to form the database to model the writing profile for each writer. The writer's profile is a very important factor that is considered to accurately estimate a full likelihood ratio. In the absence of appropriate databases, many of these likelihood ratios will include verbal rather than numerical estimates of the support offered by the analysis. Extended writing samples such as a paragraph of writing as well as signatures were considered. A statistical model for writer verification was developed; it allows computing the likelihood ratio based on a variety of different feature types. In a related work, similar examination of probability proportion based proof appraisal strategies in both evaluative and analytical procedures was carried out using a sample collected from female and male author. This paper features that investigative setting still remains rather past contemplations practically speaking; it is also attested that L R can be useful for analytical procedures bolstered through various simulations  [21] . Assessment of signature handwriting evidence through score-based likelihood ratio based on comparative measurement of relevant dynamic features was carried out by  [22, 9]  presented score-based approaches to calculating forensic probability ratios that have been established as becoming more progressively common in forensic literature. This method is genuinely direct for the score-based L R numerator, which involves creating a list of scores obtained by combining proof objects from the same source. [23]  Asserted that recent analytical developments paired with modern statistical computational tools have led to the proliferation of adhoc techniques for quantifying the probative value of forensic evidence. Quantifying the probative value of forensic evidence is subjected to many sources of variability and uncertainty. [24] In a related manner, statistical problem and pitfalls identifiable with forensic likelihood ratio were identified. Original and disguised handwriting were gotten from each writer over a period of six months and a skilled forger was asked to forge these writings. BPNN in the context of this paper was to model the handwriting pattern of each writer over a period of time. . and ! ( . Weight from second hidden unit i and output unit j is ! ) . Each scanned, segmented and clustered characters and alphabets which were collated over a period of six months were trained to learn the pattern for each writer. The developed writing model for each writer is one of the criteria to eliminate the presence of nuisance parameters when estimating a full L R . This research shows that modeling handwriting pattern for a writer is an important factor in achieving a full L R which will eliminate the use of nuisance parameters in the computation process, lower the disagreement rate with respect to the Hypothesis in support of the prosecutor and also produce no inconclusive result after estimation of L R .
paper_492	 Based on known functions of neuroscience the neural network that performs serial parallel conversion and its inverse transformation is presented. Although status of nervous system of the animal change according to the recognition by sensory organ and to the manipulation of the object by muscle in the vicinity of the animal itself, the evolved animals have in addition another nervous system so-called long-term memory or episodic memory being involved experience and prediction. The nervous system of long-term memory behaves freely but keeping consistency of the change in the environment. By the workings of long-term memory, lot of information are exchanged between fellows, and lot of time series data are conserved by characters in human society. In other words, the nervous system of animals called advanced higher animals is locally same as very primitive animal's nervous system. Chapter 3 describes the transition of the activated parts of the neural network corresponding the changes of the environment, the short-term memory and the long-term memory. The nervous system related to short-term memory is activated in synchronization with environmental events, but the nervous system involved in long-term memory is highly layered to form an image corresponding to past and future events. Even bacteria like animals in the early stages of evolution must have some eating behavior such as moving relying on light and smell to search for food and determine whether they can be eaten. The number of logical elements used may not differ much from the sum of nerve cells in insects or zooplankton. The same is true for the recognition process. The same applies to general figures. Deductive logical development is desired. Next, by providing a two-way function to the neural network both serial parallel conversion and vice versa on basic sequences is realized. The divided subsequence is defined the basic subsequence. For simplicity, assume time series data consisted of 10 type elements from a 0 to a 9 . Example shown in  Figure 1  shows that the time series data (1) arranged by randomly selecting the elements is divided into five basic subsequences (2). (1) The first element is the beginning of the first subsequence. In given example, the leading element is a1, followed by a7, a4, a6 and a6. (2) If the same element exists in already divided subsequence. Other portion will return to the initial state because no activation factors (may be activated by another time series data). The output of the element holding the activity when receiving the last c 4 of the time series data is the recognition result of the time series data c 0 c 1 c 2 c 3 c 4 . The number and their position of the activated elements is the conversion output corresponding to the serial input  [1] . The elements involved in the conversion are still activated at the time of output. Therefore, the couplings between the elements are enhanced (Hebb rule) by repeating this conversion, as a result the elements involved in the conversion will be activated by only receiving the first element of the time series data. This operation is a generation of (learned) time series data. that can be said a conversion of parallel to serial triggered by the first data. On the flow direction of data, this parallel to serial conversion is upside down with the serial to parallel conversion described above, but the basis of neural network operation is the same. When the state transition diagram of the  Figure 3  is seen as a serial parallel conversion, the bottom is inputs connected to such sensory organs. After the first data reception, the connected elements are activated as described above. On other words, both operations of the serial parallel conversion and the reverse conversion are essentially the same on the point both operations are triggered by the first data and proceed waiting for input state change. Animal's behavior is considered a time series data consisted of the data that is couple of behavior data and received data. In the  Figure 4  activity changes of each unit on processing the time series data of eating behavior is shown. It can be said as "stance to the event" that most animals have  [2] . In that case the movement of the element might be made in the highly layered nervous system. And in the nervous system, the eating behavior shown above will be nothing more than a brief occurrence that appears in a constantly continuing life. The reason why drawn a hierarchy further in the upper part of the  Figure 4 , is to suggest existence of the nervous system that processes intellectual judgment. Neuroscientist Damasio calls "image" the internal representation built in the nervous system by stimulation inside and outside the body. And argues that an evolved animal has at the heart of the nervous system an elaborate network, which is the brain  [3] . Animals in the early stages of evolution will spend most of their living time obtaining food and avoiding danger. The nervous system which is involved in the imitating function is called mirror neurons. Human beings have adopted language as a means of exchanging information between peers, and began to exchange vast amounts of time series data. If there is a problem in the episodic memory, it causes difficulties in social activities. One is time series data based on visual information of the cake in front of you and muscle movements that manipulate chocolate plates, which is produced by the nervous system that animals have from the early stages of evolution. The other is generated by the nervous system that is called " image" by Damasio, is time series data based on the shape of cake produced from family conversations the day before. The upper part shows the part related to episodic memory, and the lower part shows the part related to the short-term memory. The connection between the two sides is enhanced (Hebb rule) as indicated by a bold line, and visual information is copied to the top to reinforce the episodic memory. As a result, the shape of the chocolate in the episodic memory is identified as the thing seen in front. The following is a description of  Figure 6  associated with the definition of the category shown in Tom Leinster "Basic Category Theory"  [4] . It has been vaguely thought that the nervous system responsible for long-term memory may be in a different place from the nervous system responsible for short-term memory. It is a short-term memory to be involved in the recognition and manipulation of the thing in the vicinity of the animal itself. And the time series data brought by other brain with highly hierarchically connected structure activities is concerning long-term memory. This process is close to the rehabilitation process of the brain that has had a stroke. There may be cases of errors in the accuracy of the operation compared to the circuit using the existing logic IC because there is a probabilistic part, but the bud of a new strategy might be hidden in the vicinity of the malfunction.
paper_507	 Present study focused on the possible factors that are responsible for the landslide in hilly regions of Uttarakashi, Uttarakhand. In present study we used the already existing topographical maps, satellite imageries and field work. Integrated them together using GIS and soft computing to create a database that will generate the output for the future use for prediction of susceptibility of landslide. The main aim of present study is to integrate the result of our study with spatial data, soil parameters, land inventory and used the output as a user friendly application using GIS which could predict the future susceptibility of region to landslide and% contribution of each factor for the same. In this study, layers are evaluated with the help of stability studies used to produce landslide susceptibility map by Artificial Neural Network (ANN). A frequently used definition of landslide is "movement of mass of rock, earth or debris down a slope" in the words of Cruden  [1] . As per the official figures of United Nations International strategy of disaster reduction (UN/ISDR) and Centre for Research on Epidemiology of Disasters CRED for the year 2006, landslide ranked 3 rd in terms of number of death among the top ten natural disasters  [2] . In the present study with the help of satellite imageries like DEM from cartosat-1 and topographic maps from GSI (geographical survey of India) a susceptibility map is prepared. vector maps could be used for accessing the features for a particular location. Identification of factors which affects to the landslide. 2. Determination of the extent to which the various factors contribute to landslide. 3. Preparation of a landslide hazard zonation map that would divide Uttarkashi into different zones depending upon the factors. Uttarkashi falls under the physiographic division (s) Rohilkhand plains, Nepal Himalayas, Ganga, Yamuna daob, Siwalik range, Kumaun Himalaya, Dhaoladhar range. In the present paper landslide hazard zonation map has been prepared for the Rishikesh-Uttarkashi-Gaumukh-Gangotri. Thus a number of thematic maps (referred to as data layer in GIS) based on the specific parameters which are related to occurrence of landslide viz. slope, aspect, lithology, rainfall, land cover etc. The chosen data are related with the various factors causing landslide at a place. The maps depicting various features of Uttarkashi are in Raster Form i.e. Selection of control points to mark the boundary of map. The back-propagation training algorithm is trained using a set of examples of associated input and output values. Each node is a simple processing element that responds to the weighted inputs it receives from other nodes. The arrangement of the nodes is refer-red to as the network architecture (  figure 18 ). Figure 18 . ANNs can be grouped into two major forward and feedback (recurrent) networks. Since ANN does not understand the 'landslide prone' and 'non-landslide prone' region we explain it by giving value '1' and '0' respectively. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer (20), and between the hidden layer (20) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). We have set the error to 0.01 also referred to as goal. Weightage of different factors are shown in table 1. The first objective of present study was to study the factors causing landslide. Thus a number of thematic maps (referred to as data layer in GIS) based on the specific parameters which are related to occurrence of landslide viz. slope, aspect, lithology, rainfall, land cover etc. Slope 2. Soil texture 4. Height 5. Precipitation The next objective of our study was to present the weightage of various factors causing landslide. Here we have selected 107 points and all the six factors and a excel database was created. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer  (15) , and between the hidden layer (15) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). Here we have used the already existing topographical maps, satellite imageries and field work integrating them together using GIS and ANN MODEL to create a database that has generated the output for the future use. The result of present study with spatial data, soil parameters, land inventory and presented by a landslide hazard zonation map and a user friendly application using GIS that could predict the future susceptibility of region to landslide and percentage contribution of each factor for the same.
paper_1	 Through these messages also called status updates, they share and discuss things like news, jokes, business issues and what they go through on a daily basis. This paper reports on the successful development of a way of searching, filtering, organizing and storing the information from social media so that it can be put to some good use in an electronic learning environment. A cluster is a collection of data objects that are either similar to one another in the same group or dissimilar to objects in other groups. Clustering uses unsupervised learning technique in finding similarities between data according to the characteristics found in the data and grouping similar data objects into clusters. Its objective is to get groups of objects such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groups  [1] . It can be applied in various fields for instance taxonomy of living things, information retrieval from a document, identification of areas of similar land use in an earth observation database, discovering distinct groups by marketers in their customer bases for development of targeted marketing programs and identifying groups of houses according to their house type, value, and geographical location  [2] . Clustering is the concept that was used in this research to create groups from social media data which can be used for learning on electronic learning platforms. Requirements and Architectural Design can be done up front and then each prototype developed as the project progresses. The solution is complete when all the components are in place. This generally dealt with preliminary processing of the data collected from the users to do away with any inconsistencies and outliers  [11] . The machine learning method used was unsupervised learning in which the system was given the data so that it automatically analyzes and creates clusters from the data  [3] . The illustration of the proposed prototype is given below. The results reported in this paper were obtained from a series of evaluations that were done on the classifier on different parameters including functionality, usability, accuracy, precision and recall. This is the section that captured the users view on the functioning of the prototype. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . This is summarized in the chart below. This matrix consists of the following parameters: TP, TN, FP and FN, which are defined below. This is where the tweets were being divided into single words which were analyzed before being classified. Precision and recall were however average. These measures can be improved if large amounts of data are used to train the classifier before being used to do actual classification. Through the study, it can be underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. The system that was developed by the researcher demonstrated the learning capability of the social media by coming up with a way of creating study groups from the information shared across the social media. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. This paper addresses the limitation of social media of not being properly utilized as a platform for supporting learning activities like group formation. Through the study, it was underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_2	 The sensors used in this project were infra-red (IR) sensors and photodiodes which were placed in a Line of Sight configuration across the loads to detect the density of the traffic signal. These include loss of man-hours, accident, missed opportunities, noise pollution, air-pollution, increased fuel consumption, increased tendency to violate traffic rules, and in some cases extortion by corrupt traffic control officials. The junction is a link to three roads which include; Effurun-Sapele road, Jakpa road, and PTI road. The sensors used in this project are infra-red (IR) and photodiodes. The design had a provision for pedestrians to request for crossing the road as and when required by pressing a switch. The top down design approach was adopted here. This system was broken down into different units as listed below: (1) Power Supply Unit (2) Control Unit (3) Sensor Unit   Power Supply: A power supply of +5V with respect to ground is required for the micro controller. The complete circuitry is operated with TTL logic level of 0-5V. It comprises of a 0V to 12V transformer which is required to step down the 220V AC supply to 12V AC. Below are the ratings of the transformer  The bridge rectifier consists of four single phase rectifier diodes connected together in a closed loop to form a circuit that is capable of converting AC voltage to DC voltage  [8] . Since the peak inverse voltage of the diodes has to be greater than the peak secondary voltage of the transformer, the 1N4007 silicon diode with peak inverse voltage (PIV) of 1000 Volts was used in the circuit. The maximum output voltage of the bridge rectifier is known as the Peak Rectified Voltage, and is given as; formula_0 V be is the biasing voltage for the diode (i.e. There are two diodes conducting in each half cycle, therefore, there are two voltage drops. For this circuit, V peak rectified = 16.67-2(0.7) =15.27v dc This voltage is the input voltage of the capacitor. The PIV of the diode is calculated using the relation formula_1 Where Vp = peak voltage of transformer Vrms = root mean square voltage The Vrms of the transformer is given as 12v formula_2 Recall PIV ≥ 2Vp Therefore, PIV of the selected diode is given as 2 x 16.9706 = 33.9412v For current, the maximum current of the transformer I (max) = 500mA The forward current of the diode I f is given as:   formula_3 The reason for using PIC16F876A microcontroller over ATMEGA microcontroller is that, the former is cheaper and more readily available. The Transformer steps down the 220 v AC supply to 12 v AC. The IR sensors are used to sense the number of vehicles on the road. The Vero board is also called a strip board. The leads were then soldered to the copper tracks on the other side of the board to make the desired connections. The codes are as shown in the Appendix. Simulation was done via Proteus software. Jakpa Junction, in Effurun, Delta State, Nigeria is increasingly becoming chaotic by reason of the recurring traffic gridlock it experiences. The problem assumes a more worrisome dimension on Effurun Market days. The sensors help in keeping count of vehicles entering roads and the microcontroller subsequently allots time delay thereby giving accurate priority to each road. This new design would further reduce time delay, improve efficiency and reduce accidents by incorporating the following modifications: (1) The Airport -PTI road should have a two -way traffic light to pass the vehicles coming from Airport road to PTI road. There should also be a two -way traffic light between Effurun -Sapale road and Jakpa road to pass vehicles coming from Effurun -Sapale road to Jakpa road. (2) Solar energy should be used to support the mains power supply because of the highly erratic nature of power supply from (PHCN).
paper_3	 Also, the formative evaluation of AEHS MATHEMA by students of the Department of Informatics and Telecommunications of the University of Athens, Greece, has shown, with the exception of other things, that all its functions are useful and easy to use. They build a personalized model of the learner and apply this model to adapt the content and/or the appearance of the hypermedia, according to the learner's specific characteristics, such as the cognitive goal, level of knowledge, pre-existing or prior knowledge, interests, preferences, stereotypes, cognitive preferences and cognitive or learning style  [4] . The teaching strategies are based on  [9]  learning cycle and learning style model. The general purpose of the AEHS MATHEMA is to support learners through an interactive and constructivist educational environment in order to construct their knowledge in Physics and Technology, individually and/or collaboratively, and to overcome their misconceptions and/or learning difficulties AEHS MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and meta-adaptive navigation, interactive problem solving, and adaptive group formation. The aim of this article is to highlight the main contributions of the architecture of the MATHEMA in the improvement of the functionality of AEHSs. The key idea is the decentralization of their functions. A student model is usually created and maintained past the individual session and opened when a student logs into the system. For the AEHSs various models of architecture have been developed so far on which the designers of these systems are based on. The next thing to do is to determine dependencies between the concepts. This leads to a structure of prerequisite relationships. The prerequisites are most common in educational material. A key point in AHAM is that the adaptation rules are used to translate user actions into user model updates (as well as the presentation and adaptation of content). Also, the rules use the user model information together with the action information in order to determine the required user model updates. In order to perform adaptation based on the domain model and user model is needed to specify how the user's knowledge influences the way in which the information from the domain model is to be presented. [8] presented a similar architecture in the Proper system based on the AHAM model. Thus its architecture is a typical of a SCORM compliant LMS. The DM structure is exported by the manifest file and is stored into Java Object Files. However, they convey enough information to allow for the adaptation decision making (i.e., they include essential information related to the media type, the level of abstractness, the instructional role, etc.). The Application Model represents the main features of the application in terms of a model of the domain, the instructional or learning theory used and how the domain concepts are grouped into learning units. ULUL-ILM  [1]  is an AEHS that focuses on student's learning styles. The purpose of ULUL-ILM is to provide the AEHS that can recognize student' learning style automatically in real-time and then presents the learning content adaptively based on student learning style. It enables to recognize the student's learning style automatically in real time by means of Multi Layer Feed-Forward Neural network (MLFF). The MLFF is embedded to the system because of its ability to generalize and learn from specific examples, ability to be quickly updated with extra parameters, and speed in execution, making it suitable for real time applications. The system then enables to present and recommends a variety of learning contents adaptively towards each of the student's learning style identified in the student model through the adaptation model. The system then analyzes the learning content on each of the learning material, and then comes up with the generated teaching strategies by means of the teaching strategy generator and fragment sorting. The adaptation model enables the system to adaptively presents the content, based on the student's learning style by combining the fragment sorting and adaptive annotation technique. The course player in ULUL-ILM enables the system to adaptively presents the content with various teaching strategies towards each of student's learning style. AEHS MATHEMA architecture is based on Web technologies similar to client/server distributed computing architecture of MIT and it is mainly supported by Apache Tomcat Server 5.5 and MySQL 6.0 relational database server. These servlets are complete programs that are capable of creating JSPs. A servlet allows a programmer to utilize whatever functions a programmer needs including conditional branching and loops. This allows for much more flexibility in creating the page than XML. JavaBeans technology comes into play. The benefits of using JavaBeans components to augment JSP pages are the following  [10] : (1) Reusable components: Different applications will be able to reuse the components. This framework is best used for complex applications where a single request or form submission can result in substantially different-looking results. Figure 5  shows the architecture of the AEHS MATHEMA. Bellow the operation of the units will be presented and analyzed. In the field of Adaptive Group Formation Module selecting of the most appropriate teaching strategy, the learner's learning style is taken into account during the learner's study. The didactic design of the AEHS MATHEMA supports learners in constructing knowledge, choosing and achieving their learning goals, recognizing what they have already learned and what they are capable to do, and judging personal progress of their learning. It also offers appropriate teaching strategies to match students' learning styles, develop critical thinking and self-regulation, as well as collaborative activities, encouraging them to actively participate. In the AEHS MATHEMA, the hierarchical representation of the domain model is adopted by the ELM-ART  [5] , as follows: The first level defines the cognitive objectives, the second the basic concepts and the third the pages with the corresponding educational material. Student model contains information about student characteristics that allows the system to make adjustments using these characteristics. The characteristics that the system maintains in learner model for each learner are: name, gender, username, password, learning style, level of knowledge for each cognitive goal (general performance), level of knowledge for each basic meaning of the cognitive goal (quantitative and qualitative characterization), concepts that have been successfully evaluated for each cognitive goal, current page, current teaching method, current navigation technique, navigation history, meta-adaptive navigation status, initial Web experience and knowledge level (pre-existing knowledge) in the current cognitive goal, ways of presenting feedback messages, preference for assistance by Advisor in terms of its navigation, course sequence, and phases of the guided dialogues in interactive problem solving process in which each learner participates. The Student Model is transparent to the learner (open learner model) and is verifiable by the learner, that is, the learner can access his/her model and change the content of some characteristics. The learner during the study selects the cognitive goal he or she wishes to study, following his or her personal course in the educational content, exploits the system's assistance, navigating and studying, is informed about the information that the system maintains, who can change them by intervening and directing the system's adaptation in this way, in terms of curriculum sequencing, adaptive presentation and navigation and adaptive group formation, selects his or her learning style, selects the navigation technique, activates and deactivates the curriculum sequencing, selects the way that the feedback messages are presented, changes his or her knowledge level for each basic concept of the cognitive goal, and activates-deactivates the Advisor. Figure 6  shows a snapshot of the page showing the characteristics of the learner model where the learner can be informed about them but also modify them. The system, during the learner's study, monitors his or her interactions with the system and his or her assessment and accordingly updates the links to the course material provided and/or adapts learning material to learner according to his or her learning style. The adaptive navigation techniques that it supports are: direct guidance, link hiding, link annotation, and link sorting. This is an innovation on the architecture of AEHSs. It is the second level meta-adaptation engine that is responsible for monitoring the cognitive improvement of the learner in the cognitive goal that he or she studies, after n successful evaluations of his or her knowledge in basic concepts, showing him or her advantages and disadvantages of the four navigation techniques that it supports. The meta-adaptation engine proposes to the learner to change the navigation technique if he or she wishes after n successful assessments of the basic concepts of a cognitive goal he or she studied, showing him or her the pros and cons and additional information on each of four navigation techniques, and the learner decides whether to change it or not. Figure 7  shows a snapshot of the meta-adaptation result. Figure 7 . A snapshot of a meta-adaptation result. It includes the content of the JSP pages, Java servlets responsible for feedback and creation of guided dialogs in problem-solving, Java servlets for creating evaluation questionnaires, and so on. It is responsible for what will be presented to the learner as a result of the processing of the information that arrives from the adaptation engine, the meta-adaptation engine, adaptive group formation module, and application module. In this module there is the basic servlet, which deals with the presentation of the training material and other JSP pages and servlets, such as the page and the corresponding servlet to inform the learner about the characteristics of all learning styles, the servlet for the curriculum sequencing, the servlet for selecting and presenting the concept that the student selects to study according to his or her learning style, etc. This module is responsible for obtaining information about the data the learner entries in the system and for monitoring his or her actions (interaction with the system). Following a study  [13]  conducted on the formation adaptive groups in the MATHEMA, it was decided that: when the system creates a priority list of candidate peers for an interested learner, in the first and second position of the priority list, the algorithm will place the candidate collaborators with a concrete or abstract style and with the same active or reflective dimension of their learning style as follows: If the learner has a concrete learning style, then the algorithm will place the candidate collaborators with a concrete learning style in the first position, and in the second position, the candidate collaborators with an abstract learning style. If the learner has an abstract learning style, then the algorithm will place the candidate collaborators with an abstract learning style in the first position, and in the second position, the candidate collaborators with a concrete learning style. The collaboration protocol that uses the synchronous communication tool is as follows: (1) The learner declares willingness to collaborate either by selecting his or her partner from the priority list of candidate collaborators or by declaring a desire for collaboration so that others who would like to work with him or her can choose it, while activating the synchronous communication tool. Assessment of the pedagogical and technological aspects of the AEHS MATHEMA has been carried out. This article presents the trends of architecture of AEHSs so far and then an extensive description of the architecture of the AEHS MATHEMA. Most of AEHSs presented in this paper are based on JSP, Java servlets, and Javabeans technologies to implement their functions, as implemented by AEHS MATHEMA. Furthermore, the presented AEHSs above have at least three main modules, such as Student Model, Domain Model, and Adaptation Model as the AEHS MATHEMA also has. The architecture of AEHSs becomes more complex as more and more functions are implemented. MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and metaadaptive navigation, interactive problem solving, and adaptive group formation. The key idea is the decentralization of their functions. The main contributions of the architecture of the MATHEMA in the improvement of functionality of AEHSs are the meta-adaptation technique, the adaptive group formation technique, the problem-solving technique, and the synchronous communication protocol that it supports. Taking into account the observations and recommendations of the evaluators, some system functions, such as this offered by the asynchronous communication tool, have been improved.
paper_21	 In this paper, we present the powerful scheme ZSISMP (Zimmermann Self Invertible Stabilizer Multiplier Permutation) to attack the hardness of the minimum distance search problem of BCH codes. This scheme consists in evaluating the minimum distance of the reduced dimension sub code fixed by a Self Invertible Stabilizer Multiplier Permutation by Zimmermann algorithm. BCH codes are a family of cyclic codes, which are used in many applications, due to their powerful algebraic decoding algorithms and their error-correcting capability. The section 4 presents the main results. The conclusion and possible future directions of this research are outlined in section 5. For this reason, many researchers have explored several ways to attack the difficulty of the minimum distance search problem for large BCH Codes. This section summarizes the most important ones. In  [24] , the authors proposed an efficient method, by applying the MIM method on some sub code randomly extracted from the considerer BCH code. The proposed method MIM-RSC, has allowed an efficient local search and therefore finding the true minimum distance of some BCH codes of length 1023 and 2047 as well as obtaining good results in comparison with the previous works presented in  [17-18-19-20-21-22 ]. It is well known that for BCH (n=2 m -1, δ) codes, the multiplier permutations defined on {0, 1,..., n−1} by k 2 µ : i→2 k i (mod n) with 1≤k≤m-1 are stabilizers. From these stabilizers, we take only a Self Invertible stabilizer if it exist and by using a mathematical tool, we find the sub code fixed by this involution and then we evaluate the minimum distance by using the famous Zimmermann algorithm. The proposed scheme works as follows: Inputs: formula_0 Step 1: Find the sub code SC fixed by σ Step 2: Find the estimated minimum distance d of SC by using the Zimmermann algorithm. Output: -d as estimated minimum distance of BCH (n, k, δ) This section presents a validation of the proposed method on BCH codes of known minimum distance and its application for finding the minimum distance of BCH codes of unknown minimum distance. This section presents also a comparison between the proposed scheme and previous work on minimum distance for BCH codes. All results have been done using a simple configuration machine: Intel (R) Core (TM) i3-4005U CPU @1.70GHz RAM 4GO and are made by running the considered algorithm in 1day for each code. It is known also, that All the narrow-sense primitive binary BCH codes of length 255 have their minimum distance equal to their designed distance except BCH (255, 63, 61), and BCH (255, 71, 59). In order to validate the proposed method, it is applied on all BCH codes of known minimum distance presented in table 1. the obtained results show that the minimum weight found by the proposed method is equal to the true value of the minimum distance of all BCH codes of length up to 255. A comparison between the proposed scheme with Zimmermann algorithm, on some BCH codes are made. The table 2 summarizes the obtained results. The table 3 presents a comparison between the proposed scheme and MIM-RSC method  [24] . In the perspectives, we will apply this powerful scheme to construct good large cyclic codes, and adapt this scheme for other linear codes.
paper_31	 The realization of the intelligent system provides a scientific means of disease management for chronic disease patients, and realizes the effective management of chronic disease, which meets the design requirements and receives great patients' evaluation. From 1990 to 2017, the morbidity of chronic diseases with high incidence in China had been increasing continuously  [1] . According to China National Health and Nutrition Big Data Report 2018, 20% of Chinese suffer from chronic diseases, and chronic disease mortality was 86%. This passive recording method greatly raises the economic cost and time cost of patients with chronic diseases. In-depth interviews with 91 employees in different positions from 11 grassroots medical and health institutions since July 2019 show that the efficiency of mobile medical care in the surveyed areas has appeared  [2] . With the great progress of mobile communication technology, Internet and WSN technology with low power consumption, some scholars have applied wireless transmission technology to medical detection system. And some scholars combined Bluetooth technology with GSM short message to realize the collection and remote monitoring of physiological parameters such as blood pressure  [12]  and pulse. Chronic diseases can easily cause damage to patients' vital organs such as the brain, heart, and kidneys  [3]  to seriously affects working ability and quality of life, and its high medical expenses increases the burden of economy on society and families. Until now, the development of foreign WITMED (Wise Information Technology of med) is divided into three stages: The first stage refers to the middle of last century when Network technology was just emerging. Traditional phone calls were adopted as the main way of communication between patients and doctors. This is the second stage of the development of WITMED. TV links were put into use in mental health services and other medical treatment. In recent years, with the popularization of mobile terminals (mobile phones, tablets) and the rapid development of the Internet, Telemedicine starts to combine with big data, artificial intelligence, cloud computing, cloud services and other technologies. This is the third stage of the development of WITMED. At this stage, telemedicine has gradually transitioned to WITMED, and developed from precision medicine to disease prevention  [5] . Haji Bagheri Fard present methods based on deep learning algorithms to achieve high classification accuracy  [8] . Zigbee has been implemented on the Health Care Profile. Detecting physiological signals in the daily life of the population can effectively grasp the health status of the people in the area, and provide an important reference for the diagnosis and prevention of chronic diseases  [10] . Chronic diseases patients have to be long-term monitored, however, whether using a WIFI or GPRS, high power consumption will be arouse. Chronic diseases patients are always required to be monitored for multiple vital signs simultaneously. With the development of Internet of things in recent years, by the advantages of low power consumption, ad-hoc networking and short-distance, ZigBee technology has been in full use  [13] . As the speedy development of the escalation of people's living standard, China has gradually entered the aging society with an increasing number of patients with chronic diseases and a descent trend of age  [14] . The traditional prevention and treatment greatly increase the economic cost and time cost of patients. Due to the large amount of medical resources occupied by chronic patients, the phenomenon of difficulty and high cost of getting medical service is more prominent. Alibaba founded Alibaba health  [15] ; Tencent launched the first AI+ medical product Tencent Meiying in 2017; many large enterprises integrated medical resources through mergers and acquisitions to lay out the intelligent medical industry chain. According to the Research Report on market development and investment trend of China's smart medical industry from 2017 to 2021, the annual compound growth rate of smart medical market reach 29.6% from 2015 to 2020, and the market scale will exceed 50 billion yuan in the future. The related knowledge base in  Figure 3  can be regarded as an expert system. So the key to this research will be how to keep the integrity and effectiveness of each node's information and then how to process it. The general technique route of the algorithm is as follows: Propose a comprehensive sorting algorithm, on the basis of semantic matching and a first-come-first-served principle. Suppose there are N pieces of information to be sent according to the messages' arriving order, divide each message into many words, and mark each word as Nm, , . If there are X words appearing in the taglib and N-X words not appearing in it, the matching degree of the information is (N-X) / N * 100%. When two pieces of information are with the identical matching degree, the first arriving one will be regarded as with high matching degree. Finally, the request is inserted into the message queuing sequence by the matching degree. The general algorithm of this part is as follows: Define priority function: f x, y ax by ε, with a basic definition as follows: x: semantic matching degree, 0≤x≤1 y: a request arriving at the serial position, 0<y≤1 a is defined as the weight of semantic matching degree, 0<a<1 b is defined as the weight of serial position of Request arrives, 0<b<1, a+b=1 ε is defined as the disturbance value, 0≤ε≤0.1, with the default as 0. With the definitions above, f x, y can turn to be: f x, y ax 1 a y ε , namely f x, y y a x y ε . Generally, a is close to µ= 1 2 , formula_0 Therefore, the process of queuing algorithm based on Semantic matching is as follows: With the hypothesis that a= 1 2 (to take one half of the sum) x= 5 10 (there are 10 participles in semantic information, of which 5 are matched), y 1 (Before a new message arrives, assumes that the request queue has 10 participles waiting, then: f x, y y a x y ε =0.294, therefore, the priority value of the message is 0.294. An improved SVM calculation method is as follows: Definition 1 (conception): Define a tuple F= (X, Y, Z), in which x is the collection of all objects is the object property set, Z (X*Y), Z is a subset of X*Y. Definition 2: let {X1, Y1}, {X2, Y2} is concept form of {X, Y}, {X2, Y2b {X1, Y1}. The state matrix after row shift is multiplied by the fixed matrix to obtain the confused state matrix. The process of key plus calculation can be regarded as the result of Bitwise XOR or byte level or bit level operation. Data acquisition in  Figure 8  forms the health records in  Figure 9 .
paper_38	 A purposive sample of nine hundred and forty-three (943) first-year students constituted the population for the study were drawn from Computer Science, Mathematics and Physics undergraduate degree programmes from the Faculty of Science of the university who were admitted from the 2010/2011 to 2014/2015 academic sessions. The results revealed that with a weak correlation, OL is a good predictor on the CGPA, a dependent variable, for academic performance which holds true for students who are in the CGPA category of '1st class' and '2nd Class Lower' respectively. Education is an essential issue regarding the development of any country in the world. In Nigeria, the demand to acquire university education has been on the increase than ever before  [2]  due to the increase in the population of graduates from secondary schools  [3] . In the 2017/2018 academic session, a total number of students admitted was 4,031, and 1,632 was admitted into the Faculty of Science. formula_0 Most universities in Nigeria have been using 5.0 as their perfect CGPA score. The stakeholders agreed that the Pass Degree be abolished from the grading system and the lowest and highest CGPA scores are 0.00 and 4.00 respectively effective from the 2016/2017 academic session which implies that as long as the score is high, the better the academic performance of the students. Reference  [6]  stated that students' academic performance is not only dependent on various factors such as personal, socio-economic, psychological and other environmental variables but also quite challenging. The rest of this paper is organised as follows: section 2 is the review of related literature, section 3 presents the methodology, section 4 discusses the results obtained, and the last section presents the conclusion and recommendation. This study aims to investigate the relationship between the entry requirements into any of the Faculty of Science undergraduate full-time degree programmes in the Kaduna State University and the students' academic performance at the end of the first year of study. Determine if any of the following entry qualification used for admission, OL results only, UTME scores only or Post-UTME scores (average of OL results and UTME scores) best predict the academic performance of students in the 100 level CGPA examinations; ii. 3. 4. Performance as defined by  [7] , is an observable or measurable behaviour of a person or an animal in a particular or experimental situation in which the authors further explained that performance measures the behaviours within a specific period. The authors in  [8]  stated that the concept of academic performance is unavoidable as it "expresses the learning achievement of an individual" at the end of any academic programme of study. The numerous ways of reporting academic performance include raw scores, percentages, transformed scores, or even as categorical variables such as Excellent, Merit, Very Good, Pass, First Class, Distinction, A1, B2, C4, F9, and others. Students' academic performance which is a function of many variables, could be classified as a student, home, school, teacher, cultural and legal factors  [8] . The author's findings in his study showed that there was a consistent decline in the number of students admitted using the PUTME which cannot do better than UTME in influencing students' academic performance as the outstanding and weak students formed the upper 12.5% and lower 12.5% while the remaining 75% consists of the average students. The authors in  [8]  comparatively analysed the academic performance of graduates admitted through UTME and preliminary programmes (Certificate, Basic Studies and School of Science Laboratory Technology [SSLT]) in the University of Port Harcourt. Reference  [1]  used the Pearson Product Moment Correlation Coefficient (PPMCC) to predict the academic performance of first-year students in four departments in the University of Abuja from 2008/2009 to 2010/2011 academic sessions using UTME, PUTME and CGPA. The study by  [13]  found a significant relationship between students' scores in three examinations, namely: UTME, PUTME and 100-Level Psychology course, Faculty of Agriculture, Federal University of Agriculture, Makurdi, and thus concluded that the UTME has predictive validity for performance in the university. Their results suggested that the JAMB UTME had positive but low indices of predictive validity, which varied across the academic sessions from 2005/2006 to 2013/2014 and all programmes of study except for four departments. In contrast to the studies from the earlier mentioned authors,  [15]  investigated the relationship between 276 students' performance in the entrance examination and their performance in Mathematics in two selected Colleges of Education (CoE) in Osun and Oyo states each. The sample population consisted of students who were admitted during the 2010/2011 academic sessions but have graduated at the end of the 2012/2013 academic session. A population of 1650 students admitted into the university during the 2011/2012 academic session from Faculties of Arts, Education, Science and Social and Management Science was used to obtain their UTME, PUTME scores along with their GPA for eight semesters. The author further recommended the need for the PUTME exercise to be strengthened to have a fruitful admission of candidates. One major shortcoming of virtually all the studies reviewed is their scope in time and spread/coverage. For instance,  [12]    This study aimed to investigate which of the University's entry requirements used for the admission process best predicts the academic performance of students in the 100 level CGPA examinations. This study adopted the descriptive, also known as ex-post facto, research design that is defined by  [17]  as "a methodological approach for eliciting possible or probable antecedents of events that have occurred already and which cannot be subjected to the direct, rigorous manipulation and control". The Faculty of Science consists of nine undergraduate B. Sc. Full-time degree programmes: Biochemistry, Biological Sciences, Chemistry, Computer Science, Geography, Industrial Chemistry, Microbiology, Mathematical Sciences, and Physics. The population of the study was limited to consist of all students admitted into three Faculty of Science undergraduate degree programmes of Kaduna State University for five academic sessions from 2010/2011 to 2014/2015 using the OL, UTME and CGPA results. Out of the total sample of 3,255 students admitted between 2010/2011 to 2014/2015 academic sessions, the programme with the highest sample size is Microbiology with 491 (15.1%), followed by Biological Sciences (13.9%) whereas the programme with the least sample size is Industrial Chemistry (5.7%). The instrument used to derive the data for this study were: the JAMB UTME scores from 2010 to 2014 and OL grades in the five relevant subjects which were the pre-admission criteria and first-year CGPA results obtained from the semester examinations from 2010/2011 to 2014/2015 academic sessions. However, the OL results obtained from the database was for students admitted during 2010/2011 to 2014/2015 academic sessions. Stanine (STAndard NINE) was defined as "a nine-point scoring system with a mean of five and a standard deviation of two"  [18]  and is used in education to compare student performance for each subject. The coding for the CGPA is also shown in  Table 4 . Likelihood Ratio Tests is a statistical test of the goodness-offit between two models. In  Table 6 , the likelihood ratio Chi-Square of 18.723, 17.661 and 12.401 for Computer Science, Mathematics and Physics programmes with a significant value of 0.227, 0.281 and 0.414 tells us that the model as a whole does not predict the dependent variable, i.e., CGPA. The traditional 0.05 criterion of statistical significance was employed for all tests in  Table 7 . For Computer Science, Mathematics and Physics programmes, the first equation intercept is the log of the ratio of the likelihood of a student having a 'pass' degree to the likelihood of that student having a 'Fail' degree. Among the classification of degrees, each of the five subgroups for each programme, that is Pass, 3 rd Class, 2 nd Class Lower, 2 nd Class Upper and 1 st Class, are contrasted with the baseline group of 'fail' degree. These showed that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. Table 8  shows the summary of correlations coefficient between OL-CGPA, UTME-CGPA and PUTME-CGPA aimed at all the degree programmes for the academic sessions ranging from 2010/2011 to 2014/2015. Table 9  is the likelihood ratio test results for all academic sessions ranging from 2010/2011 to 2014/2015. As shown in the Likelihood Ratio Test results in  Table 9 , the likelihood ratio Chi-Square of 37.446, 19.938, 46.141, 14.349 and 11.167 for 2010/2011, 2011/2012, 2012/2013, 2013/2014 and 2014/2015 academic sessions which has the following as significant values of 0.001, 0.174,.000, 0.499 and 0.741 tells us that the model for students admitted during the 2010/2011 and 2012/2013 academic sessions predicts the CGPA, which is the dependent variable while the other academic sessions does not. For all the academic sessions as shown on the results, the first equation intercept is the log of the ratio of the possibility of a student having a 'Pass' degree to the possibility of that student having a 'fail' degree. For the students admitted during the 2010/2011 academic session, the slopes (B) of OL in all the CGPA categories are positive. These show that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. The relative strength of OL, UTME and PUTME on CGPA performance of students admitted in the 2010/2011 session is not statistically significant except for the slope (B) of OL in the CGPA category of '2 nd Class Upper', which is statistically significant. As for the students admitted in the other sessions, 2011/2012 to 2014/2015, the relative strength of OL, UTME and PUTME on CGPA performance are statistically insignificant except for the slope (B) of UTME in the CGPA category of '2 nd Class Upper' and '1 st Class' for those admitted in the 2012/2013 academic session, which is statistically significant. The primary purpose of this study is to investigate if OL results, UTME and PUTME scores do predict the academic performance among first-year undergraduate students in the Faculty of Science. However, by combining all the criterion variables, that is OL, UTME, and PUTME, as one variable and performing the PPMC and MLR, findings show that OL is a good predictor on the dependent variable for academic performance with a weak correlation of 0.068 which is statistically significant at 0.04 level. This predictor holds true especially for students who are in the CGPA category of '2 nd Class Lower' and '1 st Class' respectively. Although OL and UTME are still necessary as instruments for admission, it is recommended that the University be advised to include some other instruments such as senior secondary school mock examinations results for selecting candidates into any of the undergraduate degree programmes in the Faculty of Science. Also, there is a need to do a further study by including some more variables, such as age and senior secondary school mock examinations results, as criteria to significantly predict the academic performance of students successfully.
paper_57	 It is a multi-agent meta-heuristic approach and was first purposed by M. Dorigo et al. The aim is to find the shortest path. As ants are passing the terrain (graph) they mark used routes (arcs of the graph) by chemical substance called pheromone. On their way back they use the same way from which abundant loops has been removed, but the amount of pheromone (1) ∆τ k ij (t) they produced is inversely proportional to the tour length L k (t). where ρ ∈ (0,1) is the pheromone persistence (1 -ρ is evaporation rate) and m is the number of ants. how long the acquired information will be available. The probability p ij (t) of choosing the particular arc (i, j) depends on pheromone τ ij  (t)  and the heuristic η ij values which are associated with the arc (4). Let's mention ant colony system (ACS) with pseudo-random proportional rule  [3]  in which random uniformly distributed variable q ∈ (0,1) is compared with a tunable parameter formula_5 else the probability selection rule (3) is applied; random selection applied to AS rank  [4]  where random selection rate r is the probability of random selection and represents a user parameter which adjusts the balance between exploration and exploitation; prevention of quick convergence (i) and stagnation avoidance (ii) mechanisms applied to AS  [5] . The stagnation avoidance mechanism (ii) is based on the comparison of a randomly generated quantity q ∈ (0,1) with probability p k ij (t) of selected arc. Genetic algorithms (GA) were proposed by  Holland (1975) . GA belongs to adaptive stochastic optimization class and is typically used for combinatorial problems. The four main components of GA are representation (i), mutation (ii), crossover (iii) and selection (vi) mechanism. 1 . For feasibility reason the replacement node n r (new gene) is such a node from the node n i neighborhood N i , to which an arc from n i predecessor n p to n i successor n s exists  (Fig. If more such nodes occur, random selection is applied. If more of such nodes exist, random selection is applied. In GA many selection (vi) mechanisms are available, like roulette-wheel selection, tournament selection, stochastic universal sampling or reward-based selection  [7] . Since optimization process is primary done by ants cooperative behavior, the selection process has purely random concept and genetic operations serve just for selection pressure decrease. On both figures survivor strategy where parents are replaced by their children is shown. After all mutation operations are performed, crossover operations are applied. Since genetic operations may produce strings with loops, in ACO framework prior and immediately after each genetic operation a loop removal procedure is performed. The value for the number of cycles represents three macro cycles of ACO MC  [9]  for the same graph and parameters. The task is to find the shortest path between start node n s = 1 and end node n e = 80. Variable parameters were set to determine the influence of the genetic operations quantity on algorithm performance and effect of distribution of mutation operations between paths. For each setting 500 trials were performed. I.e. with no crossover pair certain amount of mutation operations should be spread out among more paths, but with 2 crossover pairs concentration of mutation operation on less paths tends to perform better. This behavior may be caused by the execution order of the GO: crossover is applied after mutation, thus crossover may re-distribute mutation substrings between more paths. The highest value 13% was received with four mutation paths with three mutation operation per path. Genetic operations where nearly always feasible; ratio accomplished / required mutation operations is 100% and for the crossover operation over 99%. In order to determine the effect of crossover operation crossover rate was let to grow up to 100%  (Table 3) . This is caused by the search space dimension. It is too large for ten ants to meet. Mutation operation causes better results than crossover operation. This can be explained by the nature of the mutation operation which creates new paths whilst crossover operation can only combine already existing solutions. Without crossover operation distributed mutation operation has better performance, but with two crossover pairs concentrated mutation operation on less paths tends to perform better.
paper_78	 Medicinal plant is important natural resource and significant for people health in many countries and regions, such as China, Japan, Korea, India and so forth  [8, 9] . Like licorice, most medicinal plants are harvested in the wild and the extended use has led to some medicinal species endangered and tending to be extinct, and the protection of such medicinal species has been urgent  [12, 13] . Some studies on the taxonomy, biochemistry, pharmacology and genetics of Glycyrrhiza uralensis have been carried out  [15, 16] , while the research concerning its community ecology is limited  [7] . Given a finite set of data matrix (species × plots), the method calculates a list of C cluster centres  formula_1 formula_2 Where i = 1, 2, …, N = the number of plots; j = 1, 2, …, C = the number of clusters; U = {U ij } = the matrix of membership values, U ij is the membership of plot i in cluster j; V is a matrix of cluster centers; m is fuzzifier which determines the level of cluster fuzziness (1 ≤ m < ∞). (dA ij ) 2 is the distance index: formula_3 X i is the vector of attribute measurements in plots, usually a vector of ordination scores; V j is the centre of cluster j, if A is a unit matrix, then formula_4 Based on the algorithm above, the procedure of fuzzy C-means clustering is as follows: (1) Selecting a common ordination method, we use Detrended Correspondence Analysis (DCA), and do ordination analysis for species data. For a plot, any value can be given as its membership value in cluster j, but the sum of memberships for a plot must be equal to 1:  2  is calculated using equations (2) and (3). Based on a general survey of Glycyrrhiza uralensis and its community distribution, five study regions, Chifeng (in inner Mongolia), Hengjinqi (in inner Mongolia), Minqin (in Ganshu), Aletai (in Xinjing) and Kashi (in Xinjing), were selected as sampling sites  [7] . The elevation was measured by a GPS, slope and aspect measured by a compass meter. Form. It is distributed from 380 to 605 m in hills with slope 10 -20° in sunny and semi-sunny slope and chestnut soil. spinosa, Lespedeza darurica, Pedicularis resupinata, Potentilla anserine, Saussurea epilobioides, Artemisia sacrorum, Artemisia mongolica, Cynanchum hancockianum, and Vicia amoena. VI. Form. It is distributed from 300 to 500 m in hills with slope 20 -30° in sunny，semi-sunny and semi-shady slope and sandy chestnut soil. The community has a total cover of 70%, a shrub layer cover of 5% and an herb layer cover of 70%. The common species are Oxytropis myriophylla, Polygonum divaricatum, Adenophora gmeliniia, Potencilla acaulis, Suaeda prostrate, Astragalus melilotoides, Allium condensatum, Artemisia ordosica, and Oxytropis grandiflora. VII. Form. It is distributed from 300 to 500 m in hills with slope 20 -30° in sunny， semi-sunny and semi-shady slope and sandy chestnut soil. The common species are Caragana korshinskii, Elaeagnus, mooceroftii, Suaeda prostrate, Artemisias phaerocephala, Saussurea laciniata, Saposhnikovia divariicata, Oxytropis glabra, and Artemisia ordosica. Form. It is distributed from 280 to 500 m in hills with slope 15 -25° in sunny and semi-sunny slope and sandy soil. The community has a total cover of 65%, a shrub layer cover of 10% and an herb layer cover of 60%. The common species are Salicornia Bigelivii, Carex duriuscula, Stipa sareptana, Artemisias phaerocephala, Alopecurus pratensis, Saposhnikovia divariicata, and Carex pediformis. Form. It is distributed from 350 to 650 m in hills with slope 15 -35° in sunny and semi-sunny slope and sandy soil. The common species are Cleistogenes squarrosa, Caragana pygmaea, Hordeum brevisublatum, Ephedra sinica, Achnatherum sibiricum, Artemisia frigida, Viola tianschanica, Carex duriuscula, and Alopecurus pratensis. Form. It is distributed from 400 to 700 m in hills with slope 10 -30° in sunny and semi-sunny slope and sandy soil. The community has a total cover of 75%, a shrub layer cover of 10% and an herb layer cover of 70%. XI. Form. It is distributed from 400 to 750 m in hills with slope 15 -35° in sunny and semi-sunny slope and sandy soil. The common species are Caragana pygmaea, Astragalinae triloa, Stipa parpurea, Festuca logae, Artemisia kaschgarica, Polygonum viiiparum, Ephedra equisetina, Glycyrrhiza inflate, and Alyssum desertorum. XII. Form. It is distributed from 400 to 800 m in hills with slope 20 -35° in sunny and semi-sunny slope and sandy soil. The community has a total cover of 80%, a shrub layer cover of 10% and an herb layer cover of 75%. The common species are Artemisia parvula, Scorzonera divaricata, Roegneria kamoji, Potentilla bifurca, Carex duriuscula, and Ranunculus japonicas.
paper_96	 This not only restricted the development of China's transportation greatly, but also threatened to people's safety seriously. This humane technology innovation is the specific embodiment of environmental science and technology aesthetic theory in the field of scientific and technological innovation. It has a positive and promoting role to the development of transportation and blind-man welfare in China. The research on "warning system of embedded electronic guide dog avoiding obstacles" in this paper is an important part of intelligent electronic guide dog system, the study of which is mainly based on the embedded system, anti-collision warning system, and the USB technology is applied to warning system of embedded electronic guide dog avoiding obstacles. [1]  In the design process of the obstacle-avoiding early warning system of embedded electronic guide dog, we firstly has carried on a comprehensive evaluation and analysis for each scheme, and then put forward a new idea that it introduces the open source embedded real-time operating system uCLinux, with a high-powered ARM core as the core processor. Sound will produce auditory stimulus for the blind. The overall design plan of obstacle-avoiding early warning system of embedded electronic guide dog mainly is the velocity and distance measurement module, the power circuit module, the LED screen show module, USBcommunication circuit module, and clock circuit module. The hardware of the obstacle-avoiding early warning system of embedded electronic guide dog can reference the Figure 1:  Figure 1 : Schematic diagram of hardware system structure  [4]   In the anti-collision early warning system, referenced in the design of obstacle-avoiding early warning system of embedded electronic guide dog, there mainly exits four steps, that is, information collection, information processing, information judgment, and warning information. [5]  And information judgment is based on the information, like the model of the guide dog, opportunity, weather, the distance between the guide dog and the obstacle, relative acceleration, relative speed and so on. Presently, the types of the generally used anti-collision warning systems mainly are radar anti-collision warning system, ultrasonic anti-collision warning system, laser anti-collision warning system, infrared anti-collision warning system, machine vision anti-collision warning system, and interactive intelligent anti-collision warning system. The selection of anti-collision warning system for the design of obstacle-avoiding early warning system of embedded electronic guide dog should start from the characteristics of the highway network and the street network construction in our country, combing with the characteristics of the obstacle-avoiding early warning system of electronic guide dog, as well as the construction of our country's highway and street traffic integrated management system. Therefore, in the design process of the obstacle-avoiding early warning system of embedded electronic guide dog, after the comprehensive and comparative analysis of the anti-collision warning techniques, we have compared the optimization principles of the anti-collision warning techniques, and finally adopt the approach of millimeter wave radar sensor + ARM microprocessor. The design of obstacle-avoiding early warning system of embedded electronic guide dog also adopts the generally used embedded processor. Presently, the generally used embedded processors mainly are ARM, Power PC, MIPS, Motorola 68000 series, etc. The full name of ARM is Advanced RISC Machines. It extensively uses the registers with a fast speed of instruction execution. Due to the operating system is an aggregation of the system program, people don't have to consider the differences between different hardware, and could achieve the target of writing application software through the system interface provided uniformly. The embedded system will further compile all of its procedure codes, including the operating system code, driver's code and application code, into a whole paragraph of executable code and in inserted them into the hardware. In this way, the embedded operating system in embedded system is more like a set of function library. These are the most remarkable features that uCLinux owned. of USB reference model  [4]   Today, with the rapid development of transport and vehicles, the frequency of traffic accidents has been being high, which not only greatly restricted the development of transport, but also made people travel in great danger. The warning system of embedded electronic guide dog avoiding obstacles studied in the project can effectively eliminate the influence of traffic environment and blind subjective factors, send the pre-trip alarm for the blind in time, effectively avoid obstacles such vehicles, to reduce traffic accidents caused due to the blindness. This humanized technology innovation is the embodiment of environmental science and technology aesthetics theory in the field of science and technology innovation practice.
paper_134	 New information and communication technologies, as well as decision support technologies, can be very effective in providing timely, accurate, and relevant information to users by collecting, storing, evaluating, interpreting, analyzing, retrieving and disseminating information to specific users  [1] . Data mining takes advantage of the progress made in artificial intelligence and statistics. Both of these areas work in model identification and data classification issues and will, in effect, be directly used in data mining, and both groups are active in identifying and using neural networks and decision trees  [2] . Data mining simultaneously utilizes several disciplines such as artificial intelligence, machine learning, neural networks, statistics, pattern recognition, and science-based systems. Detecting the purity of different materials can be done in a variety of ways  [3] . The purpose of the development of these methods is to estimate the quantitative and qualitative characteristics of the materials rapidly, non-destructively and reliably  [4] . Among the non-destructive methods, the dielectric method has advantages due to the relationship between the steady-state dielectric with some qualitative materials, the cost less than other methods, high speed and high efficiency in the construction of systems Different recognition of quality has attracted the attention of many researchers  [5] . Typically, the conductor plates of the capacitor are made of aluminum, zinc and silver, and among them, a dielectric can be placed in the air or other material. If the dielectric coefficient is bigger, it will be a better insulating property  [6] . Therefore, by measuring the dielectric coefficient, it is possible to extrapolate a lot of properties from the material  [7] . The association of these properties with many quantitative and qualitative components of the product, high speed, low cost and high efficiency of its use as a new and non-destructive technology has been considered by many researchers to measure a wide range of qualitative and quantitative characteristics of agricultural products  [8] . When in a country where milk is eaten up by 30 percent as a result of a rise in the price of a few hundred tons, what else would you expect from a 60 to 80 thousand Riels per liter consumption of lubricants  [9] ? This high price, in addition to having an impact on consumption, has been motivated to enter the market for profits  [10] . The great benefit of mixing vegetable oils with lubricant makes it hard to find the type of product. Lizhi et al. Soltani et al (2010) used a non-destructive capacitive method to detect the amount of banana fruit juice. The results of the prediction were satisfactory and the correlation coefficient between the actual yield and the expected value of the fruit was 0.85  [13] . Reggie et al. They used 4-volt sin voltage with a frequency of 120 MHz and a rectangular parallel capacitor, which obtained capacitive capacitors (DCs) and voltages (DVs). Experiments were carried out on the day of laying, on the third day, on the sixth day, on the ninth and twelfth days after the laying. The results showed a significant difference between DC / DV ratio during storage period. Using this parameter and egg mass, they extracted regression models and reported that the application of this method to the egg production line and its grading based on quality properties requires more research  [14] . Soltani et al. (2015) reported a research about egg grading using image and sensor processing. They used a double-layer neural network with two inputs (large diameter and small diameter), a hidden slab and an outlet (egg volume) to predict the volume of eggs. The samples were blended with sunflower canola and corn oil, and they made up a total of 9 classes with varying percentages (from 60 to 100% in 5% increments), each of which was shed separately, according to the desired amount, of 100 centimeters inside the sensor. The microcontroller is an electronic application chip that can increase the speed and efficiency of the circuit versus reducing the volume and cost of the circuit. Arduino has a variety of ports, controls, side panels, and screen connectivity, while it may be smaller, less expensive, and possibly faster than a microcomputer system. The AD8302 phase detection chip provides a simple method for measuring the input ratio of the domain and the phase difference of the two signals simultaneously (22). Support Vector Machine Regression (SVR) aims at finding a linear hyper plane, which fits the multidimensional input vectors to output values. One of the new ideas in identifying and categorizing patterns is a backup vector machine or SVM. Backup machines have very valuable properties that make it suitable for pattern recognition. In particular, SVM does not have the problem of local optimization in its training, it builds the categorizer with maximum generalization, determines its structure and topology optimally, and functions nonlinear differentiation easily with low computing using It forms the concept of an inner product in the Hilbert space. Using this technique, the amount of RMSE for the samples were equal to 3.56, 2.45, 3.05 and the amount R 2 was equal to 0.92, 96 and 94, respectively. According to the results, Most R 2 related to Olive-Canola (0.90) and the least amount of R 2 pertain to Olive-Sunflower (0.86). Also, in regard to the amount of RMSE, result has shown 4.81, 2.54, and 4.38 for Olive-Sunflower, Olive-Canola and Olive-corn, respectively. In this article two factors (gain voltage and phase shift voltage) were measured. Figure 2  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn Using this technique, the amount of RMSE for the samples were equal to 3.65, 2.46, 3.31 and the amount R 2 was equal to 0.92, 96 and 93, respectively. Comparison of the testing stage of these techniques showed coarse tree was the best prediction.
paper_139	 Utilizing customary data recovery  [5]  and information mining systems it get to the known and obscure data from the Web content. Traditional web mining algorithms handle with structured document  [7] [8] [9] [10] [11] [12] [13] [14] [15]  than the advanced methodology of mining algorithm can dealthe entire heterogeneous document comprises of images  [9] , graphs, videos  [16] , etc. A query is searched in a web search tool to recover some significant and required data for the client, either the search query is known or unintelligible to the client, it generally to reply with relevant data rather than redundant, however we can't guarantee that the reply for the query about the significance and redundancy. Once the input query is requested, the search engine generate the document with multiple web pages along with the links, the user will be unaware about the content of the web pages, the extracted web documents contain multiple web pages either be redundant or not. Initially calculate measure for all the vertices and maintain the set U which contains a minimum and maximum degree for all vertices and isolated measure vertex. Repeatedly include the minimum measure in the set-up and each measured vertex included only once in the set-up. Step2: Pick the minimum degree vertex 'v' in the set-up and include in the set U. Step3:While U doesn't include all vertices Step3A: Include the entire isolated vertex which is adjacent to the vertex 'v' to U. Step3B: Find the adjacent vertex 'u' to 'v' which is not in U and has maximumdegree. Update 'u' to U. Step3C: Update the value of degree for all adjacent vertices of 'u'. Iterate throughall adjacent vertices if possible. Apply the proposed KTMIN-JAK-MAXAM ALGORITHMto G 1 By step 1: deg(All Nodes of G 1 ) = 3 By step 2: Mark the node A as visited and put it onto the set U. By step 3: There is no isolated vertex in the given graph G 1 By step 4: 4.1 Investigate any unvisited adjacent node from A. 4.4 Discover the node 1, the unvisited adjacent node is from 1 as only 3. After Applying The Proposed KTMIN-JAK-MAXAM ALGORITHM To G 2 , we get  After applying the proposed KTMIN-JAK-MAXAM ALGORITHM to the above graph G 3 Here notice that, Regular connected network G 3 , after applying the proposed KTMIN-JAK-MAXAM ALGORITHM to G 3 , we get path of length 9 in figure 7. Case II: Connected Complete Network G 4 in figure 8  Here notice that, all complete connected networks G, after applying the proposed KTMIN-JAK-MAXAM ALGORITHM to G, we get linear path of length (n-1). Also we observed derived linked graph need not be unique but this approach will provide the optimized cost analysis report in future in data science field. Future work aims to create a finite automata tool to produce only relevant and without redundant information of web documents in data mining.
paper_145	 In 2016 WHO estimated that globally approximately 1.9 billion adults (age > 20 years) were overweight and more than 650 million adults were obese  [4] . Obesity is generally associated to a significantly higher risk of arterial hypertension, diabetes mellitus (DM), hepatic steatosis, hyperdyslipidemia and renal failure  [5, 6] . The major contribution of obesity is to lead the increase in the prevalence of chronic diseases and cancers  [7] [8] [9] [10] . Behavioral factors have significant effects on metabolic risk. Efforts are needed to improve the economic, political, social and environmental conditions so that congenial atmosphere prevails in the society for maintaining healthy life of the people. The investigated diabetic patients were 544. Thus, finally, the analysis was performed using the data of 635 diabetic patients and 265 non-diabetic people. The data were collected through a pre-designed and pretested questionnaire during the months of May and June, 2015 by some undergraduate and post graduate students of American International University-Bangladesh, most of whom were doctors and nurses, of the department of Public Health and they were associated with public health services. The data were collected from the diabetic patients of the working places of the investigators according to their convenience. But during investigation some of them were found as diabetic patients. The questionnaire contained questions related to sociodemographic characters of each person. Questionnaire also contained questions related to the stage and type of diabetes, treatment stage of disease, pre-cautions against the disease and the stage of complications due to the disease. The latter information were provided by the diabetic patients. The variables included for factor analysis were residence of the respondents, their age, gender, marital status, religion, level of education, occupation, type of work, monthly income and smoking habit. The respondents were classified as underweight  [BMI < 20] , normal [BMI,  20 -25] , overweight [BMI < 30] and obese [BMI 30+]. The inclusion was justified as all the R 2 values were found significant. Significant association was decided by chi-square test with p-value< 0.05 and odd ratio was calculated for respondents who were overweight and obese compared to normal group. It was observed from the analysis that among 900 respondents 7.6 percent were underweight [  Table 1 ] and 19.1 percent of them were from rural area. Maximum (43.1) of the respondents were overweight and 20.9 percent of them were rural people. Obesity was observed among 15.3 percent people and overweight respondents were 34.0 percent. The levels of obesity were significantly different among males and females [  Table 2 , P (χ2 ≥ 27.546) = 0.000]. There were 58.9 percent males among the respondents and 47.2 percent of them were normal. But more Muslim respondents (43.8%) were normal compared to Non-Muslim respondents (38.8%). Among the investigated respondents 92.6 percent were currently married and 43.1 percent of them were normal  [Table  4 ]. However, there was significant differences in proportions of different levels of obesity among the two marital groups of respondents [P (χ2 ≥ 22.933) = 0.028]. Levels of obesity was significantly associated with levels of ages [P (χ2 ≥ 18.34) = 0.008]. But if classification of respondents was done into two groups, one group of ages <40 years and another group of ages 40 years and above, both the groups were almost similarly exposed to overweight and obesity [O. R.= 0.81]. Maximum normal group of respondents (53.8%) was observed among agriculturists. Maximum (25.5%) respondents of obesity was noted among housewives. The proportions of different levels of obesity according to professional variations were significant [P (χ2 ≥ 46.472) = 0.000]. The lower income (< 20,000.00 Taka) group of people were more (34.2%) and 48.4% of them were normal [  Table  8 ]. More respondents of normal group of people were observed (49.0%) among them who had income 20,000.00 -< 30,000.00.This group of people were 20.2 percent. More overweight people was observed among them who had income 30,000.00 -< 40,000.00 taka followed by the group of people who had income 50,000.00+. But overweight and obese respondents were 62 percent more exposed to diabetes compared to other groups of respondents [O. R.=1.62] In one study  [20] , it was reported that smoking was one of the factor to increase the level of obesity. The association between smoking habit and level of obesity was significant [P(χ2 ≥ 20.189) = 0.0.002]. The coefficients of the components were presented in  Table 13 . These coefficients were observed from the first component. This information were noted from the characteristic roots of the correlation matrix of the variables, where the roots were 2.573, 1.616, 1.086, 1.053, and 1.003. The selection procedure was a convenient sampling plan. Higher (71.6%) prevalence rate of diabetes was noted among the overweight and obese group of respondents. The prevalence of overweight and obesity were significantly associated with age, religion, education, occupation, marital status, income and smoking habit. They were classified as underweight (BMI <20), normal (BMI, 20 -<25), overweight (BMI, 25 -< 30) and obese (BMI = 30+). The percentages of these four groups of respondents were 7.6, 43.1.34.0 and 15.3, respectively. Among the respondents 84.1 percent were of the age 40 years and above and among them 42.8 percent were overweight and obese. This finding is similar to that observed in both home and abroad  [23] [24] [25] . The factor analysis showed that sex, occupation, education and type of work were more important to explain the variation in the level of obesity. This is for the in service, private or government, people. People may be advised to walk daily for at least half-an-hour. The public health authority can play a decisive role for the above steps.
paper_212	 Mathematical modeling of disease has been an indispensable tool in accounting for disease transmission dynamics as well as disease spread. Epidemiological disease models have been used to explain the dynamics of HIV/AIDS in the population from the early 1900s. The purpose of this paper is to build on the concept of Gillespie's Algorithm based SIR models by developing a stochastic SIR model to simulate disease evolution in the population setting. 170 of 1999. Among the expectant women there are 13,000 new HIV infections among children. There are various questions still left unanswered to date on the HIV epidemic. These questions are encompassed in the modeling of the HIV immunology, the HIV dynamics as well as the AIDS dynamics such as the dynamic distribution of the disease in the population and its likely magnitude. This study employs mathematical modeling tools in the transmission probability of HIV and analyses done on how the cumulative number of infected individuals responds as well as the AIDS death probability and how the cumulative cases of removed individuals responds to this probability. The following reviews consider models developed for HIV/AIDS data that either differed too greatly with other model estimates or still fail even with developments on the model. The number of HIV infections in 2010 was predicted to approximately 1000000. The group most affected would be the 31-40 years group. Even with the 2013 updates of Spectrum where adjustments were made in the parameter values empirically to improve the fit to program data, the estimates given by Spectrum still differed with data available. They considered and analyzed a two stage SI model that allowed for random variation in the demographic structure of the population with the population size changing at different times which had an exponentially distributed rate of infection. The parameter depended on the varying population size N. This meant that both the population size varied as well as the transmission/contact rate. Despite the fact that a lot of research has been done on modeling disease trajectory, not much literature is available on the use of Gillespie based SIR models to simulate the trajectory of a disease in the population. In their works they build a disease spread prediction model based on the SIR model and applied parameter values to a stochastic model based on Gillespie's algorithm. This is applied to data and the conclusion was that the model well explains the process of the spread of the disease in the population  [10] . Infection-transmission deterministic models are based on the characteristics of population growth, disease occurrence, and spread within a population. In spite having a lot of work done on mathematical modeling, there isn't adequate literature on the modeling the evolution of disease in the population through simulation. This paper will contribute and build on to the existing literature on modeling disease dynamics in the population with the model tested on HIV/AIDS data 1985-2018 to investigate if the simulated values would reflect results that are close to reality. This paper will help bridge the gap between conceptual epidemiological models and its simulated version by providing a developed version of an SIR model that solves one inherent problem that deterministic models do not reflect the natural data. denotes the birth rate and death rate denotes the infection rate denotes the recovery rate t denotes time point Model development The Gillespie algorithm was used to simulate a statistically correct trajectory given initial SIR conditions. The death rates were distinguished such that one death event led an individual out of the model while the other death event led an individual into a different classes. All these aspects determine the quality of the inference drawn. denotes the rate of birth denotes the rate of non-AIDS death denotes the rate of infection denotes AIDS death rate denotes model's time step Gillespie's procedure The Gillespie simulation procedure was developed to produce a statistically correct course for finite well-mixed populations  [16, 17] . The interaction between states is made possible by events outlined in this model as birth, infection, non-AIDS death an AIDS death. , , This Gillespie's algorithm based stochastic SIR model generates a statistically correct trajectory from the initial vector as , , where i=s, i, r S+I+R=N i denotes the population size of the state at time t denotes a function characterized by two quantities as a state change vector and a propensity function. A propensity function is the probability of one event occurring in the time interval ! The SIR Markov chain model transition probabilities for a closed population are; formula_4 where denotes the rate of flow of individuals from susceptibles to infectives denotes the rate of flow of individuals from infectives to removeds. The wait times between events can either assume an exponentially distributed wait time or the rate of flow between compartments can assume any of the following distributions depending on the results. The wait times between one event and the next can assume an exponential distribution ∆ ~+ ,-   In order to assess how the simulated data performs against natural data, a modified chi-square test was used. Considering the hypotheses, 4 = If the mean and variance of the simulated and natural data are equal, the simulated mean does not fit the data. 4 5 = If the mean and variance of the simulated and natural data are not equal, the simulated data fits the natural data. A modified chi-square test for simulation models was used to see how well the simulated data fit the natural data  [18] . Variables in the model were S = 3507162, I = 45820, R = 4597, parameters in the model are crude birth rate of 0.06, non-AIDS death rate of 0.025, transition rate of 0.1 and AIDS death rate of 0.48. Curves produced are illustrated below. In order to have confidence in the predicted value, we apply a test to check the simulated values against the natural data values by employing hypotheses. The critical value was 47.4. Therefore, the conclusion is that the simulated data model fits the natural data model. In this study, a simulation was carried out on the SIR model to explain the trajectory of the disease by employing a stochastic element using Gillespie's simulation algorithm.
paper_214	 This kind of model allow to the decision maker to create complex reports and graphs based on the columns from the dimension tables and measures from fact tables, that can be the base for creating alternatives and scenarious acording to the economical indicators. The business environments require analyses on large amount of data, big data and necessitate advanced tools to query through numerous criterias and also to create different realistic scenarious that allow choosing one option, so the business manager can use the right tool to gain economic advantage. Decision analysis applies to situations which have a relatively small number of alternative solutions. Each alternative are attached estimates and probabilities of achievement. It is the only method that can be applied to unstructured problems. Simulation becomes a technical coordination of procedures using the computer. In order to build models, the decision is based on information provided far more comprehensive than reports and other economic indicators required or provided by the business itself. Quantitative mathematical models are embedded in base models, managed by subsystem a management model that requires separate users from the physical aspects of data processing and storage that extract, create, delete and modify models. Trying to solve them in a particular category determined tackling by a standard method employment. The information is selected factors that have caused the deviation from the desired result and appreciate the importance they have in context. For example, after the first phase, the scope may relate to excessive spending decisions of a functional department, inventories too high or adoption of a draft research and evaluation on the introduction of computers  [3, 4] . Modeling takes expression of reality by means of abstract entities possessing quantitative and qualitative attributes. The assessment in turn depends on the search method. For complex problems, solving is carried progressing from one situation to another, until a final statement, which is the solution. Assisting decision states that the decision is the responsibility of the user. It receives relevant and substantiated elements on activity in the real system and builds models for solving future decision making on the basis of current assessments.Of the foregoing that a decision support system provides a filtering of information provided to decision makers and indicates certain restrictions. The role of a decision support system is to automate the decision making process manager, but rather to assist and develop the capacity of its intuitive, helping him to react as quickly and with greater efficiency  [1, 5] . For an interactive decision support system architecture includes the following subsystems: Data management subsystem Subsystem management models User subsystem dialog Data management subsystem consists of the following elements: database management system oxidase data, data dictionary and declarative query language. The database contains no internal data, external data and personal data.Internal data consist from the current activities of the organization and operations of various functional departments image. Whatever the nature of their data is stored in relational databases, transactional system data or data warehouse, built on subjects of interest. In current systems, the company's intranet, are increasingly present data accessible through web browsers and multimedia items such as maps, images, sounds. The data source, internal or external, data is extracted and managed by a management database. In most cases there is SGBS transactional relational data system and a management database for multidimensional data warehouses created. The subsystem management model consists of the following components: base models, the management models, dictionary and processor execution models and integration patterns Base models contain the set of models that make it possible to analyze the facts and the choice of options in terms required by the user. The models are domain-specific and models can be classified into strategic, tactical and operational models models. Strategic models assist decision makers in developing the overall strategy of the company in matters concerning the development of corporate objectives, choice of location of equipment, environmental impact analysis on the work of the organization. Tactical models are applied to the organizational subsystems and assist the user in taking decisions for allocation and management subsystem resources available  [3, 6] . Database management system allows creating new models models using programming languages, update and modify existing models, establish interdependencies between models. Manage in a logical manner a variety of models to consistency of the data model and provides integration of application systems components maker. Design is the stage where the data warehouse model is chosen, depending on the complexity of the system real user requirements and data structure existing in the company (databases, Excel spreadsheets...) Building a data warehouse there are three models: type star, snowflake patterns type and constellation type models. Facts associated table. Using data from lists, star type structure enables higher levels of aggregation on the initial size  [4, 6] . The advantages of using databases for Decision Support Systems in businesses involves creating numerous queries by the decision maker and also allow to create many scenarious and variants to choose from. In the business area is crucial to store data into a warehouse such as a star modell and create many queries and reports so the decision maker can see the trends of the economical indicators and helps him to make a decision that can improve those numbers or it can be useful to implement a strategy for a further development  [1, 3] . The databases permits flexibility in creating queries by choosing columns from dimension tables and measures from the fact table and so the decision maker may choose what economical indicators is useful to analyse and also permits to create different types of graphs such as columns, pies, scatter points or webs. By simply analysing these reports and graphs helps the decision maker to create different scenarious by changing target indicators and corelate with the economical indicators that can't be changed  [2, 5] .
paper_216	 The Black-Scholes model is a well-known model for hedging and pricing derivative securities. A number of studies have attempted to reduce these biases in different ways. The objective of this study is to value a European call option using a non-parametric model and a parametric model. The MSE and RMSE of Wavelet model is 0.208546 and 0.456669 respectively which is much lower than that of Black-Scholes model and therefore in conclusion, Wavelet model outperforms the other model. The study was carried out using simulated stock prices of 1024 observations. The financial contracts or instruments which derive their value from some other variables are called Derivatives. Equity, commodity, bond or currency, stocks, interest rate, exchange rate or any other financial variables of interest to the researcher could be the underlying asset. The ones traded on the exchange are standardized and regulated. The Black Scholes model has been developed under some assumptions such as the risk-free rate and volatility of the returns are known and constant, the returns of the underlying security are normally distributed, markets are efficient (market movements cannot be predicted). In the previous researches, it has been shown that distribution in the option prices is skewed to the left (negatively skewed)  [3] . The implied risk neutral moment-generating functions (MGF) is estimated by the wavelet method. This method only requires a reasonable amount of data (different strikes) and is very efficient, unlike other non-parametric methods which requires large amount of data. This research therefore compared the performance of Wavelet based pricing model and Black-Scholes model in the valuation of a European option. The remainder of the paper is structured as follows: Section 2 discusses the existing literature; Section 3 presents an overview of the Black-Scholes model and Wavelet based pricing model. Their formula can be used to obtain the following parameters; the spot price, the exercise price, interest rate, time to maturity and volatility. According to the researches as far as pricing derivative securities is concerned, Wavelet based option pricing model is the latest option pricing model in the literature  [6] . The approximation of the implied risk-neutral MGF of the underlying security (asset) returns was focused by developing a non -parametric option pricing model called Wavelet model  [4] . The following are some of the applications of wavelet method in finance and economics as pointed out in  [6]  and  [7] ; Wavelets can be used in multi-scaling analysis. Employing the wavelet method to de-seasonalize prices of electricity  [9] . The de-noising ability of wavelets was also recognized in  [7] . Algorithm has a superior performance in de-noising financial data with high frequency by using the first principal component to de-noise frequency variations of a business cycle with wavelets  [12] . Where;  formula_0 From the Black-Scholes formula above, we see that the strike price (K), risk free interest rate (r), time to maturity (T), the volatility (") and the current price of the stock (S0) are required. c is the call option price, p is the put option price and N (.) is the cumulative distribution function. Based on some general assumptions like independent and identically distribution (iid) for asset returns and that the moment generating function is defined well, the wavelet based pricing model can be expressed as follows; formula_1 Where, t is the current time, is the underlying asset price at time t, # $ is the time t price for a European call option written on asset, K is the strike price and T is the future maturity date. ʆ ! ( is the bilateral inverse Laplace transform. Interest rate r is assumed to be constant. From the expression above, the core of the pricing model is; formula_2 where v is a complex value whose real part Re (v) < -1 for calls and Re (v) > 0 for puts. When T -t = 1, Θ (v) is the risk-neutral MGF for the rate of return per unit of time. This needs to be approximated by wavelets. On this criterion, the Franklin hat function performs very well because it is symmetric, smooth and piecewise continuous. To estimate the risk neutral MGF of the return per unit time, we use the Franklin hat function f (t) expressed as;  In this study, for us to compare the pricing performance of the three models, we used the commonly used statistical criteria. The data sets consist of simulated stock prices, the exercise price, the time to matureness, interest rate and volatility. In this study Monte Carlo simulation was used to generate 1024 stock prices. The focus on this study was on pricing of a European Call option using two models, one parametric model (BS Model) and one non-parametric models (Wavelet Pricing Model).
paper_219	 The overall value of data quality on enterprise networks is decided using a minimax decision model consisting of the three attributes. The minimax decision model is chosen to meet the design philosophy that little advantage to the overall enterprise network performance will result from further investment in high performing attributes prior to balancing performance across all three model attributes. A movement to shift from "platform-centric warfare" to "network-centric warfare" was initiated in the final years of the 20 th century  [1] . To implement net-ready, system developers add data tagging, search algorithms, additional communications paths, and a suite of tools to exploit the new forms of data organization to help operators sift more rapidly through data to find relevant information. When discussing the theoretical, it is easy to dismiss the challenges of limited resources to implement new policies such as schedule, budget, manpower, etc. The discussion of quality of data in a communications network is usually limited to the Quality of Service (QoS) which measures user's satisfaction based on network performance metrics like latency and bandwidth. The attributes in the new model represents data quality of the enterprise system within three contexts: net-centric measures, traditional network quality of service, and cyber-security. Admiral Jay L. Johnson, Chief of Naval Operations, stated in 1997 that the military is undergoing "a fundamental shift from what we call platform-centric warfare to what we call network-centric warfare"  [1] . Vice Admiral Arthur K. Cebrowski, U.S. Navy, and John J. Garstka proposed that adoption of network-centric operations across the military enterprise would result in benefits at the strategic, operational, and structural levels and bring forth "a much faster and more effective warfighting style"  [2] . This new warfighting style is Net-Centric Warfare (NCW): an "information superiority-enabled concept of operations that generates increased combat power by networking sensors, decision makers, and shooters to achieve shared awareness, increased speed of command, higher tempo of operations, greater lethality, increased survivability, and a degree of self-synchronization"  [1] . Vital to the value of NCW is "the content, quality, and timeliness of information moving between nodes on the [enterprise] network." The DoD introduced four criteria that must be satisfied for "Data Sharing in a Net-Centric Environment" via DoD Directive 8320.02 in 2004  [4]  which later in  [5]  expanded to the seven listed in this section. In the years following, the Chairman of the Joint Chiefs of Staff Instruction (CJSCI) 6212.01 was released that set-forth the procedures for development and certification of a Net-Ready (NR) Key Performance Parameter (KPP); the NR KPP process later being subsumed into the Joint Capabilities Integration and Development System (JCIDS) process  [3]    [7] . The NR KPP specifies the attributes required of data sharing Information Technology (IT) introduced into the net-centric operational environment: (1) IT must be able to support military operations (SMO), IT must be able to be entered and managed on the network (EMN), and (3) IT must effectively exchange information (EI)  [3] . The access process should be via the "network" using "commonly supported access methods"  [6] . Making data understandable requires alignment of terminology, data protocols, data formats, and data meaning between produced and consumer  [6] . A consumer's trust in a data asset is dependent on multiple facets: assessment of the data asset authority, clear identification of the data asset source, tagging with appropriate security metadata, and maintaining a full pedigree of the data asset throughout the full process  [6] . Net-centric operational tasks are those that "produce information, products, or services for or consume information, products, or services from external IT"  [3] . Each net-centric information exchange defined MOPs that are measurable and each information exchange must also identify how the four criteria for net-centric data sharing (visible, accessible, understandable, and trusted) are satisfied for authorized consumers across the enterprise  [3] . The combined consideration for each of these areas yields a newly defined model for the data sharing enterprise comprised of three equally important attributes: data relevance, quality of data at source, and quality of service for the enterprise network. Fig. The most commonly used subjective rating in standards and in conjunction with QoE is the mean opinion score (MOS) where score of 1 is bad, 2 is poor, 3 is fair, 4 is good, and 5 is excellent. Sampling users is the preferred and direct method for measuring QoE. Where an accurate model exists for QoE as a function of that attribute's objective measures then it's used instead of sampling users' opinions. The question arises how to get the users' opinions. Where it is not easy to design a stratified sampling plan a stratification post collection of n samples without replacement can be constructed. Traditionally these QoE MOS ratings were undertaken by panels of experts. But cost and time to use panels of experts to assess MOS has resulted in seeking alternative approaches. One alternative approach is using models formed from objective perceptual measurements to predict subjective ratings of QoE  [12] . An example of such an estimate for audio (speech wireless, VOIP, fixed, clean) is use of objective measures to form the perceptual evaluation of speech quality (PESQ) model. There are three levels of reference used in determining the models for estimating the subjecting ratings: full reference (undistorted service is available for comparison with distorted outcome), partial (or reduced) reference, and no reference. In the video standards of  [14]  and  [15]  they present a series of methods of modelling with objective measurements to predict the subjective ratings. In  [17]  and  [15]  there are equations to estimate the interpretability for still imagery and video, respectively. Over the last several decades there have been many papers on the topic of QoS. But just for completeness a brief explanation of QoS is given here. User satisfaction is traditionally associated with network metrics: delay, jitter, throughput, packet loss, order preservation. However there are a number of challenges to QoE discussed in  [19]  and  [20] . It is expected that models of QoE can be accurately built as a function of objective measures from network, applications, environment, and terminals. These models of QoE based on QoS objective measures are often referred to as "QoE\QoS" correlators  [21]  and  [12] . In  [21]  QoE for voice was modeled with QoS metric packet loss as the objective measure; and other QoE model for web browsing session times as a function of QoS metric of throughput. A simplistic rejection of non-relevant data is insufficient given the negative effect that excess amounts of data can have on human decision makers, even when the data in the network is restricted to only relevant data  [23] . 2 . still imagery, motion imagery, acoustic signal, electronic signal, radar) with each revealing different aspects for a given target of interest. The measure of tagged data relevance corresponds to how accurate the data producer was in expressing the intrinsic data relevance through the application of metadata that is understandable, relatable, and unambiguous to the data consumer. This requires a thorough understanding of the wants, needs, and priorities of the consumer to realize maximum value of tagged relevance, and therefore a higher QoE. We first discuss the design philosophy for the overall assessment of the data quality for the enterprise: (1) The enterprise data quality is determined by the attribute with the lowest user satisfaction (QoE). By focusing on all three attributes we can reduce resources required in one attribute based on the overall value of data quality of the enterprise. We explain here our method to evaluate the overall quality of data of the enterprise based on a minimax decision criterion and its connection with minimax game theory. In game theory, the minimax value of a player is the smallest value that other players can force without knowing the player's actions. The same value is also the largest guaranteed value for that player with knowledge of the other player actions. Formal minimax definition  [24]  is formula_4 where a(i) denotes the actions of the i-th player of n players, a(-i) is actions of all other players except the i-th, and v i is the value function of player i. The cells in  Table 1  consist of a left value which is the games' pay-off for player one and a right value which is the games' pay-off for player two. Rows in  Table 1  represent actions of player one and columns in  Table I  represent actions of player two. Going through each action of Player one with knowledge of player two actions L, R we have maximum payoff for player one for U of 5, for D of 5, for N of 4, making a minimax pay-off action of player one of N. Player two takes each action along columns for action L maximum payoff is 4 and 3 for action R, resulting in minimax action for Player two of R with payoff of 3. The formal definition of Wald minimax model is  And for QoS the objective measures could be latency, packet loss, jitter, and sequence of packets. formula_5 Thus, for each d in D, starting with d=QDS, a state vector s max (d) is found that produces max s in S(d) f d (s) where f d (s) is a discrete function formed by a predictive model for QoE or in the absence of a model formed from direct user sampled MOS value calculated using (3), i.e. The x-axis of the chart in  Fig. 3 , a particular value of enterprise data quality is given at the start, but from start one could continue to improve QoS and QDS under Option A with no increase in the value of enterprise data quality, whereas Option B modifying the system delivering DR does increase the overall enterprise data quality. Thus, support is indicated for further research in the development of objective measures using the definition of data relevance elements presented and to determine models for predicting QoE ratings as a function of these objective measures.
paper_241	 Transformers are the key equipment in electrical power transmission. So the use of transformers protection in electric power systems is very crucial and critical as they are required for efficient transportation of electricity to consumers for long term. This paper is concerned with the differential current protection scheme of transformer using a differential relay technique with arduino Uno microcontroller as a differential relay responsible for comparing the differential current values and sends trip signal to the relay (acting as circuit breaker) to open the circuit when there is fault in the protected zone as a result of imbalance in the differential current values. The system is efficient in transformer protection, gives better isolation, has accurate fault detection and quick response time to clearing faults The demand of electricity in our modern day as led to advancement in electrical power systems, which is reflected in the development of all the power system device generators, different sizes of transformers, transmission lines and the protection equipment. In A. It is expensive uninterrupted and desired to be kept in good condition always to have supply. Due to advancement in technology and daily use of electrical devices by industries, organizations and individuals, there is an increase in electricity demand which most likely results systems overload, reducing its efficiency and can cause damage to the transformer  [1] . The system is capable of sustaining a variety of environmental and operating impacts that resemble normal operating conditions which may include lightning striking the transmission lines, excessive loading, deterioration or breakdown of the equipment insulation resulting to power systems experiencing occasional faults such as phase to phase or phase to ground faults, over current, over voltage or even temperature based faults. These faults, as in the case of phase to phase or phase ground faults could cause an imbalance of phase current (i.e. differential current) and can be prevented using differential protection and microcontroller based relay protection. The protective relay techniques provide accurate reproduction of normal and abnormal conditions for correct sensing and operation. Different methods have being suggested by the researchers and are adopted in implementing a differential protection scheme for power transformer which is microcontroller based,  [3]  designed a differential current protection of singlephase transformer using Arduino with voice alert. [4] Ochieng designed a microcontroller based power transformer protection system which uses a current sensor as the interfacing instrument between the current transformer and the pic16f690 microcontroller. This paper is concern with the implementation of digital differential current protection of a single phase transformer using Arduino Uno microcontroller as a mechanism of differential relay. When there is an occurrence of internal fault an imbalanced or non-zero differential current will flow through the operating coil of the differential relay which would be substituted with a microcontroller that sends a trip signal to the relays needed to open the circuit. The magnetizing inrush current is a phenomenon that occurs during brief initial state of energization of the transformer even when the secondary side has no load connected to it and has its current a lot higher than the rated current  [6] [7] [8] . This 12V is regulated to 5V (using L7805CV voltage regulator) which is the required voltage for the Arduino. The flow chart above is a brief description of how the differential protection scheme works using arduino microcontroller as the differential relay. The differential protection of a transformer is implemented using Arduino Uno microcontroller as a decision making device that sends a trip signal to the relay (acting as circuit breaker) whenever there is faults (internal or external faults). This paper seeks to design an alternative method of transformer protection with a digitally displayed, Arduino based system that will intelligently monitor faults which may arise due to current imbalance of the transformer and prompts a safety measure to protect the transformer.
paper_251	 Cloud computing is associated with a new paradigm for the provision of computing infrastructure and services. It represents a shift away from computing as a product that is purchased, to computing that is delivered as a service to consumers over the Internet from large scale data centers or clouds. Clouds provide an infrastructure for easily usable, scalable, virtually accessible and adjustable IT resources that need not be owned by an entity but can be delivered as a service over the Internet. Heterogeneity in the computational requirement, dynamic choice and infrequent usages types of resources of the users in modern era has main challenge for service provider (application developer and hardware manufacturer). Secondly, Now computing power or connected computing power (with network) has more demanding and significant role in almost all areas of epoch including market analysis, searching, map, accounting, medical, trading, shopping, rescue operations and many more, the list is endless. Various devices (Computing) and application has been developed and developing to fulfill the common users need. Hence demand of users is heterogeneous in nature so that varieties of application (hardware & software) have been developed to achieve the highest user satisfaction. Cloud computing is a new computing paradigm based on utility computing model which will fulfill the user's requirement dynamically on rent basis. According to the Lewis Chunningham  [20]  "Cloud computing is the internet to access someone else's software running on someone else's hardware in someone else's data center". More comprehensive concept about cloud computing has been narrated and drafted by National Institute of Standard & Technology (NIST): According to NIST-"Cloud computing is a paradigm for facilitating expedient, on-demand network access to a shared cluster (pool/collection) of configurable computing power and resources (like applications, services, networks, servers, and storage,) that can be expeditiously provisioned and exemption with least management endeavor or without service provider interaction. This cloud paradigm endorsed availability and is possessed of five imperative characteristics, three service models, and four deployment models." Hence agent based solution has been proposed to meet the requirement of the modern cloud computing with pace of dynamic provision to insure shrink in shrink out (elasticity) of the cloud service provider to achieve highest scalability and reliability in extent of the maximum availability of the service to the requisites. Outcome the results shows that the provisioning of SaaS (Software as a service) and its monitoring using agent has gives better result which is more efficient than existing approach. For the proposed system an java web application using JSP (Java Server pages) application has been chosen to develop on to the codevnySaaS cloud service provider. Fault tolerance While author  [1]  and  [2]  proposed an agent based solution to solve the above listed QoS parameter that greatly affect the performance of cloud service especially SaaS. But the main problem while looking  [1]  and  [2]  is the realization and effectiveness of the agent with cloud for better optimization of the service delivery. The main lacking point in the article  [1]  and  [2]  is validation of the proposed mechanism. Our main research work is to enhance the agent based model for SaaS delivery in the cloud as depicted in the  [1]  and  [2] . To Evaluate and delivered the cloud computing services (SaaS) using agent (for better and fast delivery) using public cloud such as "New Relic and cloud bees". Model for Proposed Work Our proposed work is to schedule and monitor cloud SaaS application onto the cloud and evaluate the performance of the same using proposed agent based. Resources: Algorithm for Scheduling (influenced from Aneka) developed onto the Cloudbees Proposed Algorithm Step1: Register user login Step2: Sign in the user login Step3: Select multi monitoring agent in cloud environment (for each subc_request with QoS constraints) Step 4: Mapping of resources between cloud client, cloud agent and cloud service providers. For evaluating the performance of the proposed agent system, the obtained results have been compared with  [2]  in which author has proposed "MABOCCF" the realization of the federation of different cloud (cloud interoperability) using agent. Author has choose two matrices to evaluate the performance of the it's proposed MABOCCF techniqueaverage user satisfaction and another one is average utilization ratio which has been derived from following fundamental (base) matrices 1. Proposed multi agent based solution has influenced from [2] but it's not the realization of cloud federation rather it has to evaluate the scheduling and monitoring of the SaaS (task) application in public cloud's (cloud federation not interoperability). Table 2  shows the response time of the deployed SaaS obtained results and has been compared with existing agent based method (in seconds) . Table 3  shows the user satisfaction of the deployed SaaS obtained results and has been compared with existing malty agent based method  Rather than performing analytical calculation, proposed system has been compared with the author's MABOCCF and NMABOCCF method. This paper presents the enhanced agent based solution to ensure better elasticity and monitoring solution. Proposed agent based solution for guaranteed better elasticity and their efficient monitoring of the resources in the cloud which helps to gather analytical statistics of the resources currently held and will be used such a memory, number of instances and CPU. The developed java web application (SaaS) has been developed with the help of codenvy SaaS developed platform. To deploying created SaaS application in the cloud a PaaS service has been required to be subscribed, for this cloudbees PaaS service has been chosen.
paper_272	 Since many years up to date now, we are still constructing the vacuum circuit breaker by classical design, but the interacting among the characteristics inside each vacuum interrupter must be scientifically analysis as a high values of the general specification which must be thoroughly understood before the breaker can be applied with safety confidence. In addition the inside electroplates some characteristics can be described as followings: Non-sustained disruptive discharge voltages In general, the above characteristics are not involving associated with only vacuum switchgear itself a lone, but when connects the switchgear with power distribution transforms or synchronous motors, there are also many negative interactive processes for instance hysteresis currents, eddy currents and high voltage capacitive inside each transformer (Parasites electrostatics charged) feedback to the interrupter process. The name vacuum arc is really incorrect, indeed, it's a contradiction  [2]  "If there is a vacuum there is no arc, and if there is an arc there is no vacuum". J=2*10 6 (A/m 2 ) Joule heating as thermionic emission Where Ø is the thermionic work function. K is Boltzmann's constant (1.37x10-23 Joules/K) The constant A is approximately 6x10 5 for most metals According to the Harris model and classification who was classified "the positive ions initiated from ionization process in an Anode plate and generates joule heating to a cathode plate surface during switching process"; i. Ionization zone ii. Acceleration zone iii. The total above processing time was measured by Harris approximately (20-250µs)  [15] . Current chopping refers to the prospective over voltage events which can result with certain types of inductive load (power transformer) due to the premature suppression of the power frequency current before normal current zero in the vacuum interrupter. It is important to appreciate that while current chopping and voltage escalation can occur in a single -phase circuit, virtual current chopping is specifically a 3-phase characteristic: the effects of normal current chopping, multiple re ignition and over voltage escalation in one phase can generate surge over voltages in the second and third phases. Although the current in the vacuum interrupter can chop to zero almost instantaneously (fraction of a microsecond), the current in the load inductance-3ph coils in the power transformer cannot attain zero value instantaneously. Since the high frequency characteristics of the power system equipment are depend on stray capacitances that mentioned above and inductances also and they in turn depend on physical dimensions, equipment layout, materials, cable lengths, all these can be represented as follows  [17] ; The figure represents TRV & Re-ignitions computed in the system when switching off power transformer load that the transient voltage approximately (350µs-450 µs) and shows very high frequency sub transients both going in the negative direction of the voltage -Left side. The analysis shows that the effect of damping in an oscillatory circuit can be described in terms of a single parameter, designed ŋ, or its reciprocal ʎ, which is the ratio of the resistance to the surge impedance of the circuit: This fact permits the construction of generalized damping curves for different values of ŋ, from which the solutions to many practical problems can be extracted with about the same effort as one expand in using a table logarithms. We note that the only parameter involved is η. This has been done in Figure. Where η =0.5, the sine function changes from a circular to a hyperbolic function. We might have developed the curves for this condition, following the same argument. However to gain familiarity with these curves, consider a specific example where the inductor current is required in a circuit in which the components have the following values: R=10 5 Ω. L =5 Henrys, C = 2X10 -8 F. These values are typical of unloaded transformer, where R represents the equivalent loss resistance. Suppose Vo =13.8√2 KV So. Zo = √L/C = 5X104 ohms η. The ionization electroplates zone will be evaluated between (20us -250 us) respectively. The real factor for switching transition rating interrupter depends on the values du/dt & di/dt that will be ranging from (700us -1500us) for VD4. Chopping current times (400us-800us). Transient over-voltages (450us -480us-500us). The other processing steps as followings: a) Chopping current b) Restrikes voltages c) Prestrikes voltages d) Multi reignitions e) Voltage escalation f) Post-arc currents g) NSDD (non-sustained disruptive discharges) It is often misunderstood how all these phenomena are created and what are their real causes.
paper_294	 Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. Moreover fresh cassava has a very short postharvest storage lifespan and as such, the Tiv people developed different ways of processing it into durable forms soon after harvest, as well as organizing their local markets where the cassava products would be sold. Mean and standard deviation statistics was used in answering the research questions. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. The photographs people take, videos they shoot, speeches and music they record, capture in bits, every moment of their life, culture, event and times that inevitably speak volumes of their history. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. Moreover fresh cassava has a very short postharvest storage lifespan and as such, the Tiv people developed different ways of processing it into durable forms soon after harvest, as well as organizing their local markets where the cassava products would be sold. Despite these postharvest losses management strategies, the different varieties of food stuffs processed from cassava are threatened to extinction due to the flooding of expensive western food stuffs in the markets; and the indigenous local markets where the products could be sold are not accessible due to lack of good access roads. 1. What are the Tiv management strategies for postharvest losses of cassava? 2. 3. What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? They are not restricted to any class of persons in the community but freely available to all. They are the venue through which the overall information resources are made freely available to all (Edoka, 2000; Assoh, 2011; Shidi, Aju & Ashaver, 2014). The fundamental mission of a public library is to collect, organize, store or preserve, and provide access to knowledge and information. Apparently, in order to achieve this mission, public libraries preserve a variety of information materials including print materials such as books, monographs, serials and periodicals; and non-print materials such as audio or audiovisual materials such as cassettes, microfiche and films among others. This is their most crucial function of all. This is in line with Agber and Mngutyô (2013); Agber, Ugbagir, Mngutyo and Amaakaven (2014) who reiterated that for developing countries such as Nigeria, audio and audio-visual media are appropriate information formats for capturing information for the people. The term cassava is most likely derived from the Arawak word for bread, casavi or cazabi, and the term manioc from the Tupi word maniot, which French explorers converted to manioc  (Lebot, 2009) . The Tiv people used to make a ridge round their houses and plant cassava on it; and when the cassava grew up, it became a cassava fence surrounding the house. Moreover, other varieties of cassava in Tiv land of Benue State include: Akpu from which akpu are processed, Yakpe, Genyi, Wari and Gyo-Akom among others. Cassava (Manihot esculenta Crantz, Euphorbiaceae, Dicotyledons) is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world, mostly in the poorest tropical countries. Consumer-driven trends which have contributed to this shift include rising incomes in urban areas, changing dietary habits, more women in the work-place, reduced time for meal preparation and growing demand for safety, quality and convenience  (Rolle, 2006) . A direct loss is disappearance of food by spillage, or consumption by insects, rodents, and birds. Essentially, cassava postharvest losses can be defined as both the physical losses such as weight and quality suffered during postharvest handling operations of cassava and the loss of opportunities as a result of inability of producers to access markets or only lower value markets. In pursuance to boost agricultural development in Benue State, postharvest losses must be managed; and to achieve this, the Tiv people developed different management strategies for postharvest losses of cassava. These include: 1. Peeling the cassava, slicing, sun drying it and grinding into flour known as Mwem ma Kpor (Cassava Flour) 3. Peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom 4. Peeling the cassava, fermenting, sieving and squeezing it into lumps known as Akpu among others methods of processing. Incidentally, these methods of cassava processing among the Tiv people practiced as strategies for the management of postharvest losses of cassava were passed down from one generation to another orally. These are the Benue state Library Board Headquarters in Makurdi and its branches in Gboko, Vandeikya, Katsina-Ala, Gungur, Aliade and Adikpo with about 33 workers  (Mngutyô & Amaakaven, 2013) . The snowball sampling was adopted in selecting the subjects. The researchers did this until they arrive at the sample size of 680. Section C also consisted of 7 variables of ways public library has impacted on Tiv management strategies for postharvest losses of cassava and Section D consisted of 5 variables of challenges faced by public libraries in acquisition and storage of audiovisual materials on Tiv management strategies of postharvest losses of cassava in Benue state. The 19 item questionnaire adapted a 4 point rating scale and respondents were asked to respond by ticking the correct or applicable responses (SA) strongly agree, (A) agree, (D) disagree and (SD) strongly disagree. Data were analyzed using mean and standard deviations. Demographically, data were collected from 680 Library Staffs, Cassava Farmers and Library Users, out of which 348 were male representing 51.2% while 332 representing 48.8% were female. What are the Tiv management strategies for postharvest losses of cassava? This means that peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips), peeling the cassava, slicing, sun drying it and grinding into flour known as Mwem ma Kpor (Cassava Flour), peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom, slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base), peeling the cassava, fermenting, sieving and squeezing it into lumps known as Akpu, peeling the cassava, grating, squeezing (demoisturizing) it, mixing with salt and other ingredients and frying with palm or groundnuts oil known as Kweesi or Akweesa Akom and peeling the cassava, grating, squeezing and sieving the starch and preparing it for food as Kamu Akom are the management strategies for postharvest losses of cassava. Table 2  shows the mean rating scores of all the items are below 2.50, which implies that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? This implies that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. 1. Based on the results in  Table 1 , the findings from the study revealed that processing cassava into Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom, Mtuhem (Base), Akpu, Kweesi or Akweesa Akom and Kamu Akom are the Tiv management strategies for postharvest losses of cassava. Table 2 , findings of the study revealed that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). 3. Findings from the study based on the result in  Table 3  showed that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. The research discovered that the Tiv people had various strategies they adopted in managing postharvest losses of cassava. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. Importantly, if public library has acquired and stored audiovisual materials on Tiv management strategies for postharvest losses of cassava, cassava farmers would have accessed these information materials to improve on various ways of managing postharvest losses of cassava in Benue State of Nigeria. The study concludes that public library must make it a point of duty to start acquiring, storing and preserving information materials on indigenous knowledge, particularly in audiovisual format, since this is the only way the library will be useful to the host communities.
paper_298	 Four research questions guided the study. The instrument used for data collection was questionnaire. Data generated was analyzed using simple percentage and descriptive statistics. Poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. The study recommended that the library management should organize a periodic user education, orientation and sensitization programmes for the undergraduate users to create awareness and enable them gain the needed skills to use the library catalogue maximally when searching for information resources. The university library goals and objectives are to provide adequate and relevant information resources both in print and online for university community to support teaching, learning and research (these for undergraduates students may refer to class work, assignments, research/project work, term papers, seminar presentation by providing relevant information and services provision for effective and efficient achievement of academic pursuit). The library ensures that the resources acquired are well organized to allow easy access by the library users  [1] . The traditional goals and objectives of the library catalogue are to enable users to search a library's collection to find items pertaining to specific titles, authors, or subjects. Like the card catalogues, sheaf catalogues, books, machine readable catalogues (MARCs) and online public access catalogues (OPACs). OPAC has brought a changing relationship between libraries and their users in the changing ICT environment. Library OPACs first emerged in the late 1970s and early 1980s and have gone through several cycles of change and development all geared towards improvement. Again, awareness of the library catalogue is the ability of the students to have communication and consciousness of its essence, its retrieval technique as well as their relevance to the information user. Students' skills in search catalogue can be created through library tours, orientations, seminars, lectures, library staff, handbooks and use of library studies. Although, much awareness of the retrieval tools may be created in the libraries, it does not necessarily mean its accessibility, not to speak of its use. The studies of Oghenekaro found that users exhibit patterns of library catalogue usage, that education, experience and sophistication of library users determine the pattern or level of library catalogue use  [1] . This is important because, in spite of the benefits which students can derive from catalogue use, its use is still poor in Nigerian university libraries. It is on this note that this study investigates the use of library catalogue by undergraduate student in the university library. It must contain different types of materials, very rich in nature, comprehensive in coverage with adequate bibliographical tools describing the location of each item, which is significant to the whole concept of library and librarianship. However, it is observed sometimes that the bibliographic tools that supposed to lead or guides user to the location of a particular item in the library are either found in adequate, misleading, totally not provided or somehow incomplete. The consequences of this, a user may end off scanning from one shelve to another in search of a document, which is often a waste of time or failure to locate and retrieve the needed material. The study is designed to achieve the following objectives: a. b. The gender breakdown is presented on  Table 1 . The awareness of respondents would depend on the way the library informs their users about the use and functions of library catalogue revealing the available resources and its location in the library. In order to find out how respondents became aware of the library catalogue in the university library, options were provided in the questionnaire on the sources of library catalogue and they were asked to indicate the sources of their awareness. Their responses were presented in the  Table 4  below. This implies that majority of respondents became aware of information retrieval tools through library staff and user education programmes these are more formal sources of awareness about library catalogue function and use. Again, university website could be used to provide access to library resources especially e-resources, therefore library management need to make it as one of the major access points and retrieval of information resources. Never  Total  F  %  F  %  F  %  100  20  40  28  56  2  4  50  200  14  32  20  45  10  23  44  300  24  44  20  37  10  19  54  400  42  75  12  21  2  4  56  500  58  100  0  0  0  0  58  Total  158  60  80  31  24  9  262  Key: F=Frequency, %= Percentage. Total  F  %  F  %  F  %  F  %  F  %  F  %  100  10  20  0  0  6  12  28  56  6  12  0  0  50  200  10  23  6  14  10  23  16  36  2  5  0  0  44  300  10  19  6  11  14  26  12  22  6  11  6  11  54  400  14  25  0  0  36  64  6  11  0  0  0  0  56  500  2  3  0  0  54  93  2  3  0  0  0  0  58  Total  46  18  12  5  120  46  64  24  14  5  6  2  262  The finding indicated poor use of OPAC 12 (5%) at the university library. This finding is in agreement with Fabunmi & Asubiojo whose study on OPAC use by students of Obafemi Awolowo University, Ile-Ife found out that though many students were aware of the OPAC, few actually made use of it. Table 7  revealed that 118 (45%) of the respondents use catalogue to access information for assignment, followed by those that use it to retrieve information for research 96 (37%). The results are presented in  Table 8 . Table 8  focused on the challenges associated with catalogue use for information retrieval. Higher number of the respondents indicated poor computer skills hindering navigation when searching OPAC 96 (37%) and inability to locate materials on the shelves, indicated in the catalogue as being available 58 (22%) were the challenges affecting the use of catalogue as an information retrieval tools. Again with that of [Ogunniyi & Efosa whose study concluded that the problem of catalogue use is associated with lack of knowledge on how to use the library catalogue  [11] . Another source of frustration was reasonable number of the respondents indicated poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. Considering the important roles library catalogue plays in effective and efficiency use of library information resources, the research recommends the following to improve the use of library catalogue for information access and retrieval: a. Because most challenges associated with the use of the OPAC often have to do with lack of ICT skills on how to make use of it. d. Regular shelf reading should be done so as to establish right contact between library users and library materials and avoid misplacement or wrong shelving of information resources.
paper_305	 Thesis 1 in the application of an ensemble learning algorithm in development of a computer program that can greatly enhance the underwriting process. Further, the complexity of loan decision tools and variation between applications is an opportunity for the use of a machine learning tool to provide learning capability that does not exist in other technologies. The purpose of this study was to develop a loan decision system using the logistic regression Meta modeling algorithm -Logitboost around Java based open source software for the Kenya commercial banks. Further, the study champions the use of open source software tools in business intelligence applications. The general objectives of this study were to: 1) Implement the meta learning algorithm -LogitBoost to develop as system for evaluating credit applications to support loan decisions in Kenyan financial institutions 2) Outline some of the challenges of using the learning algorithm in the decision-making process for the banking industry in Kenya 3) Champion the applicability of Java as an open source software in business intelligence applications From time immemorial in the banking sector, banks have relied on the personal assessment of loan risks or on the traditional statistical methods to predict the default of loans instead of using a standardized evaluation tool. The study was limited to the implementation of the Lo-gitBoost meta learning algorithm for classification loan analysis. Today, lenders are making increased use of new and innovative techniques -the key being data mining and machine learning to evaluate loan applications for business and financial prospects  [2, 3] . Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced  [4] [5] [6] . Better algorithms that overcome these pitfalls have been developed and are collectively known as Discriminant Analysis (DA) techniques or simply Meta learning algorithms  [3] . Although the model was reported to perform relatively better than models developed using other approaches; as part of the limitations and recommendation, they suggested that such a model is usually a black box and more insight the model parameters was required to make it more effective. Resulting classifier is a simple threshold on a single feature a) Outputs a +1 if the attribute is above a certain threshold b) Outputs a -1 if the attribute is below the threshold In this study, 'majority voting' was adopted for combining hypothesis from different learners. A accept classification for a loan decision meant pointed to a successful application while a reject classification pointed to the alternative. reweight ix. Repeat until you have T classifiers selected The development platform used for this project mainly included the following open source software products: The Java Development Kit (JDK) which is a Sun Microsystems product released under the GNU General Public License (GPL) was one of the packages used especially for the compilation of the source files. The NetBeans IDE which is a Java based open-source IDE was also used in the development of the system's graphical user interface (GUI) and for coding and testing of the system. Weka which is open source software issued under the GNU General Public License providing a collection of machine learning algorithms for data mining tasks was integrated into the development platform. The model was built using the training dataset and tested using three strategies. i. iii. iv. Select the next partition as testing and use the rest as training data. vi. Calculate an average performance. The two files can be generated by portioning a given data set into two and saving them separately. This strategy is similar to the use of two files as discussed earlier but relies on the learner to automatically partition a given data set into two given a split percentage Options: -F -R -I 15 Number of performed iterations: 15 Time taken to build model: 0.06 seconds Time taken to test model on training data: 0.01 seconds The results were interpreted along the following parameters for all the various training and testing strategies. The accuracy returned by the training set is 19 correctly classified instances out of 20 instances. This gives an accuracy of 19/20=95%  Class =Accept: The number of correctly classified instances is 12 and that of instances classified as belong to the class is 13. This gives a precision value of 12/13=0.92 Class =Reject: The number of correctly classified instances is 7 and that of instances classified as belong to the class is 7. This gives a precision value of 7/7=1 Class =Accept: The number of correctly classified instances is 12 and the number of instances belonging to the class is 12. This gives a recall value of 12/12=1 Class =Reject: The number of correctly classified instances is 7 and the number of instances belonging to the class is 8. The ROC graph is a plot of two measures: Sensitivity: The probability of true classifications given true instances i.e. P(true | true) calculated as a/a+b from a standard confusion matrix 1-Specificity: The probability of true classifications given false instances i.e. P(true | false) calculated as 1-d/c+d The ROC area has the following indicators: . 1.0. 0.9. 0.8. 0.7. 0.6. 0.5. <0.5. The value converted to 1 decimal place, these values indicate a perfect classification After a successful implementation of the stated system, the following were the key outcomes:  Three options were investigated for training the algorithm namely: a) The use of single file both for training and testing the model through stratified cross validation. This is a strategy where the training file was portioned into complementary data sets called the training set and the validation set. The model accuracy using this procedure was 86.86% making it a fairly reliable strategy b) The use of separate training and testing data sets returned an accuracy of 95% making it a relatively better strategy c) The use of a ratio to determine the size of the training and testing files from one data set returned an accuracy of 88.64% Therefore, it implies from these findings that the use of separate files for training and testing of the model returns the best model accuracy and hence should be adopted. The trained model was subjected to 20 instances of unclassified data which had been carefully selected from a portion of the training and through analysis returned 19 correctly classified instances resulting in a predictive accuracy of 95%  Three suggestions are likely improve the model and hence the predictive accuracy of the learner: The training and testing procedures can be done severally with different input parameters and file sizes to settle on the most effective set for different learning processes. As a conclusion, the reported work indeed confirmed that: 1) Machine learning procedures can be applied in financial modeling applications to augment manual underwriting techniques 2) These procedures can greatly improve the efficiency of such techniques because of their ability to handle large items of data generating very useful statistics 3) This work can be improved through the use of enhanced data set pre-processing procedures, the use of a cost matrix as well as parameter tuning to settle on the most effective set for various data mining requirements.
paper_310	 We are now living in the 21 st century. This project describes how to control a robot using mobile through Bluetooth communication, some features about Bluetooth technology, components of the mobile and robot. We present a review of robots controlled by mobile phone via moving the robot upward, backward, left and right side by the android application such as Arduino, Bluetooth. Here we use mainly Arduino UNO (ATMEGA 328P), Bluetooth module (HC-05). The controller acts accordingly on the DC motors of the Robot. Related reference articles implementing wireless control of robots have been studied as mentioned in  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] . Our system aims to achieve the target to design a system that can provide following functionalities with a simple and easy-to-use interface: a) Develop an android application that will act as an remote of a robot. f) Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. h) Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. The system consists of following parts: a) Arduino UNO (ATMEGA 328P) b) Bluetooth module (HC-05) c) Smart phone d) Motor driver (l293D) e) Arduino software (version 1.8.1) The basic building blocks of the project have been described below: Microcontroller will act as the brain of the robot. In this system we will be using microcontroller named Arduino UNO which contains ATMEGA 328P microcontroller chip (  Figure 1 ). Arduino has it own programming burnt in its Read Only Memory (ROM). Cprogram is very easy to implement for programming the Arduino UNO. Generally our transmitter will be smart-phone and receiver will be Bluetooth module (  Figure  2 ). It also helps to send the instruction of forward, backward, left, right to the microcontroller. The novelty lies in the simplicity of the design and functioning. It is also interfaced with the microcontroller  (Figure 4 (a) ) and with circuit connections  (Figure 4 (b) ). Arduino software (  Figure 5 ) is used to put the instruction of whole functions of this system to the microcontroller. Here we use programming language 'C' for coding. By this software we put the data and instruction for forward, backward, left, right operation of this system. In android application when we press a button, a corresponding signal is sent through the Bluetooth to Bluetooth module (HC-05) which is connected with the Arduino board. Now that pin gives the output to the motor driver section. Motor driver switches accordingly the data bit, if the data bit is low then the corresponding pin of the motor driver doesn't work else highbit then thecorrespondingpinof the motor driver is on. There are two steps of the programming. First set up section where we define all the variables. Second loop part where the program runs continuously. The working principle is kept as simple as possible. The microcontroller, thereby, sends instructions, which when executed, helps in functioning of the motor driver. Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. Our proposed project consists of the following three sections: a) Input section b) Microcontroller section c) Output section In our android application base Bluetooth controlled robotic car, the user interacts with the system with a smart phone. In this method user must be present within in range (< 15 meters) to control the system. In future we would try to extend the range using Internet of Things (IoT)  [12] . When user sends any data to the Arduino board then the corresponding pin of Arduino goes to high state and switches the motor driver ic in the on mode. The corresponding motor moves as per the input data. These commands help the microcontroller to interface with the Bluetooth module HC-05 and also with the motor driver IC L293D. Here the Bluetooth module act as a receiver which receives the instruction from the smart phone (remote or transmitter).
paper_333	 The aim of this study is to design a model for Enset diseases diagnosis using Image processing and Multiclass SVM techniques. This diagnosis apply K-means clustering, color distribution, shape measurements, Gabor texture extraction and wavelet transform as key approaches for image processing techniques. Bacterial Wilt and Fusarium Wilt disease and collected 430 Enset leaf images from Areka agricultural research center and some selected areas in SNNPR. The proposed model demonstrated with four different kernels, and the overall result indicates that the RBF Kernel achieves the highest accuracy as 94.04% and 92.44% for bacterial wilt and fusarium wilt respectively. Food security is a challenge in many developing countries like Ethiopia. Around 80% to 85% of people in Ethiopia are dependent on agriculture; among these more than 20% of them depend on Enset crop production in the country. The government committed 15% to 17% of expenditures to the sector; it covers 41% of gross domestic product (GDP), over 90% of export value and directly supports 80% of the populations' livelihoods  [1] . Cheesman. The visual observation of the experts is the main approach that commonly used for detection and identification of such plant diseases. There is a potential need for technology supported alternative systems to support the manual identification of Enset crop diseases so as to optimize the accuracy for remedial action. In our country few researchers found the promising solutions to different plant diseases diagnosis such as maize, rose flower, coffee and others using computer vision and machine learning techniques  [3, 4, 5] . The remaining part of this paper is organized as follows. Experimental results are reported in Section IV. Section V presents the conclusion and discusses of future works. So far a number of fungal, nematode, viral and bacterial diseases were reported to cause damage at different degrees of intensity that was mainly explored by Quimio research finding since 1992. Those images are collected by using a digital camera and some of them are collected from secondary sources like research centers which collects pictures for visual inspection of Enset disease. From all those disease category a total of 460 Enset leaf images are collected from which 368 is used for training and 92 images are used for testing. B. Test Result For the experimentation a total of fifteen experimental setups which is color, texture morphological and a combination of each and a multiclass Kernel support vector machine model: Linear Kernel, polynomial Kernel, RBF kernel and Quadratic Kernel was used. From the total dataset 20% is used for testing and the distribution of each disease category is shown in table 2. In order to measure the performance of the classifiers K-fold (in this case k is 10) cross validation method was used. From the figure it is shown that the RBF kernel classifier has classified the given enset image in its correct class category by 94.04% and 92.44% respectively. We designed a system which can detect Enset disease automatically and this will help the farmers to detect the diseases in its early stage and to take relevant action.
paper_389	 The experimental corpus has been tested by Changjiang Daily for many years. Experiments are carried out to analyze the influence of the choice of conditional random field model parameters and the selection of Chinese character annotation sets on the experimental results. Word probability, the paper explores the probability characteristic of word location. Common Chinese character tagging method is based on the Chinese characters appear in the words of different locations marked different labels. For example, "O" can be used to represent individual Chinese characters, "B" means Chinese characters appear in the head, "I" means Chinese characters appear in the middle or the end of the word. We first set three candidate marks "O", "B" and "I" for each Chinese character and add a start node "BOS" to the head, An end node "EOS". According to the meaning of "O", "B" and "I", the rules are summarized as follows: 1, the sentence of the first Chinese character tag can not be I, the last character of the mark can not be B. The probability of the i-character of the position as the prefix: pY> 95% 昨狭抨第竞耽 3 Z The probability of the i-character as the suffix: pZ> 95% 丸役袄侣丸瞰 4 R The probability of the i-character individual word of the position: 85% <= pR <= 95% 吧枚刘磊躺却 5 U Position of the i-character as a prefix of the probability: formula_0 The probability of the i-character as the suffix: 85% <= pV<= 95% 貌型胁帘午岸 7 D The probability of the i-character position of the individual word: pD <= 5% 言辽改信申仪 8 E The probability of the i-character of the position as the prefix: pE <= 5% 业络王场姆杨 9 F The probability of the i-character as the suffix: pF<= 5% 增隐晋香浓伊 In order to deal with the long-distance information, this paper takes the context distance as 2. The probabilities of the position probabilities in  Table 3  are extracted from the training corpus and the probability of each position is calculated according to the following probability formula: P (individual word) = number of occurrences of the individual idiom of the word / total number of occurrences of the word * 100% (1) P (prefix) = number of occurrences of the word as a prefix / total number of occurrences of the word * 100% (2) P (suffix) = number of occurrences of the word as a suffix / total number of occurrences of the word * 100%  3 After many experiments and comparisons, the probability of more than 85% or probability of less than 5% of the word as a location probability feature, but also on the selected word for some filtering, the elimination of some unnecessary words, such as At the same time, select the probability of greater than or equal to 85% of the word is subdivided into the probability of 85% <p <95% and p> 95% of the two sets. Appropriate increase of p> 95% of the number of occurrences is in order to improve the characteristics of the sample expectations, and achieved good word effect. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. Combinations: A combination of 2 or 5 different word or string features at different positions. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. CRF word segmentation system is a word segmentation system based on conditional random field, and the Chinese word segmentation method is adopted. Then the training corpus is trained to generate a CRF model, and some training parameters such as iteration number are added in the training process. The coding method is GB code. The corpus content mainly comes from newspaper news. The format consists of a sentence segment consisting of words marked with spaces. The performance of a word segmentation system mainly depends on two aspects: segmentation precision and word segmentation speed. The formula is as follows: Correct rate P = number of words correctly recognized / total number of system output words * 100% Recall rate IP correctly identify the number of words / test words in the total number of words * 100% F value F=*P*R/(P+R)*100％ The speed of word segmentation is another important index of word segmentation performance. The main factors that affect the speed of word segmentation are the structure of word segmentation dictionary and word segmentation algorithm. As the automatic word segmentation of the various types of knowledge to be obtained from the word dictionary, the system in the word processing needs frequent query word dictionary, word dictionary query speed will directly affect the speed of the word segmentation system. In addition, automatic word segmentation system should also be easy to expand, maintainability and portability; to support different regions, different application areas of different application goals; vocabulary and processing functions, processing methods can be flexible combination of loading and unloading, thereby enhancing the system Processing precision and processing speed; also, to build a "information processing with the modern Chinese word segmentation standard" to match the common or common modern Chinese word segmentation. Moreover, since the word position probability feature is extracted completely from the training corpus, some of the participle criterion information of the corpus is extracted to a certain extent, so that when the test set is tested, Corpus. In order to compare the availability of the conditional random field model, we also established two word segmentation models: Hidden Markov Model (HMM) segmentation model and Maximum Entropy (MEM) segmentation model. In the experiment, only some feature information is used, and most of the features are extracted from the training corpus, we have achieved good results. As the Yangtze River Daily corpus is from the newspaper news, for the news corpus in the special format, such as title, poetry, weather forecasting, etc., to our model training has a certain impact, so if the corpus in the handling of these disturbances, Our model should have better performance.
paper_391	 Thus, the aim of this study was to identify the best classifier, and to predict the pattern from the TT data set using the data mining algorithms technique. The data for this study were the Tetanus Toxoid data set from the Ethiopian Demographic and Health Survey (EDHS) 2011, and analyzed using the Knowledge discovery process of Selection, Processing, Transforming, mining, and interpretation. The WEKA 3.6.1 tool was used for classification, clustering, association and attribute selection. The accuracy rate of the classifiers on training data is relatively higher than on test data and the multilayer perceptron is the best classifier in our data set on Tetanus toxoid. Single data instance test using Naïve Bayesian was done by creating test 1, test 2, test 3, and test 4 data test instance, three of them are correctly predicted but one of them incorrectly classified. The literacy status of the mother has high information gain with the value 0.046. Tetanus is caused by a toxin produced during the anaerobic growth of Clostridium tetani. Infection is acquired through environmental exposure of any broken skin or dead tissue such as a wound or when the umbilical cord is cut to the spores of the bacteria. WHO estimates that only 5% of Neonatal Tetanus (NNT) cases are reported, even from countries with well-developed surveillance systems  [2] . Globally tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually, accounting for up to 25% neonate's death in some African countries  [3] [4] [5] . Ethiopia's Expanded Program on Immunization (EPI) started in 1980 and remains the single most important component of primary health care supported by the Ministry of Health. The vaccine to prevent Maternal Neonatal Tetanus (MNT) introduced as part of routine immunization programs in over 100 countries by the end of 2011. Five doses of TT can ensure protection throughout the reproductive years and even longer. It is a young and fast-growing field also known as knowledge discovery from data (KDD) and used for discovering interesting patterns from data in various applications  [7] . This health care data include relevant information about Client, their treatment, and resource management data. The information is rich and massive. In this study, we have used several data mining techniques; classification, clustering, association and outlier detection techniques over the nominated Tetanus Toxoid immunization dataset of the EDHS 11. The  Figure 1 (Adapted from  [7] ) shows, the basic phases of the knowledge discovery from data, we have undergone. Those patterns that remain represent the discovered knowledge. The EDHS 2011 was conducted by the Central Statistical Agency (CSA) in collaboration with Measure DHS and ICF International under the auspices of the Ethiopian Ministry of Health (EMoH). It has a dimension of 7033 rows and 12 columns. Data values and attributes were modified, added and/or deleted, filtered, recorded, dropped the missing values, transformed and attributes are integrated in order to be used by the machine learning techniques in the analysis step for the study. arff" file format. Such models, called classifiers, predict categorical class labels (nominal, ordinal). The classification has numerous applications, including fraud detection, target marketing, performance prediction, manufacturing, and medical diagnosis. How does this classification work? Decision tree (J48) approach It is a flowchart-like a tree structure. Where each branch represents an outcome of the test, each internal node denotes a test on an attribute, and each leaf node holds a class label. [8, 9]  (b). They can predict class membership probabilities like, the probability that a given tuple belongs to a particular class  [10, 11] . Confusion matrix; is used in this study, include accuracy, sensitivity, specificity, and precision. Understanding them will make it easy to grasp the meaning of the various measures. True   In K-fold cross-validation, the initial data are randomly partitioned into k mutually exclusive folds, D1, D2 … DK, each of approximately equal size. Training and testing are performed k times. To discover acceptable classes using Simple K-Means based on the principle of maximizing the similarity between objects in the same class i.e., intra-class similarity and minimizing the similarity between objects of different classes i.e., inter-class similarity  [7] . "How does the k-means ( ) algorithm work?" For each of the rest objects, an object is assigned to the cluster to which it is the most similar, based on the Euclidean distance between the object and the cluster mean. From the selected 7037 mothers, 3351 of mothers received TT Immunizations. The 5680 of mothers were from rural Ethiopia, and more of them (3484) were in the age range from 25-34. (Table 1)  As can be seen in  Figure 2 , the accuracy rate of the classifiers on training data is relatively high. (Table 3)  Simple K-Means preferred the method of clustering for this project we have adjusted the attributes of our cluster algorithm by clicking Simple K-Means. The only attribute of the algorithm we are interested in adjusting here is the "Num clusters field", which tells us how many clusters into five based on the instruction given. Briefly described as follows: Cluster 0-This group have 1534 (27%) instances of which consist of mothers who live in the rural, no access to radio, no access to TV, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 1-This group consists of 1004 (18%) instances of which mothers who live in the rural, no access to radio, access to TV yes, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, head of household male, tt-vaccinated yes. Cluster 2-This group consists of 2063 (37%) instances of which mothers who live in the rural, no access to radio, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated No. Cluster 3-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 4-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, orthodox in religion, Amhara in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education no education, women age 35-49, mother marital status married, hh_head Female, tt-vaccinated yes. As explained in the    The literacy status of the mother has high information gain (0.046) and followed by the literacy status of the mother (0.041) and the least information gained was head of household (0.00000147). (Figure 3 ) In this study, the data mining tool and algorithm like (J48, k-nearest, and Bayes) is used for selecting the training and test data, for classification, and K-means approach for clustering, single-dimension association rule to identify the best association.
paper_402	 As a result, the BPNN with the selected features was able to interpret more accurate results (r 2 = 0.991) than the model with all the features (r 2 = 0.816). The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. Specifically, ANN has been used to solve a wide variety of civil engineering problems  [2] [3] [4] . Mainly, ANN was utilized to model the nonlinear behaviour of fatigue and creep of Reinforced Concrete (RC) members  [5] [6] [7] [8] . Recently, research interest has revolved around the development of ANN models to interpret the behaviour of structural materials such as steel, concrete, and composites  [9] [10] [11] [12] [13] [14] . The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete  [14] [15] [16] [17] . Afterwards, several authors began developing ANN models for the prediction of compressive strength of high performance concrete  [18] [19] [20] [21] . In this study ANN is employed with other machine learning techniques to identify the parameters that capture the compressive strength of UHPC using data collected from the literature. Several researchers have been investigating the mechanical behaviour of UHPC and its applications over the last four decade, where it was founded that UHPC exhibits a compressive strength that would range from 150 to 810 MPa  [22, 23] . Ghafari et al. [19]  used the back-propagation neural network (BPNN) and statistical mixture design (SMD) in predicting the required performance of UHPC. Meaning, the model does not produce any analytical model with a mathematical structure that can be studied. This will reduce the amount of parameters in the model, which will improve the computation complexity of the ANN model and simplify the derivation strategies of a mathematical model used to predict the compressive strength of UHPC. In addition prediction of compressive strength of high strength and high performance concrete was addressed by other researchers  [20, 21] . Four sets of open human motion data and two types of machine learning algorithms were used. Four types of machine learning algorithms were used as wrappers for the SFS. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. There are two types of search algorithms: sequential forward selection and sequential backward selection. In this study, the previously verified ANN model was used as the objective function and the forward selection was used in selecting the relevant features. Figure 1  shows the algorithm SFS uses when performing forward selection. Figure 2  shows the plot of all the scenarios with the minimum point circled at 11 neurons. As a result, the model that used the selected features showed stronger agreement with the experimental results in contrast with that prior to the selection. Table 3  shows the statistical measurements calculated for both cases. As a result, the ANN model with the relevant features was capable of predicting 89.6% of its values within the aforementioned boundaries, as opposed to the ANN model with all the features which predicted 58.7% of its values within the boundaries. fc = θ1C + θ2SI + θ3FA + θ4W (2)     Since the developed LSR model is capable of accurately predicting the experimentally measured compressive strength, a parametric study was conducted, using this model, to study the effect of Fly Ash and Silica Fume on the compressive strength of UHPC. It can be concluded from this study that: 1) The use of ANN with SFS reduced the number of input parameters needed to accurately predict the compressive strength of UHPC mix for the prediction of compressive strength, making it less computationally expensive. 3) The ANN model with the selected relevant input parameters also showed a lower deviation (89.6 %) than the ANN model with all the features (58.7%). 4) LSR was implemented using the selected input parameters to develop an analytical model that can be used to accurately predict the compressive strength of UHPC.
paper_418	 An important aspect of descriptive time series analysis is the choice of model for time series decomposition. This paper examined the challenges in choosing between additive and mixed models in time series decomposition. Table 1 shows that the column variances of Buys-Ballot table is constant for additive model but depends on slope and seasonal effects for mixed model. Results show that seasonal variances of the Buys-Ballot table is constant for additive model and a function of slope and seasonal effects for mixed model. Also, when there is no trend (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. An important goal in time series analysis is the decomposition of a series into a set of non-observable (latent) components that can be associated to different types of temporal variations  [1] . For equation  (1)  it is assumed that the error term t e is the Gaussian white noise ( )  For equation  (2)  it is equally assumed that the error term t e is the Gaussian white noise ( )  An additive model is based on the assumption that the sum of the components is equal to the unadjusted data. The additive model was adopted when the magnitude of the seasonal pattern does not change as the series goes up and down while the additive model was adopted when the magnitude of the seasonal pattern does not change as the series goes up and down  [5] . An important aspect of descriptive time series analysis is the choice of model for time series decomposition. Iwueze and Akpanta  [6]  pointed out that an additive model is appropriate when the seasonal standard deviations show no appreciable increase or decrease relative to any increase or decrease in the seasonal means while a multiplicative model is usually appropriate when the seasonal standard deviations show appreciable increase/decrease relative to any increase or decrease in the seasonal means. Linde  [7]  observed that, the differences between the Additive and Multiplicative and the models are (i) for the additive model, the seasonal variation is independent of the absolute level of the time series, but it takes approximately the same magnitude each year while in the multiplicative model, the seasonal variation takes the same relative magnitude each year. The implication of this is that when the test for constant variance says the appropriate model for a study series is not the additive model; an analyst still faces the challenge of distinguishing between mixed model and the multiplicative model. For a series that has linear trend, the row, column and overall means and variances of the Buys-Ballot table for additive and mixed models are obtained by Iwueze and Nwogu  [14] , Nwogu el al  [15]  are given in  Table 1 . The column variances of the Buys-Ballot table is constant for additive model, but a function of slope and seasonal indices for the mixed model. formula_5 Source: Iwueze and Nwogu (2014), Nwogu el al (2019). These properties of row, column and overall averages and variances of the Buys-Ballot table are what could be used for estimation and assessment of trend parameters, estimation and assessment of seasonal indices and choice of the appropriate for time series decomposition. That is when 0 b =  (Table 1)  For mixed model, we obtain using the expression in  Table  1  ( ) ( )  formula_8 when there is no trend. formula_10 When there is no trend ( 0 b = ) we obtain from  (Table 1)  . The data of 120 observations has been arranged in a Buys-Ballot table as monthly data (s = 12) and for 10 years (m = 10). The expressing of a linear trend and seasonal indices for an additive model is given as    This paper has discussed the Buys-Ballot procedure for comparing the row, column and overall means and variances of the Buys-Ballot table for additive and mixed models in time series decomposition when trend-cycle component is linear. Results show that seasonal variances of the Buys-Ballot table is constant for additive model and a function of slope and seasonal effects for mixed model. Also, when there is no trend ( ) 0 b = , the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models.
paper_428	 The effects of different pertinent physical parameter such as magnetic parameter, Williamson parameter, radiation parameter and Prandtl number on temperature and velocity distributions are observed through graph. In Non-Newtonian fluids, the most frequently encountered fluids are pseudoplastic fluids, and Navier-stokes equations alone are insufficient to describe the rheological properties of these fluids, therefore, to overcome this defect, several rheological model like Ellis model, Power law model, Carreaus model and Cross model are presented, but little attention has been compensated to the Williamson fluid model and estimated to explain the rheological properties of pseudoplastic fluids. In this model both maximum viscosity μ (viscosity as shear rate tends to infinity) and minimum viscosity μ (viscosity as shear rate tends to zero) are to be taken. Khan et al. Azimi et al. Mixed convection is a coupled phantasm of two heat transfer mechanisms force convection and natural convection that act simultaneously to transfer heat in a fluid flow. It play significant role in field of technology. Merkin et al. Bau investigated the thermal convection in a saturated stratified medium bounded between two parallel eccentric cylinders with the help of a regular perturbation expansion along Daarcy-Rayleigh number; it was observed that the appropriate preference of eccentricity values can maximize the heat transfer inside annulus of various thermal insulators  [37] . Fu et al. investigated flow reversal of mixed convection in a three dimensional channel and concluded that an increase in Richardson number, natural convection dominates the flow and thermal field of combine convection  [39] . Kaya found nonsimilar solutions of steady laminar mixed convection heat transfer flow from a perpendicular cone in a porous medium with influence of radiation, conduction, interaction and having high porosity  [40] . Jafari et al. Bég et al. Chaudhary and Jain studied the impact of mass transfer, radiation and hall on MHD mixed convection flow of viscoelastic fluid in an infinite plate  [43] . Malleswaran & Sivasankaran carried an analysis for mixed convection flow and noticed that the average heat transfer decreases with an increase in Richardson number but in general heat transfer is better at force convection mode than free convection mode  [45] . The skeletal part of porous wedge is known as Frame and it is frequently a solid, but structures akin to foams. General example of porous wedge is sand, soil, sandstone and foams. Ashraf et al. E 0 = E 6 , E′ 0 = 0, F 0 = 1 (9) E′ ∞ = 1, F ∞ = 0 Where E 6 is injection/suction parameter. The system of ordinary differential equations 7 and 8 subject to the boundary conditions 9 and 10 is first reduced to a system of first order ordinary differential equations using the substitutions E K =`, $ K = a, F K = b. This With the boundary conditions E 0 E 6 , $ 0 0, F 0 1, $ ∞ 1, F ∞ 0 (13) The resulting system in Eq. (11) (12) (13)  is solved numerically with the help of 5 th order Runge-Kutta-Fehlberg method. The transformed governing equations  (11) (12)  subjected to boundary conditions  (13)  are solved numerically by employing the fifth order Runge-Kutta-Fehlberg method. The agreement of our work with the prior results is stable. Figures (1-2)  illustrate the influence of wedge angle parameter < with on velocity and temperature profile. Since the wedge angle parameter < is a dependent over the pressure gradient, and its values may be positive or negative. It is observed from graph that with an increase in magnetic parameter M, there is a decline in the velocity distribution. Figure 6  drafts the non-dimensional velocity E′ for different values of suction parameter E 6 . Figures 7-8  illustrate the behavior of thermal radiation and Prandtl number on fluid flow region with M= E 6 = O = < =1.
paper_432	 The mathematical analysis method used. The Cotangent Bundles * of manifold d provides the basic model of a symplectic manifold. In case M is model on Euclidean space we have ≈ and so we want to assume that * ≈ * . We assume that is 1-periodic in time and grows a symptotically quadratically on each fiber. Generically, the corresponding Hamiltonian system `(%) =& H '%, % ). Given any , manifold , of dimensionn, with -. 1, for any 0 ∈ , the tangent space at 0, denoted , is the space of linear derivations on 1 2, , that vanish on 3 2, , . Thus,T 5 M can be identified with (1 2, , /3 2, , )* the space formula_0 , is called the cotangent space at 0; it is isomorphic to the dual * , of M . Observe that if = 06 ∘ 8, as  (,)  , and ( 1 2, (,) / 3 2, (,) ) ** described above, DC corresponds to the linear map in T 5 * (M) . We see that(E > ) ,…, (E ) is a basis of T 5 * (M). formula_1 [8] The Cotangent Space T 5 * (M) of a manifold at 0 ∈ is defined as the dual vector space to the tangent spaceT 5 M. A dual vector space is defined as follow ∶ given an − dimensional vector space G, with basis H , I=1, 2, 3,…, , the basis J = of the dual space G * is determined by the inner product. When we take the basis vectors H =  The Cotangent Bundle Τ * contains the following classes of Lagrangian submanifolds; The fibers of Τ * . Let ∈ and let I; T { * Q → Τ * be the natural inclusion mapping. Since  formula_3  formula_7 which is the change of variables formula for integrals. [9]   Let (H, |,`, 0) be a locally trivial fiber space whose total space and base space are path -connected and & a pathconnected topological space. For the mapping.∅: & → | to have a lift ‹ satisfying the condition ( ‚ ) =J ‚ , where ‚ ∈ &,J ‚ ∈ H, 0 (J ‚ ) =d 0 =( ‚ ), it is necessary that∅ (π n (&, ‚ )) ⊆0 (a n (H,J ‚ )) (3. For cOEOE . From which the required inclusions follow easily. A , principal bundles is a quintuple (0, , , R , where: 0 V →0 is a , right at action with the property of local triviality: Each point e ∈ has an open neighborhood for which there exists a , -diffeomorphism. 8 ' ( K , n, ' ) = 8 ' ( K, n ). ' (n, ) = (n`n, ). these are easily seen to be free and proper. [n, ] = [n,`n, ] . Now consider a G on a manifold. A tube for the action at ± is a −equivariant diffeomorphism from some twisted product × ° to onopen neighborhood of ±in , that maps [J, 0] H to ±. It is applications has been limited by the fact the proof is no constructive. The symplectic reduction of the cotangent bundles * Q has more structure than a symplectic manifold. [17]   Let act freely and properly by cotangent lifts on * Q, and let Jbe the momentum map of the G action (with respect to the canonical symplectic form on * Q. Leta ³ : → / is projection. Define the map, ∅ : ´s > (0) → * ( / ) by, for every 0 ∈ µ * Q. The left -hand side has the reduced symplectic form corresponding to the canonical symplectic form on T * Q , and T * (Q G ⁄ ) has the canonical symplectic form. Note that ∅ is "injective mod '', meaning that ∅(± 1 ) =∅(± 2 ) if and only if ± > =n.
paper_444	 Resource constrained project scheduling problem is to make a schedule for minimizing of the completion time or total cost subject to precedence rules and resource constraints. The goals of the model are to minimize the completion time and the total cost which composed by the activity cost and the additional resource cost. The other constraint is the resource constraint in which the demand of resource shall not exceed the total supply of resource for each resource type at any time. The resource constrained project scheduling problem (RCPSP) takes into account the balance of makespan and total cost through resource allocation and reasonable activity schedules while the precedence constrains between the activities and resource constrains are all satisfied. Xie  [9]  supposed that the increased quantities of resource were real-value variables, and built a multi-mode resource constrained project scheduling model for minimizing both the project makespan and cost. Its goal was minimizing the expected weighted sum of the absolute deviations between the planned and the actually realized activity starting times. Its constrains were the resource and priority rules. Chen  [11]  developed a project scheduling problem model under fuzzy resource constrained, of which the fuzzy duration time and fuzzy resource availability were represented by triangular fuzzy number. Then, we build a multi-objective model which is under resource and precedence rule constrained to minimize the resource cost and the project completion time. The construction of this paper is organized as follows. A multi-objective programming model is built under the resource restricts and precedence rules of activities constrains in order to balance the completion time and the total cost of the project. (4) No interruption is allowed for each activity in progress. " : The set of underway activities at time #. The uncertain resource constrained time-cost trade-off problem can be described as following optimization model: formula_0 In the above model, objective ○ 1 is to minimize the project total completion time; Objective ○ 2 is to minimize the expected project total cost which consists of the activity cost and the additional resource cost. Constraint ○ 3 declares that finish-start precedence relation among project activities. Constraint ○ 5 illustrates the total cost of project has two pasts. The other is additional resource costs, in which means the product of the cost per time unit of additional resource, project completion time and the increased quantities of resource . Constraint ○ 6 shows the range of decision variables. Theorem 1. Then excepted value of G is formula_1 Theorem 2. Theorem 3. The manager tends to the demand of resource shall not exceed the total resource supply with at least given confidence level 0.9 for any time # and each resource type . The cost per time unit of additional resource ! is 280 yuan. The constrains are recourse constraint and precedence constraint. Table 1 . The information of the activities. Cost  Preceding activity  1  0  0  0  0  2  1  5  1200  0  3  5  24  28800  2  4  27  16  103680 3  5  30  24  172800 3  6  22  24  126720 4  7  19  9  41040  4  8  24  6  34560  3  9  12  4  11520  8  10  22  20  105600 7  11  7  8  13440  6  12  20  8  38400  7  13  24  17  97920  5  14  12  6  17280  4  15  16  4  15360  7  16  11  6  15840  6  17  10  8  19200  14  18  8  6  11520  2  19  24  11  63360  18  20  13  4  12480  6  21  9  4  8640  12/15  22  3  4  2880  21  23  4  4  3840  7  24  0  0  0  22  With the above demand, we can present the following model:  In the real-life project, due to the influence of uncertain environment, managers should consider the trade-off between the completion time and cost.
paper_462	 "5 + 3" new training mode of training in combination. Since the implementation of the new training mode, the quality of postgraduate training has been significantly improved, the employment rate of graduates has been steadily improved, the influence of the school has been expanded, and the experience for relevant units to carry out the reform of postgraduate training mode of clinical medicine master's degree has also been provided for reference. Chongqing Medical University was founded in 1956. It was founded by the former Shanghai First Medical College (now Shanghai Medical College of Fudan University) and moved to Chongqing. Pilot units for professional degrees. In view of the main problems existing in clinical master education, the school has carried out a series of reforms and explorations on the organic link between clinical master education and standardized resident training (hereinafter referred to as standard training). For a long time, the medical postgraduates trained in our country are seriously lacking in practical operation ability, resulting in the embarrassing situation that "medical doctor will not see a doctor". In the long-term practice of training clinical master, Chongqing Medical University has gradually explored a new "5+3" training mode which combines clinical master education with regular training, and completed the educational practice of "Innovation and Practice of the Training Mode of Clinical Master Professional Degree in China"  [3] . It has effectively solved the problems of imperfect training mode of clinical master, lack of mature training mode of high-level clinicians, low overall training quality of clinical master, inability of clinical master to apply for the qualification of practicing doctor, difficulty of effective connection with regular training, difficulty of seamless connection between professional degree education and industry admission standards, and inadaptability of current management system and mechanism to professional degree postgraduates. The reform of training mode for clinical master of Chongqing Medical University is premised on defining training objectives, based on innovative training mode, with improving training quality as the core and linking up with professional qualification as the grasp  [4] . To vigorously develop professional degree postgraduate education and improve the quality of training high-level applied talents is the focus of the current degree and postgraduate education work. The school accurately grasps the law of professional degree postgraduate education, changes the concept of professional degree postgraduate education, and orientates the training purpose of clinical master as "training doctors who can really see a doctor", aiming to improve the clinical practice ability of postgraduates as the main objective, and is in line with the training goal, thus laying a solid foundation for the organic connection between the two. In order to effectively improve the quality of clinical master training and strengthen the management of clinical rotation process, the school has set up postgraduate management offices in clinical colleges, implemented the system of professional degree tutorial group, and established three-level management systems of schools (graduate schools), departments (graduate management offices) and clinical departments (tutorial groups)  [6] . The tutorial responsibility system is applied in postgraduate training, but after clinical master's enters the clinical rotation, many times are not in the Department where the tutor is located, and there is a problem that no one manages the professional degree students when they rotate in the clinical departments outside the department  [7] . The tutor who applied for the postgraduate examination is the first tutor (defense tutor). Each clinical rotating Department of postgraduate clinical rotation appoints a teacher qualified as a professional master's tutor as rotating responsibility tutor to ensure that clinical master's is in clinical rotation. Effective management during the transition period. In order to consolidate the reform results of the training mode of clinical master, gradually standardize the management system and continuously improve the quality of training, the school continuously improves the rules and systems around the education of clinical master, covering the methods of re-examination admission, the quantitative assessment of the selection of tutors, the training program, the tutor group system, the curriculum, the professional course examination, the assessment of clinical ability, the regulation of publishing papers, the management of research funds and institutions  [8] . In terms of setting up, a complete system of management rules for clinical master's degree has been established, and the experience of reform has been standardized and institutionalized, thus forming a long-term mechanism for linking professional degree education with professional qualification certification. In order to speed up the development of professional degree postgraduate education, the school has formulated an enrollment policy conducive to the development of professional degree  [9] . The deputy chief physician who meets the age requirement can recruit professional degree postgraduates, and more than two postgraduates must have one professional degree. Through the above measures, the problem of low enthusiasm of faculties and tutors in guiding graduate students with professional degrees has been solved, and the construction of professional degree tutors has been accelerated. In view of the fact that the subsidy of clinical master is much lower than that of regular trainees during the clinical rotation period, the school has made many investigations and repeated demonstrations to improve the treatment of Postgraduates during the clinical rotation period and to improve the enthusiasm of students in clinical training  [10] . Aiming at the characteristics of professional degree postgraduate education and adapting to the innovation of its training mode, the school has established a funding system different from academic degree postgraduates, and continuously improves the standards and coverage of grants. First of all, in order to ensure the clinical rotation time, the master of clinical courses in our school are arranged uniformly in the evening and weekend of the first semester. It is closely related to clinical practice. The curriculum involves many independent modules, such as clinical research methods, clinical diagnostics, internal medicine, surgery and so on  [11] . Thirdly, according to the requirements of standardized resident training, and with the cooperation of training bases, starting from medical ethics, medical ethics, laws and regulations, professional ethics and basic clinical skills, we will offer lectures on medical law, applied psychology, humanistic literacy and doctor-patient communication to comprehensively improve the comprehensive quality of clinical master. Therefore, the training object, the training teachers and the responsible subjects can be combined into one, which provides a common basis for the integration of training. Requirements for the first stage of training. Therefore, according to their own characteristics, each unit has formulated a variety of assessment indicators system, and its level is uneven, resulting in different clinical ability of clinical master. There are mainly "pre-job training", "centralized departure assessment", "stage assessment" and "annual assessment"  [13] . The school has established a multi-level and whole-process clinical competence assessment system, which is suitable for clinical master's clinical competence assessment and regular training and graduation assessment. Before graduation, clinical masters not only have to defend their dissertations, but also have to pass strict clinical competence assessment, which increases the workload of students. The staff of the Graduate Management Department of the school conducted a thorough investigation on the effect, problems and suggestions of the reform of the training mode of clinical master in various clinical colleges and departments, and organized a meeting to solve some common problems. Fourth, direct link the workload of tutors' guidance to professional degree postgraduates with the promotion of their professional titles, so as to improve the enthusiasm of tutors' guidance to professional degree postgraduates. There are three difficult problems in the training process of master's degree postgraduates of clinical medicine specialty: first, the graduates of clinical medicine specialty with bachelor's degree must work in medical institutions for one year before they can apply for the qualification of practicing physician; second, the graduates with bachelor's degree can't obtain the qualification certificate of practicing physician and can't carry out clinical training; third, some of the graduates with bachelor's degree must work in medical institutions for one year. By promoting the combination of clinical master training and regular training, the problem of clinical master qualification certification has been solved, and a new "5 + 3" training mode has been constructed. This model is based on practice in an all-round way, does not need the national single enrollment index, does not need the national special allocation, has the characteristics of innovation, practicability, commonality, feasibility, and has the realistic basis for comprehensive promotion and implementation. The school has thoroughly reformed the training mode of clinical master, organically docked school education with post-graduation education and lifelong education, promoted the seamless docking of professional degree education with industry access standards, accelerated the internationalization process of higher medical personnel training mode, and formed a new systematic training mode for clinical doctors, which has great application value in the whole country. In view of the characteristics of professional degree postgraduate education, the school grasps the development trend of postgraduate education, and closely combines the admission criteria with the industry admission criteria in all aspects of enrollment, cultivation and award of posts, which ensures the complete docking of personnel training and qualifications, and provides mature experience for the seamless docking of professional degree education and Industry admission criteria in China. It provides a mature reference model for the docking of professional degree postgraduate education and industry access standards in other fields in China. The new "5+3" training mode built by the school covers all aspects of training, such as curriculum system, assessment system, award system, award system, rotation system, tutor system and management mode. Benefit. It has overcome many obstacles and steadfastly promoted the reform. Dozens of brothers such as Fudan University learn from our experience. This model is in line with the reality of the development of medical and health care and higher education in China. The employment rate of graduates has been guaranteed to be 100% for a long time. The article "Construction and Practice of Quality Assurance System for Master of Clinical Medicine Degree Postgraduates" published in "Degree and Graduate Education" won the second prize for Excellent Thesis of "Degree and Graduate Education", and it is also the only prize-winning thesis in the field of medical science. Since the implementation of the reform of the training mode of clinical master, the combination of degree education and vocational education in our school has become closer. Professor Jin Xianqing and Professor Xie Peng were appointed members of the National Medical Graduate Education Steering Committee in 1998 and 2010 respectively. The school assists in formulating the standardized training policy for general practitioners and residents in Chongqing, the construction of bases, the training and assessment system, and teacher training. The implementation of "three combinations", namely, the combination of enrollment and enrollment, training and training, degree award and physician access. The "four certificates association" should be implemented, namely "licensed physician qualification certificate", "standard training qualification certificate", "diploma certificate" and "degree certificate"  [17] . We should build "five guarantees", innovate the management system and mechanism, and ensure that the reform is in place. It has effectively solved the problems of imperfect training mode of clinical master, lack of mature training mode of high-level clinicians, low overall training quality of clinical master, inability to apply for the qualification of licensed physician, difficulty in effectively linking up with regular training, and difficulty in seamlessly linking professional degree education with industry access standards. Chongqing Medical University combines many years of clinical master's education practice, highlights its own characteristics, gives full play to the advantages of running a school, and strives to solve the problem of linking professional degree education with vocational qualification certification. The unit pays attention to the degree education of clinical medicine specialty, strengthens the exchange study, improves the quality of medical higher education in our country, realizes the seamless connection between degree education of clinical medicine specialty and professional qualification certification, and explores the new mode of integrating medical education of our country with international practice.
paper_476	 The inverse relationship of frequency with cost and operational efficiency becomes the key to the decision of delivery frequency. This paper uses JD Logistics as an example to quantitatively analyze the relationship mentioned above. The results show that: (1) The cost and resources operational efficiency are closely related to the order splitting ratio under the same delivery frequency; (2) The delivery frequency has different effects on the operational efficiency of the resources in different links. As the volume of orders and consumers' demand of rapid delivery services increasing, express companies are required to increase the daily delivery frequency to cope with the pressure of delivery during peak period and meet the consumers' needs effectively. However, the increasing in delivery frequency will lead to changes in the workload of different resources (distributors, facilities, equipment, etc.) Fan Xuemei et al. Jesus et al. This paper uses Jingdong Logistics (JDL for short) as research objects. Next, it uses the simulation model to study the effect of changes in delivery frequency on delivery costs and resource operation efficiency under different quantity of delivery orders. The equipment usage costs can be divided into two parts: the fixed costs and the variable costs. The fixed costs occurred in the use of equipment in the three links. The variable costs occurred due to the volume of orders, which include operating cost of the sorting equipment, rental cost of the site, units fixed cost of transport vehicle, fuel costs, toll and so on. In addition, the number of working facilities and the operating time are affected by factors such as order quantity, delivery frequency, sorting equipment efficiency, and unit load of transport vehicles. Labor costs are composed of fixed wages and performance wages for employees in the three links mentioned above. The employees include direct employees who are vehicle drivers and indirect employees who are managers. The number of employees is affected by factors such as order quantity, transportation efficiency, delivery frequency, and the number of transport vehicles. Sorting staffs consist of sorters and on-site logistics personnel. The sorter is the person who operates on the sorting equipment. The on-site logistics personnel is the person who puts the sorted goods into a mail packages and places them on a pallet and then sends it to the transportation vehicles. The number of sorting staffs is influenced by factors such as the amount of cargo, the number of sorting equipment, the worker's efficiency, the sorting time requirements, the area of sorting venues, the number of logistics personnel in the yard, the delivery frequency and so on. This article considers the utilization efficiency for the facilities, personnel, and technology such as turnover rate of transportation vehicles, the utilization rate of delivery site, and the delivery personnel loading rate, etc., which are put into the sorting, transportation, and terminal delivery links. The utilization of the site space is obtained by dividing the actual leased area of the site by the available area. The usable area of the site consists of public area and working area. The personnel load rate in the process is calculated by dividing the actual workload by the authorized workload. At the same time, the factors such as sorting time requirements, proportion of per-order batch, efficiency of the delivery personnel, and unit fuel consumption of the vehicle, will have a direct impact on the delivery frequency. Among them, the positive loop represents that there is a mutually reinforcing relationship between the factors, while the negative loop indicates that there is a balanced relationship between factors (Such as: Delivery frequency --> + Consumer demand response ability --> + Impact of demand response capacity --> + Consumer satisfaction --> + Order quantity due to change in satisfaction --> + Total daily order quantity --> + Per batch delivery order quantity-->+ Sorting center cargo volume-->+ Number of on-site logistics personnel--> -Onsite logistics personnel operating time-->+On-site logistics personnel costs-->+ Sorting personnel costs-->+ Sorting costs-->+ Total cost --> Total profit --> + Delivery facility input --> + Delivery frequency). This model passed the mechanical error checking and dimension consistency testing by VENSIM. The extreme condition when the order quantity equals to zero was examined as well. Since the current order quantity of each batch is mainly determined by the consumer's shopping habits, which cannot be arbitrarily changed, this paper splits the orders of the first batch (60% of the total orders). Second  Third  Fourth   1  60  30  40  20  2  30  30  20  20  3  10  30  30  20  4  -10  10  30  5  ---10  Table 4 . Setting reason for each scheme. The delivery frequency in scenarios 2 and 3 was increased by 9.5% over that in scenario 1. The minimum value of 3.9% appeared in the order volume of 3086-3154 units. In scenarios 2 and 3, when the order quantity was lower than 3,500, the differences between the total delivery costs in Operational Efficiency: A Case Study of Jingdong Logistics different scenarios were very small; when the order quantity was higher than 3,500, the increased rate of cost in scenario 3 was higher for a while than that in scenario 2. The total delivery cost in scenario 4 was increased by 25% in average than that in scenario 1. The four scenarios showed a lower average unit cost when the order quantity was around 4,700 units indicating that this order quantity was a batch of economic orders. In  Figures 10 and 11 , it can be seen that the sorting cost of scenario 2 increased by an average of 157 yuan per day compared to scenario 1, which was due to the increase of delivery Higher frequency led to an increase in the total working time of sorting equipment and equipment costs. The difference of average unit sorting costs among scenarios 1, 2 and 4 decreased as the order increased, which is because the change in the sorting costs under different order quantities was small and the difference in the delivery order quantity increased. The increased percentage behaved a negative growth trend followed by a positive growth trend as the order increased. In scenarios 2 and 4 the orders can be met by just increasing the frequency of existing vehicles, however, in scenario 1, new vehicles were needed to meet the transportation needs, which resulted in higher transportation costs. Figures 14 and 15  illustrate that the terminal delivery cost of scenario 2 was increased by an average of 5% compared to scenario 1, and the cost in scenario 4 was increased by an average of 15% over that in scenario 1. The main reason is that the increase in the delivery frequency improved the demand for the number of delivery personnel and caused the delivery time of delivery personnel to be shorter in a single batch. The vehicles utilizations of the three sorting centers in scenario 1 showed the first drop point when the order volume was 2530, 2567, and 2350, and the second drop point occurred at 5000, 5200, and 4800. This is because the order volume was so small in scenario 4 that in the unit batch delivery personnel was working inefficiently. The results of this study indicate that under the impact of order volume there is no fixed relationship between delivery frequency with delivery cost and resource operational efficiency. The change of delivery frequency has different effects on the resource operational efficiency in different stages of the delivery process. With the same delivery frequency, different order splitting ratios affect delivery cost and resource operational efficiency. For example, when the JDL order volume fluctuates between 3086-3154, one should adopt scenario 2 or scenario 3; when the order volume is greater than 5100, scenario 4 should be adopted. Recommendation II: one should consider constraints such as delivery resources and consumer satisfaction to achieve the appropriate decision of delivery frequency. In the same delivery link the demand for delivery resources can be different due to different delivery operational capacities. In this case, if scenario 2, 3, or 4 is adopted, one can increase the total number of order shipments by 2,400, 1,400, and 4,450 units respectively without increasing the number of transport vehicles. When the delivery frequency is increased from 3 times per day to 4 times per day, scenario 3 should be adopted from the perspective of increasing consumer satisfaction; scenario 2 should be adopted when the area of delivery stations and the delivery personnel are tight. The delivery frequency has different effects on the resource operational efficiency in different delivery stages.
paper_479	 Handwriting is also a key element in document examination as it leaves a forensic document examiner with the task of determining who the writer of a particular document is and this is achieved through the likelihood ratio (L R) paradigm. Inability to model an individual's handwriting over time has made estimating a full likelihood ratio for comparative handwriting analysis impossible thereby employing nuisance parameters and subjectivity in computation of L R that is not full. This research employed back propagation neural network (BPNN) to model the writing pattern of individuals with input layer as the features of handwriting characters, two hidden layers of three neurons each, activation function sigmoid (s) and an output handwriting. With the help of handwriting model for individual writers, little or no assumptions and no nuisance parameters were employed in achieving full likelihood ratio for comparative handwriting analysis in forensic science. It has been established that there are varieties of handwritten documents ranging from forgeries, counterfeiting, identity theft, fraud, suicidal note, contested wills. The likelihood ratio paradigm has been studied as a means for quantifying the strength of evidence for a variety of forensic evidence types in handwriting and other types of forensic evidence such as earmark, speech, footprint, fingerprint, glass fragments and DNA  [1] [2] [3] [4] [5] [6] [7] [8] . For the handwriting evidence, previous literatures reported many factors and the inability of the forensic examiner to model the writing profile of a writer because it is often believed that an individual's writing profile is a latent characteristic that cannot be observed directly, and is not easily modeled because an individual's writing profile cannot have static characteristics and may change over time  [9] [10] [11]  which has resulted to adopt ad-hoc methods to compute likelihood  [9, 12, 13]  when full likelihood ratio is not achievable. The purpose of this paper is to illustrate how neural network approach to comparative handwriting analysis can greatly affect the outcome of a forensic investigation and make full likelihood ratio achievable. formula_0 Formally, posterior belief in favour of H p (for example the defendant is guilty) with fair facts are used to make a final decision on the evidence. Likelihood ratio = Posterior ratio / prior ratio Likelihood ratio according to  [14]  are increasingly being adopted to convey expert evaluative opinions to courts. The task of verification, which is to determine whether two writing samples compared side-by-side, originates from the same person, was the principal problem addressed. The problem of estimating a L R for handwriting has proven to be a non trival task due to the inability to model the writing pattern of an individual and due to the absence of a large database of handwriting and other factors. [15] [16] [17] [18]  estimated a L R for handwriting using Bayesian approach but in the presence of nuisance parameter, and their works had no underlying principle and model in which this L R was estimated. [17]  based their approach on distribution of measurements from comparing items but their approach was similar to that of marginal L R when full L R was not an option due to the presence of nuisance parameter. In statistics, a nuisance parameter is defined as: ''A parameter of a model where there is no scientific intrigue except for whose qualities are normally required (yet when all is said in done are obscure) to make deductions about those parameters which are of such intrigue [25].'' [19]  described a statistical model for the writer verification task to determine if two documents were written by the same writer, their model followed the scenario proposed by  [17]  thus suffers from the same weakness  [20]  presented one conceivable way to deal with gauge a probability proportion in near handwriting analysis was delineated. Assessment of signature handwriting evidence through score-based likelihood ratio based on comparative measurement of relevant dynamic features was carried out by  [22, 9]  presented score-based approaches to calculating forensic probability ratios that have been established as becoming more progressively common in forensic literature. Nonetheless, this method creates uncertainty for the generation of denominator databases -in general, how is the best way to produce a list of scores among two items from different source. [23]  Asserted that recent analytical developments paired with modern statistical computational tools have led to the proliferation of adhoc techniques for quantifying the probative value of forensic evidence. Various things come together to form a BPNN, such as hundreds of single units, artificial neurons or processing elements (PE), connected to coefficients (weights), which represent a neural structure and are organized in layers. Weight from the constant 1 and two hidden unit j is #$",*"  formula_2 After randomly selecting the weights of the network, the backpropagation algorithm is used to measure the necessary corrections. Due to the complexity of modeling the handwriting of a writer and the absence of industrial size databases from which different handwriting can be described  [17, 9]  estimated a marginal L R when the full L R was not possible also in the presence of some parameters considered to be nuisance. An inconclusive state is declared if and only if L R value = 1 With the BPNN algorithm to model handwriting pattern for each writer and L R estimation described in  [10]  this paper was able to deal with the problem of inconclusiveness as there was no inconclusive results reported in the investigation, determine who the writer of a questioned document is as well as eliminate the inclusion of parameters considered nuisance in investigation because each writer's handwriting profile could be ascertained due to the BPNN training of the Handwriting categories. Statistically the estimation of the interval is considered to be robust over the estimation point, therefore we do consider the estimation of the interval and take into account the decision condition, and we conclude that both the down and top intervals must be of the same sign; that is, either both positive or both negative. Table 1  shows the results of our Estimated Likelihood Ratio (ELR) for a writer against any other author in a collection of appropriate databases where the upper confidence interval (CiU) and the lower confidence interval (CiL) are indicated. This research shows that modeling handwriting pattern for a writer is an important factor in achieving a full L R which will eliminate the use of nuisance parameters in the computation process, lower the disagreement rate with respect to the Hypothesis in support of the prosecutor and also produce no inconclusive result after estimation of L R .
paper_492	 Based on known functions of neuroscience the neural network that performs serial parallel conversion and its inverse transformation is presented. Transition of the activation portion in this type of operation is illustrated. Although status of nervous system of the animal change according to the recognition by sensory organ and to the manipulation of the object by muscle in the vicinity of the animal itself, the evolved animals have in addition another nervous system so-called long-term memory or episodic memory being involved experience and prediction. The nervous system of long-term memory behaves freely but keeping consistency of the change in the environment. In parallel with advances in technology, metals, chemicals, and even semiconductors have emerged as a new material for new products. It is human intelligence that gave birth to the technology. But the structure of our brain has not changed since tens of thousands of years ago. In other words, the nervous system of animals called advanced higher animals is locally same as very primitive animal's nervous system. In Chapter 2, the circuits by combining the basic functions of neural circuits for serial parallel conversion and inverse of the conversion are shown. The nervous system related to short-term memory is activated in synchronization with environmental events, but the nervous system involved in long-term memory is highly layered to form an image corresponding to past and future events. Acceptance and generation of time series data of the nervous system of long-term memory is carried out in consistent with short-term memory. The same applies to general figures. Many East Asian recognize kanji as a combination of parts. The recognition technology of the figure has evolved by the neural network which starts with the perceptron, but it is forced to judge by the relation of the part and the whole as in the example of kanji when the scale of the figure increases. In other words, a trade-off is made between the recognition power of the part in the complex figure recognition and the processing power of the data sequence of the recognition result, and by learning the animal might got the most efficient processing method. Deductive logical development is desired. Next, by providing a two-way function to the neural network both serial parallel conversion and vice versa on basic sequences is realized. The neural circuit corresponding to each basic subsequence can be easily configured in the neural network, leading to the realization of processing by hardware of general time series data. Example shown in  Figure 1  shows that the time series data (1) arranged by randomly selecting the elements is divided into five basic subsequences (2). The dividing is done by the following procedure. In given example, the leading element is a1, followed by a7, a4, a6 and a6. (3) If the maximum length of subsequence is defined, a new subsequence is allocated after the subsequence that reaches the maximum length, add new element to the concerned new subsequence. When the first data c 0 is received activate the bottom. For the next data c 1 additionally activates the elements which has been activated by the first data c 0 . Because the elements activated by c 1 is randomly connected to input, not all elements activated by c 0 are additionally activated. By receiving c 2 , c 3 and so on the activated portions become narrow and narrow. The output of the element holding the activity when receiving the last c 4 of the time series data is the recognition result of the time series data c 0 c 1 c 2 c 3 c 4 . The number and their position of the activated elements is the conversion output corresponding to the serial input  [1] . Its output is the result of the serial parallel conversion, it is the result of AND logic of the output of the activated elements. Therefore, the couplings between the elements are enhanced (Hebb rule) by repeating this conversion, as a result the elements involved in the conversion will be activated by only receiving the first element of the time series data. that can be said a conversion of parallel to serial triggered by the first data. On the flow direction of data, this parallel to serial conversion is upside down with the serial to parallel conversion described above, but the basis of neural network operation is the same. When the state transition diagram of the  Figure 3  is seen as a serial parallel conversion, the bottom is inputs connected to such sensory organs. On the other hand, when the state transition diagram of the  Figure 3  is seen as a parallel serial conversion, another waiting state is needed. The essence of the neural network which performs the parallel serial conversion and the neural network which performs the serial parallel conversion is the same except that the conversion result comes out downward or upward. On other words, both operations of the serial parallel conversion and the reverse conversion are essentially the same on the point both operations are triggered by the first data and proceed waiting for input state change. For each basic subsequence a neural network that accepts and generates the basic subsequence is considered. Since general time series data is consisted of hierarchy of basic subsequences, basic units can process general time series data by identifying the outputs of lower layer basic units as the new time series data. The movement will be mentioned in the next chapter. The ability to imitate fellow's action is indispensable. On your birthday day, you may want to see the cake in front of you, identify the cake from store A, which was the one that was the subject of a conversation with your family the day before, and worry about the difference from the previous day's expectations. One is time series data based on visual information of the cake in front of you and muscle movements that manipulate chocolate plates, which is produced by the nervous system that animals have from the early stages of evolution. While the behavior of the former nervous system is reflection of the visual information and movement of the objects nearby, the latter nervous system not only no needs to be synchronized with the former, but also moves independently. However, if the nervous system of the episodic memory is activated by remembering the A shop and the chocolate plate while looking at the cake, the difference between reality and the expectation might become a problem. Figure 5  is an illustration for showing the state change of each part in the neural network. The upper part shows the part related to episodic memory, and the lower part shows the part related to the short-term memory. The part shown in red is a part that is particularly activated, the lower red disk is the part activated by the visual data of the chocolate plate that placed in front, the upper red disk is the part that is activating by recalled episode about the chocolate plate. When studying objects that are intertwined with portion and whole system such as the nervous system, the idea of category theory can be incorporated to develop the whole without focusing on the details of the object. A category N consists of: [Def.1] a collection ob (N) of objects; In this paper, object corresponds to nerve cells in neuroscience and in neural networks corresponds to basic units (or combinations thereof) described in  Figure 6 . [  formula_0 The composite of the coupling is considered to be a hierarchical connection of the objects, and the identity, which is considered a special morphism, can correspond long axons extending beyond hierarchies. According to the Hebb law, if there are two nervous system activated, the binding between the elements in the two categories will be enhanced. It has been vaguely thought that the nervous system responsible for long-term memory may be in a different place from the nervous system responsible for short-term memory. Consistency is required when the objects in which both memories are involved are the same. In the study of memory and language, it is a meaningful theme that covers the homogeneity of behavior between parts of the nervous system  [6, 7] . From the viewpoint of neuroscience, even if there are no "parts" equivalent to the basic unit in the process of stimulation from the sensory organs propagating to the cerebral cortex, axons parallel to the propagation direction connect between the layers, and it can be said that it is a passage of serial parallel mutually converted information.
paper_507	 Integrated them together using GIS and soft computing to create a database that will generate the output for the future use for prediction of susceptibility of landslide. The main aim of present study is to integrate the result of our study with spatial data, soil parameters, land inventory and used the output as a user friendly application using GIS which could predict the future susceptibility of region to landslide and% contribution of each factor for the same. As per the official figures of United Nations International strategy of disaster reduction (UN/ISDR) and Centre for Research on Epidemiology of Disasters CRED for the year 2006, landslide ranked 3 rd in terms of number of death among the top ten natural disasters  [2] . As far as Indian scenario is concerned approximately 0.49 million km  2  or 15% of land area of country is vulnerable to landslide hazard and 80% is spread over Himalayas, Nilgiris, Ranchi plateau and eastern ghats  (GSI 2006)    [3] . Uttarakhand is an appropriate choice for the study since the newly developed state has been fighting with the catastrophe and is making front paper headlines for landslide at Vishnuprayag, Baldora, Lambagharchatti, Jharkula, phatabyung, and Amiya landslides  [4] . The recent landslide of 2012 took a heavy toll on life and property, many people lost their lives, and thousands of tourist were stranded due to disrupted communication services  [5] . The phenomenon can be easily classified and described by two nouns. The traditional practice of Landslide prevention is enabling people with Landslide Hazard Zonation Maps. al, 1999 and varnes 1984)    [7, 8, 9] . In the present study with the help of satellite imageries like DEM from cartosat-1 and topographic maps from GSI (geographical survey of India) a susceptibility map is prepared. 2. 3. Uttarkashi falls under the physiographic division (s) Rohilkhand plains, Nepal Himalayas, Ganga, Yamuna daob, Siwalik range, Kumaun Himalaya, Dhaoladhar range. Therefore, the relation between landslide occurrence and the conditioning parameters used is crucially important for landslide susceptibility mapping. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area (Mohammad Onagh, 2012). Thus a number of thematic maps (referred to as data layer in GIS) based on the specific parameters which are related to occurrence of landslide viz. DEM (Digital elevation model) was obtained from BHUVAN. The factors in relevance to the landslide susceptibility analysis of Uttarkashi are:             The data that has been acquired is raw and is to be converted into the software readable form to be used in the susceptibility analysis. The shape files created using the raw data are:            An artificial neural network is a "computational mechanism able to acquire, represent, and compute a mapping from one multivariate space of information to another, given a set of data representing that mapping". The purpose of an artificial neural network is to build a model of the data-generating process, so that the network can generalize and predict outputs from inputs that it has not previously seen  [11, 12] . The ANN is a black box model is a multi-layered neural network, which consists of an input layer, hidden layers, and an output layer. An artificial neural network "learns" by adjusting the weights between the neurons in response to the errors between the actual output values and the target output values. At the end of this training phase, the neural network provides a model that should be able to predict a target value from a given input value. The arrangement of the nodes is refer-red to as the network architecture (  figure 18 ). Figure 18 . Architecture of neural network (source:  (Lee, 2009) ). ANNs can be grouped into two major forward and feedback (recurrent) networks. In the former network, no loops are formed by the network connections, while one or more loops may exist in the back propagation neural network with feed forward approach. Since ANN does not understand the 'landslide prone' and 'non-landslide prone' region we explain it by giving value '1' and '0' respectively. As we have seen neural network can compute the output for a given input. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer (20), and between the hidden layer (20) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). In the training process we change the weights in that way in which the network output and the true values get closer and closer to each other. The results are compiled below. The first objective of present study was to study the factors causing landslide. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area  [13] . Thus a number of thematic maps (referred to as data layer in GIS) based on the specific parameters which are related to occurrence of landslide viz. Slope 2. Height 5. Land use 7. Precipitation The next objective of our study was to present the weightage of various factors causing landslide. An artificial neural network technique was used. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer  (15) , and between the hidden layer (15) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). The region that is prone to landslide has been depicted by 1 and the region that is not prone to landslide has been depictyed by 0. Other values for landslide susceptibility for the adjoining areas have been calculated using interpolation technique therefore the Rishikesh-Uttarkashi-Gangotri-Gaumukh route has been mapped for landslide Hazard. The result of present study with spatial data, soil parameters, land inventory and presented by a landslide hazard zonation map and a user friendly application using GIS that could predict the future susceptibility of region to landslide and percentage contribution of each factor for the same. The reliability of ANN is high over other methods.
paper_1	 Through these messages also called status updates, they share and discuss things like news, jokes, business issues and what they go through on a daily basis. This paper reports on the successful development of a way of searching, filtering, organizing and storing the information from social media so that it can be put to some good use in an electronic learning environment. A cluster is a collection of data objects that are either similar to one another in the same group or dissimilar to objects in other groups. Clustering uses unsupervised learning technique in finding similarities between data according to the characteristics found in the data and grouping similar data objects into clusters. Its objective is to get groups of objects such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groups  [1] . It can be applied in various fields for instance taxonomy of living things, information retrieval from a document, identification of areas of similar land use in an earth observation database, discovering distinct groups by marketers in their customer bases for development of targeted marketing programs and identifying groups of houses according to their house type, value, and geographical location  [2] . Clustering is the concept that was used in this research to create groups from social media data which can be used for learning on electronic learning platforms. Requirements and Architectural Design can be done up front and then each prototype developed as the project progresses. The solution is complete when all the components are in place. This generally dealt with preliminary processing of the data collected from the users to do away with any inconsistencies and outliers  [11] . The machine learning method used was unsupervised learning in which the system was given the data so that it automatically analyzes and creates clusters from the data  [3] . The illustration of the proposed prototype is given below. The results reported in this paper were obtained from a series of evaluations that were done on the classifier on different parameters including functionality, usability, accuracy, precision and recall. This is the section that captured the users view on the functioning of the prototype. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . This is summarized in the chart below. This matrix consists of the following parameters: TP, TN, FP and FN, which are defined below. This is where the tweets were being divided into single words which were analyzed before being classified. Precision and recall were however average. These measures can be improved if large amounts of data are used to train the classifier before being used to do actual classification. Through the study, it can be underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. The system that was developed by the researcher demonstrated the learning capability of the social media by coming up with a way of creating study groups from the information shared across the social media. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. This paper addresses the limitation of social media of not being properly utilized as a platform for supporting learning activities like group formation. Through the study, it was underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_1	 Through these messages also called status updates, they share and discuss things like news, jokes, business issues and what they go through on a daily basis. This paper reports on the successful development of a way of searching, filtering, organizing and storing the information from social media so that it can be put to some good use in an electronic learning environment. A cluster is a collection of data objects that are either similar to one another in the same group or dissimilar to objects in other groups. Clustering uses unsupervised learning technique in finding similarities between data according to the characteristics found in the data and grouping similar data objects into clusters. Its objective is to get groups of objects such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groups  [1] . It can be applied in various fields for instance taxonomy of living things, information retrieval from a document, identification of areas of similar land use in an earth observation database, discovering distinct groups by marketers in their customer bases for development of targeted marketing programs and identifying groups of houses according to their house type, value, and geographical location  [2] . Clustering is the concept that was used in this research to create groups from social media data which can be used for learning on electronic learning platforms. Requirements and Architectural Design can be done up front and then each prototype developed as the project progresses. The solution is complete when all the components are in place. This generally dealt with preliminary processing of the data collected from the users to do away with any inconsistencies and outliers  [11] . The machine learning method used was unsupervised learning in which the system was given the data so that it automatically analyzes and creates clusters from the data  [3] . The illustration of the proposed prototype is given below. The results reported in this paper were obtained from a series of evaluations that were done on the classifier on different parameters including functionality, usability, accuracy, precision and recall. This is the section that captured the users view on the functioning of the prototype. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . This is summarized in the chart below. This matrix consists of the following parameters: TP, TN, FP and FN, which are defined below. This is where the tweets were being divided into single words which were analyzed before being classified. Precision and recall were however average. These measures can be improved if large amounts of data are used to train the classifier before being used to do actual classification. Through the study, it can be underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. The system that was developed by the researcher demonstrated the learning capability of the social media by coming up with a way of creating study groups from the information shared across the social media. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. This paper addresses the limitation of social media of not being properly utilized as a platform for supporting learning activities like group formation. Through the study, it was underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_1	 Through Social media, people are able to write short messages on their walls to express their sentiments using various social media like Twitter and Facebook. Through these messages also called status updates, they share and discuss things like news, jokes, business issues and what they go through on a daily basis. This paper reports on the successful development of a way of searching, filtering, organizing and storing the information from social media so that it can be put to some good use in an electronic learning environment. Clustering uses unsupervised learning technique in finding similarities between data according to the characteristics found in the data and grouping similar data objects into clusters. Good clustering methods produce high quality clusters with either a high intra-class similarity within clusters or a low inter-class similarity between clusters. Clustering is the concept that was used in this research to create groups from social media data which can be used for learning on electronic learning platforms. In incremental prototyping, the whole requirements are broken down into building blocks which are incremented each time a new component is integrated based on an overall design solution. Requirements and Architectural Design can be done up front and then each prototype developed as the project progresses. The solution is complete when all the components are in place. The first task was to retrieve details of each of the students from their twitter accounts using an extension script which is part of the twitter Application Programming Interface. The third step involved using the data already preprocessed above to train the prototype. The machine learning method used was unsupervised learning in which the system was given the data so that it automatically analyzes and creates clusters from the data  [3] . They are part of the data that was used to train the system but its results are already known. The illustration of the proposed prototype is given below. The results reported in this paper were obtained from a series of evaluations that were done on the classifier on different parameters including functionality, usability, accuracy, precision and recall. This is the section that captured the users view on the functioning of the prototype. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . They also confirmed that the system simplified the process of group formation and made inclusivity of distant students in the groups possible. This is summarized in the chart below. This matrix consists of the following parameters: TP, TN, FP and FN, which are defined below. This is where the tweets were being divided into single words which were analyzed before being classified. Precision and recall were however average. These measures can be improved if large amounts of data are used to train the classifier before being used to do actual classification. Through the study, it can be underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. Through social media a better and more efficient way can be used to enable online learning generally and group formation specifically. The system that was developed by the researcher demonstrated the learning capability of the social media by coming up with a way of creating study groups from the information shared across the social media. It was able to extract tweets from various social media accounts based on a given hash tag (task) and then pass them to a Naïve Bayes classifier as input. The classifier then grouped the users into different categories based on various tweets that they posted on the task. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. This paper addresses the limitation of social media of not being properly utilized as a platform for supporting learning activities like group formation. Through the study, it was underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_1	 Through these messages also called status updates, they share and discuss things like news, jokes, business issues and what they go through on a daily basis. This paper reports on the successful development of a way of searching, filtering, organizing and storing the information from social media so that it can be put to some good use in an electronic learning environment. A cluster is a collection of data objects that are either similar to one another in the same group or dissimilar to objects in other groups. Clustering uses unsupervised learning technique in finding similarities between data according to the characteristics found in the data and grouping similar data objects into clusters. Its objective is to get groups of objects such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groups  [1] . It can be applied in various fields for instance taxonomy of living things, information retrieval from a document, identification of areas of similar land use in an earth observation database, discovering distinct groups by marketers in their customer bases for development of targeted marketing programs and identifying groups of houses according to their house type, value, and geographical location  [2] . Clustering is the concept that was used in this research to create groups from social media data which can be used for learning on electronic learning platforms. Requirements and Architectural Design can be done up front and then each prototype developed as the project progresses. The solution is complete when all the components are in place. This generally dealt with preliminary processing of the data collected from the users to do away with any inconsistencies and outliers  [11] . The machine learning method used was unsupervised learning in which the system was given the data so that it automatically analyzes and creates clusters from the data  [3] . The illustration of the proposed prototype is given below. The results reported in this paper were obtained from a series of evaluations that were done on the classifier on different parameters including functionality, usability, accuracy, precision and recall. This is the section that captured the users view on the functioning of the prototype. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . This is summarized in the chart below. This matrix consists of the following parameters: TP, TN, FP and FN, which are defined below. This is where the tweets were being divided into single words which were analyzed before being classified. Precision and recall were however average. These measures can be improved if large amounts of data are used to train the classifier before being used to do actual classification. Through the study, it can be underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. The system that was developed by the researcher demonstrated the learning capability of the social media by coming up with a way of creating study groups from the information shared across the social media. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. This paper addresses the limitation of social media of not being properly utilized as a platform for supporting learning activities like group formation. Through the study, it was underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_1	 Through these messages also called status updates, they share and discuss things like news, jokes, business issues and what they go through on a daily basis. This paper reports on the successful development of a way of searching, filtering, organizing and storing the information from social media so that it can be put to some good use in an electronic learning environment. A cluster is a collection of data objects that are either similar to one another in the same group or dissimilar to objects in other groups. Clustering uses unsupervised learning technique in finding similarities between data according to the characteristics found in the data and grouping similar data objects into clusters. Its objective is to get groups of objects such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groups  [1] . It can be applied in various fields for instance taxonomy of living things, information retrieval from a document, identification of areas of similar land use in an earth observation database, discovering distinct groups by marketers in their customer bases for development of targeted marketing programs and identifying groups of houses according to their house type, value, and geographical location  [2] . Clustering is the concept that was used in this research to create groups from social media data which can be used for learning on electronic learning platforms. Requirements and Architectural Design can be done up front and then each prototype developed as the project progresses. The solution is complete when all the components are in place. This generally dealt with preliminary processing of the data collected from the users to do away with any inconsistencies and outliers  [11] . The machine learning method used was unsupervised learning in which the system was given the data so that it automatically analyzes and creates clusters from the data  [3] . The illustration of the proposed prototype is given below. The results reported in this paper were obtained from a series of evaluations that were done on the classifier on different parameters including functionality, usability, accuracy, precision and recall. This is the section that captured the users view on the functioning of the prototype. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . This is summarized in the chart below. This matrix consists of the following parameters: TP, TN, FP and FN, which are defined below. This is where the tweets were being divided into single words which were analyzed before being classified. Precision and recall were however average. These measures can be improved if large amounts of data are used to train the classifier before being used to do actual classification. Through the study, it can be underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. The system that was developed by the researcher demonstrated the learning capability of the social media by coming up with a way of creating study groups from the information shared across the social media. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. This paper addresses the limitation of social media of not being properly utilized as a platform for supporting learning activities like group formation. Through the study, it was underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_1	 Through these messages also called status updates, they share and discuss things like news, jokes, business issues and what they go through on a daily basis. This paper reports on the successful development of a way of searching, filtering, organizing and storing the information from social media so that it can be put to some good use in an electronic learning environment. A cluster is a collection of data objects that are either similar to one another in the same group or dissimilar to objects in other groups. Clustering uses unsupervised learning technique in finding similarities between data according to the characteristics found in the data and grouping similar data objects into clusters. Its objective is to get groups of objects such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groups  [1] . It can be applied in various fields for instance taxonomy of living things, information retrieval from a document, identification of areas of similar land use in an earth observation database, discovering distinct groups by marketers in their customer bases for development of targeted marketing programs and identifying groups of houses according to their house type, value, and geographical location  [2] . Clustering is the concept that was used in this research to create groups from social media data which can be used for learning on electronic learning platforms. Requirements and Architectural Design can be done up front and then each prototype developed as the project progresses. The solution is complete when all the components are in place. This generally dealt with preliminary processing of the data collected from the users to do away with any inconsistencies and outliers  [11] . The machine learning method used was unsupervised learning in which the system was given the data so that it automatically analyzes and creates clusters from the data  [3] . The illustration of the proposed prototype is given below. The results reported in this paper were obtained from a series of evaluations that were done on the classifier on different parameters including functionality, usability, accuracy, precision and recall. This is the section that captured the users view on the functioning of the prototype. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . This is summarized in the chart below. This matrix consists of the following parameters: TP, TN, FP and FN, which are defined below. This is where the tweets were being divided into single words which were analyzed before being classified. Precision and recall were however average. These measures can be improved if large amounts of data are used to train the classifier before being used to do actual classification. Through the study, it can be underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. The system that was developed by the researcher demonstrated the learning capability of the social media by coming up with a way of creating study groups from the information shared across the social media. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. This paper addresses the limitation of social media of not being properly utilized as a platform for supporting learning activities like group formation. Through the study, it was underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_1	 Through Social media, people are able to write short messages on their walls to express their sentiments using various social media like Twitter and Facebook. These numerous amounts of information can be extracted, processed and properly utilized in areas like marketing and electronic learning. This paper reports on the successful development of a way of searching, filtering, organizing and storing the information from social media so that it can be put to some good use in an electronic learning environment. This helps in solving the problem of losing vital information that is generated from the social media. It addresses this limitation by using the data from twitter to cluster students and by so doing support group electronic learning. Good clustering methods produce high quality clusters with either a high intra-class similarity within clusters or a low inter-class similarity between clusters. In incremental prototyping, the whole requirements are broken down into building blocks which are incremented each time a new component is integrated based on an overall design solution. The solution is complete when all the components are in place. The second task involved identifying the right kind of data to use for training the expected prototype as well as testing it. The third step involved using the data already preprocessed above to train the prototype. From this, we identified the groups that students' fall that were turned into discussion groups. The prototype was then subjected to testing using the test data. They are part of the data that was used to train the system but its results are already known. The first task was to determine if the prototype achieved its overall goal which is grouping students through social media for electronic learning. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . This is summarized in the chart below. A total of 200 tweets were extracted and used for this test which was summarized in a confusion matrix. This matrix consists of the following parameters: TP, TN, FP and FN, which are defined below. The results below illustrate a summary of what was obtained when 200 tweets were used to test the Naïve Bayes Classifier. Precision and recall were however average. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. Through social media a better and more efficient way can be used to enable online learning generally and group formation specifically. It was able to extract tweets from various social media accounts based on a given hash tag (task) and then pass them to a Naïve Bayes classifier as input. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. This paper addresses the limitation of social media of not being properly utilized as a platform for supporting learning activities like group formation. Most of the information that passes through social media was being used majorly for social interaction. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_1	 Through these messages also called status updates, they share and discuss things like news, jokes, business issues and what they go through on a daily basis. They enable interaction among vast groups of people including students, businesses and their clients. Some of them include summarization, compression and k-nearest neighbor which localizes search to one or a small number of clusters. Good clustering methods produce high quality clusters with either a high intra-class similarity within clusters or a low inter-class similarity between clusters. The quality of clustering also depends on both the similarity measure used by the method and its implementation  [10] . This section looks at how the system for creating discussion groups was developed as well as a detailed explanation of the research method that was used to realize the objective of the study. In incremental prototyping, the whole requirements are broken down into building blocks which are incremented each time a new component is integrated based on an overall design solution. The solution is complete when all the components are in place. The second task involved identifying the right kind of data to use for training the expected prototype as well as testing it. This generally dealt with preliminary processing of the data collected from the users to do away with any inconsistencies and outliers  [11] . The third step involved using the data already preprocessed above to train the prototype. From this, we identified the groups that students' fall that were turned into discussion groups. They were used to confirm that the system indeed accurately did the classification given some data items. Finally, the model was used to classify a new user into a group. The first task was to determine if the prototype achieved its overall goal which is grouping students through social media for electronic learning. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . They also confirmed that the system simplified the process of group formation and made inclusivity of distant students in the groups possible. This is summarized in the chart below. The Naïve Bayes Classifier was also tested to evaluate its accuracy, precision and recall  [9] . In experimenting with the Naïve Bayes Classifier, we relied on the NLTK module which provides functions for calculating these measures for the classifier. This matrix consists of the following parameters: TP, TN, FP and FN, which are defined below. Precision and recall were however average. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. It was able to extract tweets from various social media accounts based on a given hash tag (task) and then pass them to a Naïve Bayes classifier as input. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. Most of the information that passes through social media was being used majorly for social interaction. The techniques that are currently used in group formation and learning are mostly manual and so not efficient.
paper_1	 Through Social media, people are able to write short messages on their walls to express their sentiments using various social media like Twitter and Facebook. These numerous amounts of information can be extracted, processed and properly utilized in areas like marketing and electronic learning. This paper reports on the successful development of a way of searching, filtering, organizing and storing the information from social media so that it can be put to some good use in an electronic learning environment. This helps in solving the problem of losing vital information that is generated from the social media. It addresses this limitation by using the data from twitter to cluster students and by so doing support group electronic learning. Good clustering methods produce high quality clusters with either a high intra-class similarity within clusters or a low inter-class similarity between clusters. In incremental prototyping, the whole requirements are broken down into building blocks which are incremented each time a new component is integrated based on an overall design solution. The solution is complete when all the components are in place. The second task involved identifying the right kind of data to use for training the expected prototype as well as testing it. The third step involved using the data already preprocessed above to train the prototype. From this, we identified the groups that students' fall that were turned into discussion groups. The prototype was then subjected to testing using the test data. They are part of the data that was used to train the system but its results are already known. The first task was to determine if the prototype achieved its overall goal which is grouping students through social media for electronic learning. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . This is summarized in the chart below. A total of 200 tweets were extracted and used for this test which was summarized in a confusion matrix. This matrix consists of the following parameters: TP, TN, FP and FN, which are defined below. The results below illustrate a summary of what was obtained when 200 tweets were used to test the Naïve Bayes Classifier. Precision and recall were however average. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. Through social media a better and more efficient way can be used to enable online learning generally and group formation specifically. It was able to extract tweets from various social media accounts based on a given hash tag (task) and then pass them to a Naïve Bayes classifier as input. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. This paper addresses the limitation of social media of not being properly utilized as a platform for supporting learning activities like group formation. Most of the information that passes through social media was being used majorly for social interaction. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_1	 Tweets and other updates have become so important in the world of information and communication because they have a great potential of passing information very fast. They enable interaction among vast groups of people including students, businesses and their clients. This helps in solving the problem of losing vital information that is generated from the social media. Clustering is a descriptive task of data mining. In incremental prototyping, the whole requirements are broken down into building blocks which are incremented each time a new component is integrated based on an overall design solution. The solution is complete when all the components are in place. Several activities were performed to come up with the system. This generally dealt with preliminary processing of the data collected from the users to do away with any inconsistencies and outliers  [11] . These unwanted features are not very good because they can easily cause the system to perform irregularly. The third step involved using the data already preprocessed above to train the prototype. The fourth step was testing the system. The prototype was then subjected to testing using the test data. This involved picking the details of a new student from twitter and trying to predict the class hence group that he should join. The illustration of the proposed prototype is given below. The results reported in this paper were obtained from a series of evaluations that were done on the classifier on different parameters including functionality, usability, accuracy, precision and recall. This is summarized in the chart below. The Naïve Bayes Classifier was also tested to evaluate its accuracy, precision and recall  [9] . In experimenting with the Naïve Bayes Classifier, we relied on the NLTK module which provides functions for calculating these measures for the classifier. From this analysis the classifier performed above average with an accuracy of 71.4%. Precision and recall were however average. They therefore come with a lot of challenges including time wastage. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. They therefore come with a lot of challenges including time wastage.
paper_1	 Through these messages also called status updates, they share and discuss things like news, jokes, business issues and what they go through on a daily basis. Tweets and other updates have become so important in the world of information and communication because they have a great potential of passing information very fast. These numerous amounts of information can be extracted, processed and properly utilized in areas like marketing and electronic learning. This helps in solving the problem of losing vital information that is generated from the social media. Clustering is a descriptive task of data mining. It can be applied in various fields for instance taxonomy of living things, information retrieval from a document, identification of areas of similar land use in an earth observation database, discovering distinct groups by marketers in their customer bases for development of targeted marketing programs and identifying groups of houses according to their house type, value, and geographical location  [2] . Good clustering methods produce high quality clusters with either a high intra-class similarity within clusters or a low inter-class similarity between clusters. The system design methodology used was incremental prototyping. In incremental prototyping, the whole requirements are broken down into building blocks which are incremented each time a new component is integrated based on an overall design solution. The solution is complete when all the components are in place. Several activities were performed to come up with the system. This generally dealt with preliminary processing of the data collected from the users to do away with any inconsistencies and outliers  [11] . The third step involved using the data already preprocessed above to train the prototype. The fourth step was testing the system. The prototype was then subjected to testing using the test data. This involved picking the details of a new student from twitter and trying to predict the class hence group that he should join. The illustration of the proposed prototype is given below. This is the section that captured the users view on the functioning of the prototype. This is summarized in the chart below. The Naïve Bayes Classifier was also tested to evaluate its accuracy, precision and recall  [9] . A total of 200 tweets were extracted and used for this test which was summarized in a confusion matrix. This matrix consists of the following parameters: TP, TN, FP and FN, which are defined below. The results below illustrate a summary of what was obtained when 200 tweets were used to test the Naïve Bayes Classifier. This classifier was doing the classification using the unigrams. This is where the tweets were being divided into single words which were analyzed before being classified. From this analysis the classifier performed above average with an accuracy of 71.4%. Precision and recall were however average. They therefore come with a lot of challenges including time wastage. They therefore come with a lot of challenges including time wastage.
paper_2	 In addition to this, the current traffic light systems encourage extortion by corrupt traffic officials as commuters often violate traffic rules because of the insufficient time allocated to their lanes or may want to avoid a long waiting period for their lanes to come up. The suggested case study, Jakpa junction is a typical example of a traffic congested area. The junction is a link to three roads which include; Effurun-Sapele road, Jakpa road, and PTI road. As a result of this a lot of time is wasted in the process. A traffic control model using signaling system in a discrete cross-road with NE-555 timer circuit was implemented by  [3] . Fuzzy Logic offers the possibility to 'compute with words', by using a mechanism for representing linguistic constructs common on real world problems. Real world complex problems such as human controlled systems involve a certain degree of uncertainty, which cannot be handled by traditional binary set theory. However, this system has the disadvantage of the controller since it depends on the preset quantification values for fuzzy variables  [6]  conducted a cross sectional study targeting traffic control in the city of Nairobi's Central Business District and its surroundings. Data was analyzed and presented using descriptive statistics; tables and graphs by using excel 2003. The simulation runs results showed that the adaptive algorithms can strongly reduce average waiting times of cars compared to the conventional traffic controllers. The complete circuitry is operated with TTL logic level of 0-5V. It comprises of a 0V to 12V transformer which is required to step down the 220V AC supply to 12V AC. It is further filtered through a 1000µF capacitor and then regulated using 7805 regulator to get +5V. Another purpose of an optocoupler is to prevent rapidly changing voltages or high voltages on one side of a circuit from distorting transmissions or damaging components on the other side of the circuit. The Transformer steps down the 220 v AC supply to 12 v AC. IR sensors are placed on the intersections on the road at fixed distances from the signal placed in the junction. The IR sensors are used to sense the number of vehicles on the road. The Vero board is also called a strip board. It is a widely used type of electronic prototyping board characterized by a 0.1-inch rectangular grid holes with parallel strips of copper cladding running in one direction all the way across one side of the board. The components were placed on the plain side of the board, with their leads protruding through the holes. The leads were then soldered to the copper tracks on the other side of the board to make the desired connections. The codes are as shown in the Appendix. On these days traffic rules are usually violated because of the complex traffic situation. This new design would further reduce time delay, improve efficiency and reduce accidents by incorporating the following modifications: (1) The Airport -PTI road should have a two -way traffic light to pass the vehicles coming from Airport road to PTI road. There should also be a two -way traffic light between Effurun -Sapale road and Jakpa road to pass vehicles coming from Effurun -Sapale road to Jakpa road.
paper_3	 The aim of this article is to present the general architecture trends of Web-based Adaptive Educational Hypermedia Systems (AEHSs) and to give a complete description of architecture of the AEHS MATHEMA. In the beginning, a related work on the architecture trends of Web-based AEHSs is presented. They build a personalized model of the learner and apply this model to adapt the content and/or the appearance of the hypermedia, according to the learner's specific characteristics, such as the cognitive goal, level of knowledge, pre-existing or prior knowledge, interests, preferences, stereotypes, cognitive preferences and cognitive or learning style  [4] . The aim of this article is to highlight the main contributions of the architecture of the MATHEMA in the improvement of the functionality of AEHSs. The key idea is the decentralization of their functions. The rest of the paper is organized as follows: In the section 2 related works for various models of architecture of AEHSs is presented. In the section 3 the architecture of the AEHS MATHEMA is presented in detail. For the AEHSs various models of architecture have been developed so far on which the designers of these systems are based on. The AHAM architecture model proposed by  [6]  is shown in the  Figure 1 . The AEHSs fit so nicely in this model. The rules together form the adaptation model in AHAM. A key point in AHAM is that the adaptation rules are used to translate user actions into user model updates (as well as the presentation and adaptation of content). Also, the rules use the user model information together with the action information in order to determine the required user model updates. In order to perform adaptation based on the domain model and user model is needed to specify how the user's knowledge influences the way in which the information from the domain model is to be presented. The ADAPT project has identified high level design dimensions for AEHSs, which are: (1) context of use (CU), (2) content domain (DM), (3) instructional strategy (IS), (4) instructional view (IV), (5) learner model (LM), (6) adaptation model (AM), and (7) detection mechanism (DE). [8] presented a similar architecture in the Proper system based on the AHAM model. Proper's architecture is a combined architecture of SCORM LMS and AEHS. Thus its architecture is a typical of a SCORM compliant LMS. Moreover domain independent data of UM is stored into the database while at the same time, data about user knowledge is stored into Java Object Files. All the runtime data about user actions and performance is stored into Java Object Files via JSP and Java servlets. In the  Figure 3  the architecture of the WELSA  [14]  system is presented. The system then enables to present and recommends a variety of learning contents adaptively towards each of the student's learning style identified in the student model through the adaptation model. The result of that analysis is called domain model. AEHS MATHEMA architecture is based on Web technologies similar to client/server distributed computing architecture of MIT and it is mainly supported by Apache Tomcat Server 5.5 and MySQL 6.0 relational database server. For supporting features, it uses the (JSP) pages, Java servlets, JavaBeans, JavaScripts, and Apache Struts technologies. The Apache Struts is a formalized framework for the architecture style design pattern Model View Controller (MVC). Figure 5  shows the architecture of the AEHS MATHEMA. The didactic model uses the information contained in the learner model to provide the two basic services, namely, adaptive presentation and adaptive navigation. The model that supports the AEHS MATHEMA is the overlay model. The characteristics that the system maintains in learner model for each learner are: name, gender, username, password, learning style, level of knowledge for each cognitive goal (general performance), level of knowledge for each basic meaning of the cognitive goal (quantitative and qualitative characterization), concepts that have been successfully evaluated for each cognitive goal, current page, current teaching method, current navigation technique, navigation history, meta-adaptive navigation status, initial Web experience and knowledge level (pre-existing knowledge) in the current cognitive goal, ways of presenting feedback messages, preference for assistance by Advisor in terms of its navigation, course sequence, and phases of the guided dialogues in interactive problem solving process in which each learner participates. The Student Model is transparent to the learner (open learner model) and is verifiable by the learner, that is, the learner can access his/her model and change the content of some characteristics. Figure 6  shows a snapshot of the page showing the characteristics of the learner model where the learner can be informed about them but also modify them. This is an innovation on the architecture of AEHSs. Figure 7  shows a snapshot of the meta-adaptation result. Figure 7 . A snapshot of a meta-adaptation result. Figure 9  shows a snapshot of the page responsible for selecting preferences of the learner. Following a study  [13]  conducted on the formation adaptive groups in the MATHEMA, it was decided that: when the system creates a priority list of candidate peers for an interested learner, in the first and second position of the priority list, the algorithm will place the candidate collaborators with a concrete or abstract style and with the same active or reflective dimension of their learning style as follows: If the learner has a concrete learning style, then the algorithm will place the candidate collaborators with a concrete learning style in the first position, and in the second position, the candidate collaborators with an abstract learning style. If the learner has an abstract learning style, then the algorithm will place the candidate collaborators with an abstract learning style in the first position, and in the second position, the candidate collaborators with a concrete learning style. After creating the list, the system informs the learner that his or her most important candidate collaborator is at the top of the list, while the less important is at the end of the list. Figure 10  shows a snapshot of adaptive group formation in the MATHEMA. Regarding adaptive and intelligent techniques of the AEHS MATHEMA mentioned above, the students having evaluated them consider that the adaptive presentation of different teaching strategies is quite useful (93.0 percent) and fairly easy to use (86.0 percent), the different ways of navigating are quite useful (100.0 percent) and handy (90.7 percent), the meta-adaptive navigation is quite useful (88.4 percent) and fairly easy to use (86.0 percent), the adaptive group formation of collaborative teams to assist them in collaborative problem solving is useful (83.7 percent), but less user-friendly (58.1 percent). This article presents the trends of architecture of AEHSs so far and then an extensive description of the architecture of the AEHS MATHEMA. Most of AEHSs presented in this paper are based on JSP, Java servlets, and Javabeans technologies to implement their functions, as implemented by AEHS MATHEMA. The architecture of AEHSs becomes more complex as more and more functions are implemented. MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and metaadaptive navigation, interactive problem solving, and adaptive group formation. In order to support all these functions the architecture of the MATHEMA is more complicated from other AEHSs. The key idea is the decentralization of their functions.
paper_21	 The use of this efficient local search had yield to determine the error correcting capability of many BCH codes of length 1023 and 4095. In telecommunication and storage systems, the fundamental problem is the reproduction at one point exactly or approximately the selected data at another point. An efficient solution of this problem is the use of error correcting codes. The error correcting codes improve the reliability of such communication, notably on channels that are subject to noise, by adding redundancy in data. The remainder of this paper is organized as follows: The next section presents the main related works. The section 3 presents the proposed scheme ZSISMP. The section 4 presents the main results. The conclusion and possible future directions of this research are outlined in section 5. This section summarizes the most important ones. In  [9] , Augot, Charpin, and Sendrier presented an algebraic system constructed from Newton's identities. [15] . The artificial intelligence Simulated Annealing presented in  [16] , Tabu Search  [17] , Hill-Climbing  [18] , Genetic Algorithm  [18] [19] , Ant Colony Optimization  [20] , Metropolis Algorithm  [21] , was shown to be useful to attack the difficulty of the minimum distance search problem for BCH Codes. In  [22] , Aylaj and Belkasmi improve the classical Simulated Annealing presented in  [16] . This approach consists in adding to all-zero codeword a level of noise and considering the minimum distance as the smallest level of noise from which the Soft-In decoder fails in correction. The table 2 summarizes the obtained results. These results demonstrate that the proposed scheme outperform greatly the famous Zimmermann algorithm. It shows that the proposed scheme greatly passes the MIM-RSC method. This table shows the height capacity of the proposed technique to find a minimum weight codeword.
paper_31	 According to China National Health and Nutrition Big Data Report 2018, 20% of Chinese suffer from chronic diseases, and chronic disease mortality was 86%. In-depth interviews with 91 employees in different positions from 11 grassroots medical and health institutions since July 2019 show that the efficiency of mobile medical care in the surveyed areas has appeared  [2] . Since the end of last century, some Chinese hospitals and universities have carried out some researches in this field. In the 1950s, American scholar Wittson had used two-way TV systems in medical field. At the end of the last century, the rapid development of semiconductor technology drove the revolution of information technology. This is the second stage of the development of WITMED. This is the third stage of the development of WITMED. Zigbee has been implemented on the Health Care Profile. There are some defects of the current WITMED (Wise Information Technology of med) system as follows: (1) The inadequate reliability of data transmission. (2) The problem of power consumption. Chronic diseases patients have to be long-term monitored, however, whether using a WIFI or GPRS, high power consumption will be arouse. (3) The problems of unitary monitoring data. (4) The problem of data processing. The current WITMED system focuses only on data acquisition instead of processing uploaded data and returning the result. The system architecture diagram is as follow:  Fast and correct data acquisition is the basis of the platform's efficient operation. The related knowledge base in  Figure 3  can be regarded as an expert system. The general technique route of the algorithm is as follows: Propose a comprehensive sorting algorithm, on the basis of semantic matching and a first-come-first-served principle. First, match the Word segmentation information by queued delivery with the taglib of the analysis system. If there are X words appearing in the taglib and N-X words not appearing in it, the matching degree of the information is (N-X) / N * 100%. The general algorithm of this part is as follows: Define priority function: f x, y ax by ε, with a basic definition as follows: x: semantic matching degree, 0≤x≤1 y: a request arriving at the serial position, 0<y≤1 a is defined as the weight of semantic matching degree, 0<a<1 b is defined as the weight of serial position of Request arrives, 0<b<1, a+b=1 ε is defined as the disturbance value, 0≤ε≤0.1, with the default as 0. With the definitions above, f x, y can turn to be: f x, y ax 1 a y ε , namely f x, y y a x y ε . Generally, a is close to µ= 1 2 , formula_0 Therefore, the process of queuing algorithm based on Semantic matching is as follows: With the hypothesis that a= 1 2 (to take one half of the sum) x= 5 10 (there are 10 participles in semantic information, of which 5 are matched), y 1 (Before a new message arrives, assumes that the request queue has 10 participles waiting, then: f x, y y a x y ε =0.294, therefore, the priority value of the message is 0.294. This research proposes an improved space vector model algorithm to ensure whether the patient has a related disease by the similarity between the physical signs and the disease on the basis of calculation of the sign value, continuous sign value and the disease characteristic value. An improved SVM calculation method is as follows: Definition 1 (conception): Define a tuple F= (X, Y, Z), in which x is the collection of all objects is the object property set, Z (X*Y), Z is a subset of X*Y. Definition 2: let {X1, Y1}, {X2, Y2} is concept form of {X, Y}, {X2, Y2b {X1, Y1}. Definition 3: The concept similarity is calculated as:  formula_1 Patients' health information involves their privacy. AES encryption process is shown in  Figure 4 : (3) column Mixed Operation The column mixed transformation is realized by matrix multiplication. The state matrix after row shift is multiplied by the fixed matrix to obtain the confused state matrix. The column mixed operation formula is shown as  Figure 6 : (4) key plus calculation Key plus calculation operates the 128 bit key is XOR bit by bit with the data in the state matrix. The process of key plus calculation can be regarded as the result of Bitwise XOR or byte level or bit level operation. The calculation is shown as  Figure 7 :   After logging in the system, enter the sign input interface and put on the sensor, the system will automatically input the relevant data collected, as shown in  Figure 8 : After inputting the data of relevant signs, the system will automatically generate diagnosis & treatment files and health assessment according to the relevant data. If the signs are abnormal, a warning will pop up, shown as  Figure 9 : The system can also display personal health data in the form of reports and trend charts, as shown in  Figure 10 :  Simulation and test show that the method adopted in this paper is correct. Data acquisition in  Figure 8  forms the health records in  Figure 9 . Through the big data platform, the system intuitively displays the backstage analysis data in the form of reports and trend charts as shown in  Figure 9 . The intelligent system realizes data acquisition, data encryption and processing, and big data analysis. The system can gather and process big health data of chronic patients, and realizes the discovery, tracking and treatment of chronic diseases.
paper_38	 The study adopted the descriptive research design. Pearson Product Moment Correlation (PPMC) Coefficient and Multinomial Logistics Regression (MLR) were the statistics used to answer the four research questions used. It concluded that the use of OL and UTME as instruments is not enough to select candidates for admission and therefore recommended that other instruments such as senior secondary school mock examinations need to be included as part of the entry requirements in the admission criteria. Education is an essential issue regarding the development of any country in the world. It is a progressive development of knowledge and skills of examinees through stages of teaching and learning at various levels  [1] . The number of undergraduate population in Nigerian Universities has increased from 103 in 1948 to an estimated population of 600,000 in 2018  [4] . There have been various perspectives presented by researchers and psychologists about what is academic performance and its importance. Formula 1 is used for calculating the CGPA. formula_0 Most universities in Nigeria have been using 5.0 as their perfect CGPA score. The rest of this paper is organised as follows: section 2 is the review of related literature, section 3 presents the methodology, section 4 discusses the results obtained, and the last section presents the conclusion and recommendation. The objectives of this study are to: i. The following research questions directed the study: 1. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? What is the relationship between OL results, UTME scores and Post-UTME scores of students and their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? 4. How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? Performance as defined by  [7] , is an observable or measurable behaviour of a person or an animal in a particular or experimental situation in which the authors further explained that performance measures the behaviours within a specific period. The underlying assumption made in such selection is that those admitted by satisfying the admission criteria will be successful in the successive academic activities attached to their studies. Wide disparities have cited between UTME and PUTME scores and the progress/performance of students especially those with exceptionally high UTME scores. The authors tested their nine hypotheses using an independent samples t-test and two-way analysis of variance. The article recommended that JAMB should embark on a more realistic review of the content of the UTME to enhance its predictive validity. The author further recommended the need for the PUTME exercise to be strengthened to have a fruitful admission of candidates. One major shortcoming of virtually all the studies reviewed is their scope in time and spread/coverage. This section discussed in detail the methodology employed such as research design, sample of study, instruments used, the procedure for the collection of data and data analysis. The correlational design is also sub-divided into explanatory and predictive research designs. The Faculty of Science consists of nine undergraduate B. Sc. Full-time degree programmes: Biochemistry, Biological Sciences, Chemistry, Computer Science, Geography, Industrial Chemistry, Microbiology, Mathematical Sciences, and Physics. The sample distribution is as shown in  Table 1 . The UTME was wholly multiple-choice objective questions conducted via Computer-Based Tests (CBTs) by JAMB. The semester examinations were mostly essay type questions. The stanine grades in the OL results obtained at either NECO or WAEC were collected and coded as shown in  Table  2 . The total score for five relevant subjects in OL is then computed and coded together with the UTME and PUTME scores which are as shown in  Table 3 . The coding for the CGPA is also shown in  Table 4 . SPSS is an acronym for Statistical Package for Social Sciences, but now it can also be referred to as Statistical Product and Service Solutions. It was used in this research study. Since the focus of the study is to determine the predictive validity of OL results and CGPA scores (OL-CGPA), UTME and CGPA scores (UTME-CGPA), and PUTME and CGPA scores (PUTME-CGPA), the statistics employed on the extracted data were Multinominal Logistic Regression (MLR) and Pearson Product Moment Correlation (PPMC) coefficient. There are three types of linear relationship that may exist between these two variables namely positive linear correlation, negative linear correlation and no correlation. The results of the data analysis are presented in tables according to the research questions that guided the study in this section. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? For the Computer Science programme, as shown in  Table 5 , the correlation coefficient indicated a low negative correlation in UTME-CGPA (-0.092) and PUTME-CGPA (-0.001) and a low positive correlation for OL-CGPA (0.072). Similarly, there exists a low negative correlation in UTME-CGPA (-0.082) and PUTME-CGPA (-0.038) and a low positive relationship in OL-CGPA (0.089) for the Mathematics programme. In the Physics programme, there exists a low positive relationship in OL-CGPA (0.016), PUTME-CGPA (0.028) and a low negative relationship in UTME-CGPA (-0.031). How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? Likelihood Ratio Tests is a statistical test of the goodness-offit between two models. The traditional 0.05 criterion of statistical significance was employed for all tests in  Table 7 . PPMC was used to analyse the data for this research question. The summary of  Table 8  shows the correlations of OL and CGPA scores, UTME and CGPA scores and PUTME and CGPA scores for all the programmes used for the analysis. In the 2010/2011 academic session, the correlation coefficient indicated a low positive correlation in OL-CGPA (0.198), UTME-CGPA (0.189), and PUTME-CGPA (0.232). In 2011/2012, OL-CGPA (-0.114) and PUTME-CGPA (-0.043) display low negative correlation while UTME-CGPA (0.114) has low positive correlation. In 2012/2013, UTME-CGPA (-0.363) and PUTME-CGPA (-0.123) have low negative relationship whereas OL-CGPA (0.111) has a low positive relationship. In 2013/2014, there exist a low positive association for OL-CGPA (0.061), UTME-CGPA (0.056), and PUTME-CGPA (0.038). The OL-CGPA (-0.071), UTME-CGPA (-0.090), and PUTME-CGPA (-0.040) in the 2014/2015 session signifies a low negative correlation. How well do OL results, UTME scores and Post-UTME Sa'adatu Abdulkadir and Francisca Nonyelum Ogwueleka: Predicting Students' First-Year Academic Performance Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria scores of students predict their first-year CGPA across the academic session, 2010/2011 to 2014/2015? Table 10 , the traditional 0.05 criterion of statistical significance was also used. The authors in  [21] , in their study also recommended the need of potential researchers to compare the OL, UTME and Post UTME terms and scores across Nigerian Universities for standardisation and a model for educational development in the twenty first century.
paper_57	 Stagnation is undesirable state which occurs at a later phases of the search process. In this case some heuristic information which guide search process is useful. One of the well-known graph search algorithm that utilizes a heuristic is A* search  [1]  or ACO algorithm. During the search process each ant sets off from ant colony (start position) and moves to search food (destination). how long the acquired information will be available. At the beginning when no pheromone values are available heuristic values η ij takes dominance. The probability p ij (t) of choosing the particular arc (i, j) depends on pheromone τ ij  (t)  and the heuristic η ij values which are associated with the arc (4). Heuristic values η ij affect probability only at the beginning when pheromone values are low. This occurs in later stages of the search process, where pheromone values tend to be high, and thus the chance of further exploration is low. Genetic algorithms (GA) were proposed by  Holland (1975) . The four main components of GA are representation (i), mutation (ii), crossover (iii) and selection (vi) mechanism. For feasibility reason the replacement node n r (new gene) is such a node from the node n i neighborhood N i , to which an arc from n i predecessor n p to n i successor n s exists  (Fig. 2) . 3 ). Prior the genetic operations all loops are removed from the tours. It is applied on random selected tour T k (t) in random selected node. If more candidates by which the selected node can be replaced occur, the new node is random chosen from the candidates. If mutation fails on all nodes of the tour, another tour is chosen. After all mutation operations are performed, crossover operations are applied. Parent strings are random selected. If no tour has common node with the first selected tour, another first tour is selected and the random selection process is repeated. Since genetic operations may produce strings with loops, in ACO framework prior and immediately after each genetic operation a loop removal procedure is performed. Genetic operations do not have to be necessarily feasible. Feasibility of genetic operations depends on the graph and generated tours. 4) . For each setting 500 trials were performed. The results received with GO are better almost in any case. 5 -7) . 5 ). 7) . with no crossover pair certain amount of mutation operations should be spread out among more paths, but with 2 crossover pairs concentration of mutation operation on less paths tends to perform better. It does not have the highest value on edges of the surface where the highest amount mutation operation is. Genetic operations where nearly always feasible; ratio accomplished / required mutation operations is 100% and for the crossover operation over 99%. 9 ). This is caused by the search space dimension. GO does not affect the length of the search process. Limit of crossover is 60% of crossover rate. Mutation operation causes better results than crossover operation. The higher amount of mutation operations the higher the performance gain is. No limit for amount of mutation operation was found during the simulation. Without crossover operation distributed mutation operation has better performance, but with two crossover pairs concentrated mutation operation on less paths tends to perform better.
paper_78	 Glycyrrhiza uralensis is an endangered medicinal plant species and mainly distributed in North China. The results suggest that fuzzy C-means clustering is an useful technique for classification of plant community. Medicinal plant is important natural resource and significant for people health in many countries and regions, such as China, Japan, Korea, India and so forth  [8, 9] . The Fuzzy C-means clustering attempts to divide a finite collection of n elements (plots) formula_0 into a collection of C fuzzy clusters with respect to some given criterion. Given a finite set of data matrix (species × plots), the method calculates a list of C cluster centres  formula_1 formula_2 Where i = 1, 2, …, N = the number of plots; j = 1, 2, …, C = the number of clusters; U = {U ij } = the matrix of membership values, U ij is the membership of plot i in cluster j; V is a matrix of cluster centers; m is fuzzifier which determines the level of cluster fuzziness (1 ≤ m < ∞). A large m results in smaller memberships u ij and hence, fuzzier clusters. Based on a general survey of Glycyrrhiza uralensis and its community distribution, five study regions, Chifeng (in inner Mongolia), Hengjinqi (in inner Mongolia), Minqin (in Ganshu), Aletai (in Xinjing) and Kashi (in Xinjing), were selected as sampling sites  [7] . Glycyrrhiza uralensis + Stipa bungeana. The community has a total cover of 75%, a shrub layer cover of 20% and an herb layer cover of 65%. spinosa, Lespedeza darurica, Pedicularis resupinata, Potentilla anserine, Saussurea epilobioides, Artemisia sacrorum, Artemisia mongolica, Cynanchum hancockianum, and Vicia amoena. The average cover of Glycyrrhiza uralensis in this community is 25% with a density of 3600 ha -1 . Glycyrrhiza uralensis + Polygonum bistorta. The community has a total cover of 70%, a shrub layer cover of 5% and an herb layer cover of 70%. The common species are Oxytropis myriophylla, Polygonum divaricatum, Adenophora gmeliniia, Potencilla acaulis, Suaeda prostrate, Astragalus melilotoides, Allium condensatum, Artemisia ordosica, and Oxytropis grandiflora. Glycyrrhiza uralensis + Ephedra przewalskii + Cancrinia discoidea. The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 60%. The common species are Caragana korshinskii, Elaeagnus, mooceroftii, Suaeda prostrate, Artemisias phaerocephala, Saussurea laciniata, Saposhnikovia divariicata, Oxytropis glabra, and Artemisia ordosica. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 59500 ha -1 . The community has a total cover of 65%, a shrub layer cover of 10% and an herb layer cover of 60%. The common species are Salicornia Bigelivii, Carex duriuscula, Stipa sareptana, Artemisias phaerocephala, Alopecurus pratensis, Saposhnikovia divariicata, and Carex pediformis. The community has a total cover of 60%, a shrub layer cover of 5% and an herb layer cover of 58%. The common species are Cleistogenes squarrosa, Caragana pygmaea, Hordeum brevisublatum, Ephedra sinica, Achnatherum sibiricum, Artemisia frigida, Viola tianschanica, Carex duriuscula, and Alopecurus pratensis. Glycyrrhiza uralensis + Astragalinae triloa + Stipa sareptana. The community has a total cover of 75%, a shrub layer cover of 10% and an herb layer cover of 70%. The common species are Artemisia scoparia, Kochia prostrate, Potencilla acaulis, Artemisia frigida, Ceratoides lates and Atraphaxis frutescus. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 6000 ha -1 . The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 65%. The common species are Caragana pygmaea, Astragalinae triloa, Stipa parpurea, Festuca logae, Artemisia kaschgarica, Polygonum viiiparum, Ephedra equisetina, Glycyrrhiza inflate, and Alyssum desertorum. Glycyrrhiza uralensis + Festuca logae + Stipa sareptana. The community has a total cover of 80%, a shrub layer cover of 10% and an herb layer cover of 75%. The common species are Artemisia parvula, Scorzonera divaricata, Roegneria kamoji, Potentilla bifurca, Carex duriuscula, and Ranunculus japonicas. These communities are important not only for protection of medicinal plant species but also for conservation of ecosystems and their environments in semi-arid and arid regions in North China  [23, 24] .
paper_96	 This humane technology innovation is the specific embodiment of environmental science and technology aesthetic theory in the field of scientific and technological innovation. It has a positive and promoting role to the development of transportation and blind-man welfare in China. During last summer, I went blind orphanage as a volunteer, and personally experienced the inconvenience and hardship of the blind when they travel. Especially when I heard the news that a blind man had been hit and killed when crossing the street just several days before because of his blindness, my compassion was once again inspired, and I was very eager to do my best to help the blind. The research on "warning system of embedded electronic guide dog avoiding obstacles" in this paper is an important part of intelligent electronic guide dog system, the study of which is mainly based on the embedded system, anti-collision warning system, and the USB technology is applied to warning system of embedded electronic guide dog avoiding obstacles. The design of obstacle-avoiding early warning system of embedded electronic guide dog is an inheritance and innovation, based on the design of traffic information collecting system, which is generally used in vehicle anti-collision at present. The overall design plan of obstacle-avoiding early warning system of embedded electronic guide dog mainly is the velocity and distance measurement module, the power circuit module, the LED screen show module, USBcommunication circuit module, and clock circuit module. The hardware of the obstacle-avoiding early warning system of embedded electronic guide dog can reference the Figure 1:  Figure 1 : Schematic diagram of hardware system structure  [4]   In the anti-collision early warning system, referenced in the design of obstacle-avoiding early warning system of embedded electronic guide dog, there mainly exits four steps, that is, information collection, information processing, information judgment, and warning information. The techniques of information collection are ultrasonic wave, laser, infrared ray, machine vision and interactive method. Presently, the types of the generally used anti-collision warning systems mainly are radar anti-collision warning system, ultrasonic anti-collision warning system, laser anti-collision warning system, infrared anti-collision warning system, machine vision anti-collision warning system, and interactive intelligent anti-collision warning system. The selection of anti-collision warning system for the design of obstacle-avoiding early warning system of embedded electronic guide dog should start from the characteristics of the highway network and the street network construction in our country, combing with the characteristics of the obstacle-avoiding early warning system of electronic guide dog, as well as the construction of our country's highway and street traffic integrated management system. The full name of ARM is Advanced RISC Machines. The ARM architecture follows the principle of reduced instruction set computer (RISC). It extensively uses the registers with a fast speed of instruction execution. Thereinto, the commercial RTOS includes WINCE and VxWotks; while the free RTOS has Linux (uCLinux and RT Linux included) and uC/OS -II. UCLinux aimed at the micro-control field, designing the Linux system, which is specially designed for the CPU without MMU (such as the S3C44B0X adopted in this project), and it has done a lot miniaturization work for the Linux kernel. These are the most remarkable features that uCLinux owned. The reference model is shown in  Figure 2 . This humanized technology innovation is the embodiment of environmental science and technology aesthetics theory in the field of science and technology innovation practice.
paper_134	 Data mining, also referred to as knowledge extraction from databases, is one of the most important analytical methods for identifying the relationships between the various elements of the information collected in order to discover the useful knowledge and support of strategic decision-making and sustainable development systems in various industries. Data mining takes advantage of the progress made in artificial intelligence and statistics. In recent years, nondestructive methods have been considered in purification. The purpose of the development of these methods is to estimate the quantitative and qualitative characteristics of the materials rapidly, non-destructively and reliably  [4] . Based on the dielectric method, when a material is placed in the alternating electric field, the positive and negative charged particles in it will constantly tend to move in the electric field. When in a country where milk is eaten up by 30 percent as a result of a rise in the price of a few hundred tons, what else would you expect from a 60 to 80 thousand Riels per liter consumption of lubricants  [9] ? The great benefit of mixing vegetable oils with lubricant makes it hard to find the type of product. The second pertain to data mining algorithms; third part related to samples and used methods in the article. Lizhi et al. The results of the PLS calibration model showed good results for the adulterated oils with soybean oil as a counterfeit oil. Reggie et al. (2006) predicted egg quality parameters using its capacitive properties. Experiments were carried out on the day of laying, on the third day, on the sixth day, on the ninth and twelfth days after the laying. Soltani et al. The correlation coefficients and the mean square error of the statistical indexes were 0.98 and 0.0006, respectively. 70% of the data was for network training, 15% for validation and 15% for the network testing. According to the researches that carried out in relation to the determination of the quality and content of agricultural products and food industries, it can be concluded that different methods have been developed to counteract the adulteration in these products. In this study, a recent study has been carried out to identify the authentication of olive oil. Samples of olive oil provided from Khorramshahr Oil Company and produced at Rudbar oil plant located in Manjil. The samples of sunflower oil, canola oil and corn oil which are known as adulterated oils were also obtained from national markets. The dielectric experiments started on the dielectric parameters of olive oil one day after the preparation of the sample. The device used consists of the Arduino board, ICL8083 and AD8302. (Figure 1 .) One of the new ideas in identifying and categorizing patterns is a backup vector machine or SVM. Backup machines have very valuable properties that make it suitable for pattern recognition. In particular, SVM does not have the problem of local optimization in its training, it builds the categorizer with maximum generalization, determines its structure and topology optimally, and functions nonlinear differentiation easily with low computing using It forms the concept of an inner product in the Hilbert space. In this study, coarse function was used to regression test data. Using this technique, the amount of RMSE for the samples were equal to 3.56, 2.45, 3.05 and the amount R 2 was equal to 0.92, 96 and 94, respectively. According to the results, Most R 2 related to Olive-Canola (0.90) and the least amount of R 2 pertain to Olive-Sunflower (0.86). The results were predicted and modeled using regression methods. A pair of matched logarithmic amplifiers provides the measurement, and their hard-limited outputs drive the phase detector. In this study, Quadratic function was used to regression test data. Figure 2  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn Using this technique, the amount of RMSE for the samples were equal to 3.65, 2.46, 3.31 and the amount R 2 was equal to 0.92, 96 and 93, respectively. Also device used can classify samples and use for other oils.
paper_139	 Much effort made towards in investigates mechanism with identical detection algorithms, still the retrieved web documents with outmodedlink. Using this KTMIN-JAK-MAXAM algorithm accessing of web pages with reduced time and space complication. Retrieving relevant information from web without redundancy is more challenge task nowadays where in web mining communities  [3, 4] . Utilizing customary data recovery  [5]  and information mining systems it get to the known and obscure data from the Web content. Web mining is categorized into three group Web Content mining  [6] , Web structure mining, Web usage mining. Traditional web mining algorithms handle with structured document  [7] [8] [9] [10] [11] [12] [13] [14] [15]  than the advanced methodology of mining algorithm can dealthe entire heterogeneous document comprises of images  [9] , graphs, videos  [16] , etc. Once the document is preprocessed, Normalization of tokens is generated to further process the web content document. Initially calculate measure for all the vertices and maintain the set U which contains a minimum and maximum degree for all vertices and isolated measure vertex. Repeatedly include the minimum measure in the set-up and each measured vertex included only once in the set-up. After applying the above steps the entire vertex without redundant information available in the set U. Pseudo Code forThe Proposed Algorithm KTMIN-JAK-MAXAM Step1: Compute degree measure for all vertices in the setup. Step2: Pick the minimum degree vertex 'v' in the set-up and include in the set U. Step3:While U doesn't include all vertices Step3A: Include the entire isolated vertex which is adjacent to the vertex 'v' to U. Step3B: Find the adjacent vertex 'u' to 'v' which is not in U and has maximumdegree. Update 'u' to U. Step3C: Update the value of degree for all adjacent vertices of 'u'. Iterate throughall adjacent vertices if possible. 4.2 Search the node B, the unvisited adjacent node is from B as 2 and 7. 4.3 Now travel around the node 2, the unvisited adjacent node is from 2 as 1 and 4. 4.4 Discover the node 1, the unvisited adjacent node is from 1 as only 3. Also we observed derived linked graph need not be unique but this approach will provide the optimized cost analysis report in future in data science field.
paper_145	 Among them 44.3 percent were overweight and obese. Majority (70.6%) were diabetic patients. The most common medical morbidities associated with obesity include impaired glucose tolerance and metabolic syndrome  [11, 12] . It had been observed in some research findings that youth who do not meet guidelines for dietary behavior, physical activity and sedentary behavior have greater insulin resistance than those who do meet guideline  [12] . For this reasons, World Health Organization considers the epidemic a worldwide problem which requires public health intervention  [13]  that act on different factors associated with overweight and obesity as well as technological changes that have lowered the cost of living of the people so that people can avail sufficient food with required protein. Efforts are needed to improve the economic, political, social and environmental conditions so that congenial atmosphere prevails in the society for maintaining healthy life of the people. Even government and public health planners remain largely unaware of the current prevalence of obesity which is the cause of many diseases  [5] . The investigated diabetic patients were 544. The number of this latter group of respondents was 346. However, among this latter group of respondents also there were 91 diabetic patients. Thus, finally, the analysis was performed using the data of 635 diabetic patients and 265 non-diabetic people. The data were collected through a pre-designed and pretested questionnaire during the months of May and June, 2015 by some undergraduate and post graduate students of American International University-Bangladesh, most of whom were doctors and nurses, of the department of Public Health and they were associated with public health services. The data were collected from the diabetic patients of the working places of the investigators according to their convenience. But during investigation some of them were found as diabetic patients. However, from the filled-in questionnaires 356 were found in completed form and the information of these 356 respondents were included in the analysis. The questionnaire contained questions related to sociodemographic characters of each person. Questionnaire also contained questions related to the stage and type of diabetes, treatment stage of disease, pre-cautions against the disease and the stage of complications due to the disease. The latter information were provided by the diabetic patients. The inclusion was justified as all the R 2 values were found significant. Obesity was observed among 15.3 percent people and overweight respondents were 34.0 percent. There were 58.9 percent males among the respondents and 47.2 percent of them were normal. The corresponding figure among females were 37.3 percent. But more Muslim respondents (43.8%) were normal compared to Non-Muslim respondents (38.8%). Similar normal group was noted among the other group of respondents. The overall normal group was maximum. Among the smokers 47.2 percent were normal and 37.2 percent were overweight. Table 11  Among the smokers 47.2 percent were normal and 37.2 percent were overweight. The results related to the justification of inclusion of variables were presented in  Table  12 . The factor analysis extracted 5 components as these components explained 73.309 percent variation in the observations of obesity. The coefficients of the components were presented in  Table 13 . These coefficients were observed from the first component. This component explained 25.733 percent variation in the data of obesity. The second component explained 16.161 percent variation of the data. The second component showed that the most important variables to explain the variation in the data of obesity were residence followed by religion and income. This component explained 10.86 percent variation of the obesity. The analysis presented here was done from the data collected from 635 diabetic patients and 265 control group of respondents. The selection procedure was a convenient sampling plan. The analysis was done from the collected information of 900 respondents. The incidence of obesity cannot be avoided, but its prevalence can be reduced by implementing appropriate action plan. These are: a) Halt the rise in body weight by encouraging people so that they can take healthy home made food and avoid restaurant food / first food. People may be advised to walk daily for at least half-an-hour. The public health authority can play a decisive role for the above steps.
paper_212	 Among other countries in the world, Kenya is among the twenty two that account for 90% of expectant women living with HIV. Among the expectant women there are 13,000 new HIV infections among children. The number of those that died account for 7% of the global total. Sex workers have the highest percentage of HIV prevalence at 29.3% according to the Kenya HIV estimates report-National Aids Control Council. There are various questions still left unanswered to date on the HIV epidemic. This model approximated that there would be 6000000 cases of HIV and 400000 cases of AIDS in China if there were no forms of interventions implemented. The group most affected would be the 31-40 years group. The group comprising of the largest individuals predicted to be living with HIV/AIDS was approximately 650000. There is need now more than ever to develop the SIR model since its application is going beyond epidemiological application such as how cues influence behaviour in a social setting and the spread of ideas  [5] . In several concentrated epidemics, HIV prevalence estimates do not match reported cases and mortality estimates do not match reported deaths, even after adjusting. There are issues estimating prevalence in high risk groups and the size of high risk groups. They considered and analyzed a two stage SI model that allowed for random variation in the demographic structure of the population with the population size changing at different times which had an exponentially distributed rate of infection. The parameter depended on the varying population size N. This meant that both the population size varied as well as the transmission/contact rate. The initial conditions changed over time and demographics not being included such that change over time was described as; (1) formula_0 The Kermack-McKendrick theory was later developed to a version where they tackled the problem of endemics  [14] ,  [15] . This transformed the theory to the basic SIR model such that when demographics were included becomes (4) (5)  (6)  where N denotes the total host population. denotes the birth rate and death rate denotes the infection rate denotes the recovery rate t denotes time point Model development The Gillespie algorithm was used to simulate a statistically correct trajectory given initial SIR conditions. The model explored how altering transmission dynamics affected the model as a whole. The death rates were distinguished such that one death event led an individual out of the model while the other death event led an individual into a different classes. All these aspects determine the quality of the inference drawn. denotes the rate of birth denotes the rate of non-AIDS death denotes the rate of infection denotes AIDS death rate denotes model's time step Gillespie's procedure The Gillespie simulation procedure was developed to produce a statistically correct course for finite well-mixed populations  [16, 17] . The compartments consist of initial state values S(t 0 ), I(t 0 ) and R(t 0 ) are contained in a vector and described at initial time t 0 . , , This Gillespie's algorithm based stochastic SIR model generates a statistically correct trajectory from the initial vector as , , where i=s, i, r S+I+R=N i denotes the population size of the state at time t denotes a function characterized by two quantities as a state change vector and a propensity function. , the state change vector defined as , , where is the change in state i caused by one event. Assuming that the resulting state is . A Markov chain model is one where the probability of the next event depends on the probability of the present state. A Markov chain is interpreted here then, as a stochastic discrete-valued model with the Markov property that future states of a process depend on the current state. The transition probabilities assigned are defined on an open interval (t, t + ), such that the probability an individual moves from the susceptible compartment to the infectives compartment is  [ 1 ] . The data was obtained from NACC for HIV/AIDS cases. Considering the hypotheses, 4 = If the mean and variance of the simulated and natural data are equal, the simulated mean does not fit the data. 4 5 = If the mean and variance of the simulated and natural data are not equal, the simulated data fits the natural data. A stochastic SIR model was simulated with a mean step size of 0.006336446. Variables in the model were S = 3507162, I = 45820, R = 4597, parameters in the model are crude birth rate of 0.06, non-AIDS death rate of 0.025, transition rate of 0.1 and AIDS death rate of 0.48. Curves produced are illustrated below. Therefore, the conclusion is that the simulated data model fits the natural data model. The simulated curves were compared to HIV/AIDS data. Recommendation for research would be to explore other variations of the SIR model such as SI, SEIR under Gillespie's algorithm. The SIR model as well as Gillespie algorithm could continue to be applied other areas such as viral marketing and behavioural science as has already been done successfully.
paper_214	 It is the only method that can be applied to unstructured problems. Quantitative mathematical models are embedded in base models, managed by subsystem a management model that requires separate users from the physical aspects of data processing and storage that extract, create, delete and modify models. Evaluation of search results depends on the method of presenting results and depends on the facilities of component dialog with users that provide inputs. Solving the result of communication between all stakeholders, sharing the general manager responsibilities both at decisionmaking levels and the corresponding subproblems defined. For example, after the first phase, the scope may relate to excessive spending decisions of a functional department, inventories too high or adoption of a draft research and evaluation on the introduction of computers  [3, 4] . Modeling takes expression of reality by means of abstract entities possessing quantitative and qualitative attributes. The assessment in turn depends on the search method. Methods called heuristics, based on a thorough analysis of the issue. Issues raised by the communication solution, accepting the decision or the additional costs of implementation are sluggish, and the decision-maker plays the important role of mediator. Data external economic information circulated nationally and internationally and usually come from the industrial sector of which the company, legal regulations. Personal data is data that relates to the behavioral aspects of decisionmakers in making decisions. The data source, internal or external, data is extracted and managed by a management database. The management of the database depends on the organization of data. In most cases there is SGBS transactional relational data system and a management database for multidimensional data warehouses created. The data dictionary is a catalog of all data from the database. It contains data definitions, data sources and their intrinsic significance. The subsystem management model consists of the following components: base models, the management models, dictionary and processor execution models and integration patterns Base models contain the set of models that make it possible to analyze the facts and the choice of options in terms required by the user. The models are domain-specific and models can be classified into strategic, tactical and operational models models. The models are used currently in operational and transactional system that aims of the organization. Database management system allows creating new models models using programming languages, update and modify existing models, establish interdependencies between models. In building a data warehouse is based on the analysis of data. Design is the stage where the data warehouse model is chosen, depending on the complexity of the system real user requirements and data structure existing in the company (databases, Excel spreadsheets...) Building a data warehouse there are three models: type star, snowflake patterns type and constellation type models. In such a model is highlighted: quantitative data centralized called measures of activity quantitative criteria for centralized aggregation  ,  referred sizes  relational table that stores the measures identified by   the facts dimensions is called table  Tables where aggregation criteria has explicit codes,  called type tables list. Facts associated table. The star is the type of aggregation criteria when codes are explained in type tables list. Data warehouse star The eastern type constellation when several schemes that use the same type star catalogs. The advantages of using databases for Decision Support Systems in businesses involves creating numerous queries by the decision maker and also allow to create many scenarious and variants to choose from. The databases permits flexibility in creating queries by choosing columns from dimension tables and measures from the fact table and so the decision maker may choose what economical indicators is useful to analyse and also permits to create different types of graphs such as columns, pies, scatter points or webs. By simply analysing these reports and graphs helps the decision maker to create different scenarious by changing target indicators and corelate with the economical indicators that can't be changed  [2, 5] .
paper_216	 The Black-Scholes model is a well-known model for hedging and pricing derivative securities. A number of studies have attempted to reduce these biases in different ways. Amongst the non-parametric approaches used to improve the accuracy of the model in this study is the Wavelet-based pricing model. The MSE and RMSE of Wavelet model is 0.208546 and 0.456669 respectively which is much lower than that of Black-Scholes model and therefore in conclusion, Wavelet model outperforms the other model. The financial contracts or instruments which derive their value from some other variables are called Derivatives. The ones traded on the exchange are standardized and regulated. European-style options can be exercised at the maturity date only while American options can be exercised at any time prior to or on the maturity date. American options need more complex pricing methodology due to the extra feature of early exercising. In the Kenyan market, derivatives are yet to be developed. The derivatives market has been inactive due to some of the factors such as low level of investor awareness and sophistication, inadequate risk management, lack of commodities on large scale and inadequate liquidity. The remainder of the paper is structured as follows: Section 2 discusses the existing literature; Section 3 presents an overview of the Black-Scholes model and Wavelet based pricing model. Section 4 describes the data, shows the empirical results and performance measures of the models. A lot of improvements have been done to the original Black-Scholes formula since the paper of  [2] . For out-of-money puts, there is high implicit volatility relative to the at-the-money calls and puts  [3] . In comparison between the risk-neutral MGF and the implied risk-neutral PDF, the risk-neutral MGF has a number of advantages even though between them, there is a one to one relationship. Also, wavelets can be used to improve analysis of volatility since they are a preprocessing denoising tool  [10] . The de-noising ability of wavelets was also recognized in  [7] . Evidence have been provided to support that wavelet based local linear approximation  [11] . Algorithm has a superior performance in de-noising financial data with high frequency by using the first principal component to de-noise frequency variations of a business cycle with wavelets  [12] . Lastly, wavelets can be used to estimate parameters of the models which are unknown using wavelets in pricing of an American derivative security by levy process  [13] . Another example is the application of the wavelets in nonparametrically estimation of a diffusion function  [15] . Where;  formula_0 From the Black-Scholes formula above, we see that the strike price (K), risk free interest rate (r), time to maturity (T), the volatility (") and the current price of the stock (S0) are required. is the cumulative distribution function. ʆ ! ( is the bilateral inverse Laplace transform. The underlying asset dynamics and investor expectation in option prices is captured by the MGF Θ !$ + of the logarithmic returns /0 7 . This needs to be approximated by wavelets. In order to approximate the implied MGF using the wavelet method, one has to choose a particular wavelet from a large family of wavelets. On this criterion, the Franklin hat function performs very well because it is symmetric, smooth and piecewise continuous. This function also emulates the probability density function of asset returns. The focus on this study was on pricing of a European Call option using two models, one parametric model (BS Model) and one non-parametric models (Wavelet Pricing Model). The superiority of the wavelet method comes from the ability of the wavelets to estimate the risk neutral MGF. Moreover, hedging performance may be evaluated with respect to each of the two models.
paper_219	 quality of service (QoS). It is proposed to add two new attributes pertaining to data sharing performance to QoS: data relevance (DR) and quality of data at source (QDS); and further a method to evaluate these new attributes. The QDS attribute brings distinction to the resultant data quality of the network's quality of service. One recent method subjectively assess the quality of data is to measure the user satisfaction referred to as quality of experience (QoE). The QoE is assessed for each of the framework's attributes using the best practices from survey statistics in sampling and estimation. Not all data is the same; some is more relevant to the users' needs when compared across all the data. Section 2 of this paper provides a brief overview of the origins of net-centric and net-ready policies. In the years following, the Chairman of the Joint Chiefs of Staff Instruction (CJSCI) 6212.01 was released that set-forth the procedures for development and certification of a Net-Ready (NR) Key Performance Parameter (KPP); the NR KPP process later being subsumed into the Joint Capabilities Integration and Development System (JCIDS) process  [3]    [7] . The NR KPP specifies the attributes required of data sharing Information Technology (IT) introduced into the net-centric operational environment: (1) IT must be able to support military operations (SMO), IT must be able to be entered and managed on the network (EMN), and (3) IT must effectively exchange information (EI)  [3] . To satisfy the attribute of support to military operations, IT deployed to the operational environment must support identifiable net-centric operational tasks and mission objectives  [3] . Net-centric operational tasks are those that "produce information, products, or services for or consume information, products, or services from external IT"  [3] . Fig. The most commonly used subjective rating in standards and in conjunction with QoE is the mean opinion score (MOS) where score of 1 is bad, 2 is poor, 3 is fair, 4 is good, and 5 is excellent. Sampling users is the preferred and direct method for measuring QoE. The question arises how to get the users' opinions. For a sample of size n consider v strata with sample s k in each stratum, i.e. ∑ i=1 to v n i = n and s = U k=1 to v s k . where n ≤ N and s = {1, 2,…, n}. The sample is organized into their various strata with the number of elements in each total number of strata k, in the total population N. The HT estimator for stratification becomes formula_2 and estimate of the stratified mean is formula_3 where m k = ∑ i in s k y i /n is the arithmetic mean of strata k. With each attribute having its MOS value using the stratified estimate in (3). The QDS takes into account the effects of environmental conditions on sensor performance for given design parameters. Traditionally these QoE MOS ratings were undertaken by panels of experts. A standard to address the rating of motion imagery (i.e. In  [15]  equations are formulated for quality in terms of interpretability and MOS; also with or without references. Over the last several decades there have been many papers on the topic of QoS. But just for completeness a brief explanation of QoS is given here. And the service level agreement (SLA) is the users' agreement with network provider(s) on acceptable ranges for the metrics. A simplistic rejection of non-relevant data is insufficient given the negative effect that excess amounts of data can have on human decision makers, even when the data in the network is restricted to only relevant data  [23] . still imagery, motion imagery, acoustic signal, electronic signal, radar) with each revealing different aspects for a given target of interest. The data's spatial properties relate to the location of the data collection sensor relative to the target of interest (e.g. overhead, side-view, rear-view, distant, near). Temporal properties of the data pertain to the time of data collection relative to the actions and conditions of the target of interest (e.g. target while in port, target while in open ocean, target when first detected, target after engagement). The metadata taxonomy needs to be sufficiently diverse to express the essential characteristics of the data product but not so overly detailed that the data tagging approaches the size and complexity of the data itself. A poorly performing taxonomy would be one that prompts extraneous definition of detail or that includes terminology with such subtle variation as to lack the mutual exclusivity necessary to select between one term or the other. An identical taxonomy between producer and consumer would simplify this process, but the way of expressing what one needs may not always match the way of expressing what one has to offer  [6] . We first discuss the design philosophy for the overall assessment of the data quality for the enterprise: (1) The enterprise data quality is determined by the attribute with the lowest user satisfaction (QoE). In game theory, the minimax value of a player is the smallest value that other players can force without knowing the player's actions. The same value is also the largest guaranteed value for that player with knowledge of the other player actions. Formal minimax definition  [24]  is formula_4 where a(i) denotes the actions of the i-th player of n players, a(-i) is actions of all other players except the i-th, and v i is the value function of player i. Consider a simple example in  Table 1  to illustrate the minimax in a game theory context. The cells in  Table 1  consist of a left value which is the games' pay-off for player one and a right value which is the games' pay-off for player two. Rows in  Table 1  represent actions of player one and columns in  Table I  represent actions of player two. Player one has three action options U, D, N to choose from. Going through each action of Player one with knowledge of player two actions L, R we have maximum payoff for player one for U of 5, for D of 5, for N of 4, making a minimax pay-off action of player one of N. Player two takes each action along columns for action L maximum payoff is 4 and 3 for action R, resulting in minimax action for Player two of R with payoff of 3. Thus, the minimax strategy is Player one move of N and Player two move of R with a payoff vector (4, 3). formula_5 Thus, for each d in D, starting with d=QDS, a state vector s max (d) is found that produces max s in S(d) f d (s) where f d (s) is a discrete function formed by a predictive model for QoE or in the absence of a model formed from direct user sampled MOS value calculated using (3), i.e. f d (s) = u HT . The x-axis of the chart in  Fig. 3  is organized to reflect the connection between QDS and QoS. 3 , as formula_6 In  Fig. 3 , a particular value of enterprise data quality is given at the start, but from start one could continue to improve QoS and QDS under Option A with no increase in the value of enterprise data quality, whereas Option B modifying the system delivering DR does increase the overall enterprise data quality. The framework's data quality model consists of three attributes: quality of data at the source, data relevance, and network QoS. The paper described the data quality using minimax decisions based on users' survey ratings for a given enterprise configuration.
paper_241	 Transformers are the key equipment in electrical power transmission. In A. It is expensive uninterrupted and desired to be kept in good condition always to have supply. Typically, Power systems are built to allow continuous generation, transmission and consumption of energy. The system is capable of sustaining a variety of environmental and operating impacts that resemble normal operating conditions which may include lightning striking the transmission lines, excessive loading, deterioration or breakdown of the equipment insulation resulting to power systems experiencing occasional faults such as phase to phase or phase to ground faults, over current, over voltage or even temperature based faults. These faults, as in the case of phase to phase or phase ground faults could cause an imbalance of phase current (i.e. differential current) and can be prevented using differential protection and microcontroller based relay protection. At all times, the Arduino senses the condition of the transformer. This paper is concern with the implementation of digital differential current protection of a single phase transformer using Arduino Uno microcontroller as a mechanism of differential relay. A power transformer functions as a node to connect two different voltage levels  [3] . It is transient in nature so it lasts for just a few seconds and does not cause any permanent damage to the transformer. The power transformer to be protected is connected to the main AC supply and delivered power (P=IV) to the output terminal. The step-down transformer is used to step down the primary or source voltage from 220 to 12V. This 12V is regulated to 5V (using L7805CV voltage regulator) which is the required voltage for the Arduino. The LCD display is used to display the differential current values of the transformer and to display the voltage and current level of the system as well.
paper_251	 However different users have different requirements of computational power and application and systems software. Hence demand of users is heterogeneous in nature so that varieties of application (hardware & software) have been developed to achieve the highest user satisfaction. and software tools. frequently not used. According to the Lewis Chunningham  [20]  "Cloud computing is the internet to access someone else's software running on someone else's hardware in someone else's data center". They are able to communicate i.e. social in nature, mobile i.e. autonomous and distributive in nature. They are following with respective functionality in the proposed system 1. 2. 3. i.e. These are following 1. Service scheduling delay 2. Elasticity optimization 3. Better Provisioning of the SaaS 4. The main lacking point in the article  [1]  and  [2]  is validation of the proposed mechanism. Additionally the requirements for such fast provisioning of the cloud has been discuss in the recent year in the article  [3] . Following goals has been achieved or solved with integrating of the Mobile Agent to Cloud Computing service realization 1. 2. For SaaS development Codenvy has been subscribed. 4. Resources: Algorithm for Scheduling (influenced from Aneka) developed onto the Cloudbees Proposed Algorithm Step1: Register user login Step2: Sign in the user login Step3: Select multi monitoring agent in cloud environment (for each subc_request with QoS constraints) Step 4: Mapping of resources between cloud client, cloud agent and cloud service providers. Number of tasks submitted at instant i (Ni) 2. Time to execute the task 3. Availability 4. Hence proposed system performs better than  [2]  and any other methods as shown in figure 3. Proposed mechanism has influences from the working of Aneka framework. The developed java web application (SaaS) has been developed with the help of codenvy SaaS developed platform. 1. Security enhancement using Agent for following attack internal attacks and DoS (Denial of Service) attack. 2. Develop a security perimeter based on anomaly detection using Application Process Management by integrating the mobile Agent on them for the cloud computing paradigm.
paper_272	 This paper focuses on the understanding of both characteristics and load specification relative to vacuum circuit breaker to generate precisely parameters  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] . J=2*10 6 (A/m 2 ) Joule heating as thermionic emission Where Ø is the thermionic work function. The term multiple re-ignitions refers to the series of alternate re-ignitions and high frequency (typical several hundred KHz)interruptions usually resulting in an increasing train of voltage peaks; this overall phenomena is usually defined as voltage escalation  [3, 4] . If the high frequency accompanying re-ignitions and voltage escalation in one phase couple into other two phases, the process of virtual current chopping can occur  [6] . It is important to appreciate that while current chopping and voltage escalation can occur in a single -phase circuit, virtual current chopping is specifically a 3-phase characteristic: the effects of normal current chopping, multiple re ignition and over voltage escalation in one phase can generate surge over voltages in the second and third phases. Restrike overvoltage due to the multiple re-ignition of circuit breaker when a switching interrupter process is initiated before current zero. The second transient ends up in a negative loop of current that changes the polarity to positive at just about 480 µs time scale indicates in the figure. These two high frequency transients and the voltage loop are associated with the current chop and the immediate re ignition of current. Notice that the negative polarity of the voltage loop and the negative direction of the two high frequency transients agree with the negative polarity of the last cycle of the current interrupted. We note that the only parameter involved is η. So that a family of generalized curves can be drawn from equation  4 for different values of η with dimensionless quantity -t`, as abscissa. Where η =0.5, the sine function changes from a circular to a hyperbolic function. We might have developed the curves for this condition, following the same argument. By calculating the inductor current in parallel RLC circuit under conditions of a subsidence transient, but have a far wider application. However to gain familiarity with these curves, consider a specific example where the inductor current is required in a circuit in which the components have the following values: R=10 5 Ω. L =5 Henrys, C = 2X10 -8 F. These values are typical of unloaded transformer, where R represents the equivalent loss resistance. Suppose Vo =13.8√2 KV So. Zo = √L/C = 5X104 ohms η. = 2. 2. 3. Chopping current times (400us-800us). 5. 6.
paper_294	 Cassava has played an important role as a staple crop in the feeding of the Tiv people. Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. The study sampled 377 out of the population of 20,000. Cassava has played an important role as a staple crop in the feeding of the Tiv people. The Tiv had different management strategies of postharvest losses of cassava (Akom or Alogo) that includes dried cassava chips (kpor), imyorun or imorun (gari) processing, fermented cassava (akpu) and production of base (mtuhem) among others, which were potent managing postharvest losses of cassava. Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. Specifically, the study sought to: 1. 1. What are the Tiv management strategies for postharvest losses of cassava? 3. They are not restricted to any class of persons in the community but freely available to all. This is their most crucial function of all. Moreover, other varieties of cassava in Tiv land of Benue State include: Akpu from which akpu are processed, Yakpe, Genyi, Wari and Gyo-Akom among others. Cassava (Manihot esculenta Crantz, Euphorbiaceae, Dicotyledons) is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world, mostly in the poorest tropical countries. Cassava plays an essential food security role because its matured edible roots can be left in the ground for up to 36 months. The crop therefore represents a household food bank that can be drawn on when adverse climatic conditions limit the availability of other foods. Postharvest priorities across the globe have evolved considerably over the past four decades, from being exclusively technical in their outlook, to being more responsive to consumer demand. Consumer-driven trends which have contributed to this shift include rising incomes in urban areas, changing dietary habits, more women in the work-place, reduced time for meal preparation and growing demand for safety, quality and convenience  (Rolle, 2006) . The food need not be removed any great distance from the harvest site, but it must be separated from the medium that produced it by a deliberate human act with the intention of starting it on its way to the table. These include: 1. Peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips) 2. Peeling the cassava, slicing, sun drying it and grinding into flour known as Mwem ma Kpor (Cassava Flour) 3. Peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom 4. Slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base) and 5. Apparently, any item of the instrument whose mean rating scores was 2.50 and above was considered significant and any item with the mean rating scores below 2.50 was not considered significant. What are the Tiv management strategies for postharvest losses of cassava? The collected data were analyzed and presented in  Table 1 . From  Table 1 , it can be seen that the mean rating scores of all the items are above 2.50. This means that peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips), peeling the cassava, slicing, sun drying it and grinding into flour known as Mwem ma Kpor (Cassava Flour), peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom, slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base), peeling the cassava, fermenting, sieving and squeezing it into lumps known as Akpu, peeling the cassava, grating, squeezing (demoisturizing) it, mixing with salt and other ingredients and frying with palm or groundnuts oil known as Kweesi or Akweesa Akom and peeling the cassava, grating, squeezing and sieving the starch and preparing it for food as Kamu Akom are the management strategies for postharvest losses of cassava. The collected data were analyzed and presented in  Table 3 . Table 3 , it can be seen that the mean rating scores of all the items are above 2.50. 1. Based on the results in  Table 1 , the findings from the study revealed that processing cassava into Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom, Mtuhem (Base), Akpu, Kweesi or Akweesa Akom and Kamu Akom are the Tiv management strategies for postharvest losses of cassava. Table 2 , findings of the study revealed that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). 3. Findings from the study based on the result in  Table 3  showed that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava.
paper_298	 Four research questions guided the study. Survey research design was used, registered users of the University Library between 2016-2019 numbering 920 formed the population of the study where a sample of 272 (30% sample) was used based on 95% confidence level and 5% confidence interval of Sample Size Calculator. The instrument used for data collection was questionnaire. Majority of the respondents used the library for research work rather than for leisure and recreational purposes. The library as a service oriented organization must provide the bibliographic resources and services channeled towards the fulfillment of its parent institution's goals and objectives. Like the card catalogues, sheaf catalogues, books, machine readable catalogues (MARCs) and online public access catalogues (OPACs). Library OPACs first emerged in the late 1970s and early 1980s and have gone through several cycles of change and development all geared towards improvement. Overall majority of respondents 80% satisfied with OPAC functionality  [4] . The study suggests that the users should be made familiar with the use and operation of the OPAC by providing special training  [6] . Kumar revealed that 26.99% respondents use the OPAC fortnightly and 22.84% weekly. 61.59% use the OPAC to locate a document on shelves and 58.48% to know the particular book is available on the shelves or not, 37.71% to know the bibliographical details, 31.14%. It must contain different types of materials, very rich in nature, comprehensive in coverage with adequate bibliographical tools describing the location of each item, which is significant to the whole concept of library and librarianship. b. The study is designed to answer the following research questions: a  The survey research method was adopted for the study, and questionnaire was the instruments use for collecting data. The total population of the study comprises is 920 undergraduate students that had registered with the University Library with effect from 4/1/2017 to 15/7/2019, covering 3 academic sessions. Thus, 272 respondents were selected for the study based on 95% confidence level and 5% confidence interval of Sample Size Calculator. However, out of 272 questionnaires that were distributed, the same 272 were returned and 262 were found useful because 10 respondents indicated not aware of the subject. The gender breakdown is presented on  Table 1 . The question on the demographic information of the respondents was on gender presented in  Table 1 . It shows that majority of the respondents 188 (69%) were male, followed by females 84 (31%). Librarians in discharging their responsibilities should inform library users by communicating the availability of new technology and the way it operates to them. Table 2  revealed that majority of the respondents 262 (96.3%) were aware of the existence of the library catalogue in the university library. 26 (10%) of the respondents were aware of the existence of OPAC as a retrieval tools in the library. Their responses were presented in the  Table 4  below. Table 5  reveals that 106 (40%) got their awareness through library staff, followed by those that knew it through the user education programme (GST) 88 (16%). Never  Total  F  %  F  %  F  %  100  20  40  28  56  2  4  50  200  14  32  20  45  10  23  44  300  24  44  20  37  10  19  54  400  42  75  12  21  2  4  56  500  58  100  0  0  0  0  58  Total  158  60  80  31  24  9  262  Key: F=Frequency, %= Percentage. Total  F  %  F  %  F  %  F  %  F  %  F  %  100  10  20  0  0  6  12  28  56  6  12  0  0  50  200  10  23  6  14  10  23  16  36  2  5  0  0  44  300  10  19  6  11  14  26  12  22  6  11  6  11  54  400  14  25  0  0  36  64  6  11  0  0  0  0  56  500  2  3  0  0  54  93  2  3  0  0  0  0  58  Total  46  18  12  5  120  46  64  24  14  5  6  2  262  The finding indicated poor use of OPAC 12 (5%) at the university library. Table 7  revealed that 118 (45%) of the respondents use catalogue to access information for assignment, followed by those that use it to retrieve information for research 96 (37%). It could be concluded that majority of the respondents used the library for research work rather than for leisure and recreational purposes. The results are presented in  Table 8 . It could be concluded that majority of the respondents used the library for research work rather than for leisure and recreational purposes.
paper_305	 This is primarily due to difficulties in uncovering uncertainties in information provided by credit applicants and also due to lack of reliable automated techniques that would improve the efficiency of manual underwriting procedures. In this paper, we report on the results of a MSc. Further, the complexity of loan decision tools and variation between applications is an opportunity for the use of a machine learning tool to provide learning capability that does not exist in other technologies. The development of machine learning models and tools has been welcomed as one of the most exciting in business settings. The ongoing changes in the banking industry, in the form of new credit regulations, the need for innovative marketing strategies, the ever increasing competition and the constant changes in customer borrowing patterns; call for frequent adjustments to credit management in order to remain competitive. Invariably, the amount of customer data required to effectively screen a loan application is usually huge; often not less than fifteen attributes. One of the earliest automated procedures uses statistical tools which have fallen short of the inherent challenge for today's commercial banks is their desire to understand large amounts of information and reveal useful knowledge to improve decision-making. These techniques have been found to outperform earlier approaches leading to increased competitiveness. Further, ensemble learning algorithms -those that combine a number of base algorithms, through empirical reports typically lead to better results. Credit appraisal often amounts to making a decision whether to grant or to reject an application. For many classification algorithms, this innovative strategy results in dramatic improvements in performance  [8] . Better algorithms that overcome these pitfalls have been developed and are collectively known as Discriminant Analysis (DA) techniques or simply Meta learning algorithms  [3] . There have been various other attempts to deal with the loan appraisal problem using various techniques  [10]  to varied degrees of success. Looks at all possible thresholds for each attribute 2. Resulting classifier is a simple threshold on a single feature a) Outputs a +1 if the attribute is above a certain threshold b) Outputs a -1 if the attribute is below the threshold In this study, 'majority voting' was adopted for combining hypothesis from different learners. A accept classification for a loan decision meant pointed to a successful application while a reject classification pointed to the alternative. It can be proven (as discussed here-under), that under the assumption that all individual classifiers have the same prediction rate and that the distribution of the data correctly classified by each base classifier is independent and random, this is the best possible strategy. The implementation detailed lay in the use of a logistic regression that models the posterior class probabilities Pr (G = k|X = x) for the K classes. Logistic regression models these probabilities using linear functions in x while at the same time ensuring they sum to one and remain in [0,1]. Select the single best classifier at this stage iv. Learn all K classifiers again, select the best, combine, viii. The NetBeans IDE which is a Java based open-source IDE was also used in the development of the system's graphical user interface (GUI) and for coding and testing of the system. The model was built using the training dataset and tested using three strategies. i. Separate data into fixed number of partitions (or folds) ii. This strategy relies on two separate files, one for training and the other for testing. The two files can be generated by portioning a given data set into two and saving them separately. This strategy is similar to the use of two files as discussed earlier but relies on the learner to automatically partition a given data set into two given a split percentage Options: -F -R -I 15 Number of performed iterations: 15 Time taken to build model: 0.06 seconds Time taken to test model on training data: 0.01 seconds The results were interpreted along the following parameters for all the various training and testing strategies. This gives an accuracy of 19/20=95%  Class =Accept: The number of correctly classified instances is 12 and that of instances classified as belong to the class is 13. This gives a precision value of 12/13=0.92 Class =Reject: The number of correctly classified instances is 7 and that of instances classified as belong to the class is 7. It exhibits better statistical foundations than other performance measure techniques with diverse application in medicine and computing. The ROC graph is a plot of two measures: Sensitivity: The probability of true classifications given true instances i.e. P(true | true) calculated as a/a+b from a standard confusion matrix 1-Specificity: The probability of true classifications given false instances i.e. Indicates a perfect prediction . 0.9. Excellent prediction . 0.8. Good prediction . Mediocre prediction . 0.6. Poor prediction . Random prediction . The value converted to 1 decimal place, these values indicate a perfect classification After a successful implementation of the stated system, the following were the key outcomes:  Three options were investigated for training the algorithm namely: a) The use of single file both for training and testing the model through stratified cross validation. This is a strategy where the training file was portioned into complementary data sets called the training set and the validation set.
paper_310	 We are now living in the 21 st century. Android application based smart phones are becoming each time more powerful and equipped with several accessories that are useful for Robots. We derived simple solutions to provide a framework for building robots with very low cost but with high computation and sensing capabilities provided by the smart phone that is used as a control device. Here we use mainly Arduino UNO (ATMEGA 328P), Bluetooth module (HC-05). We interface the Bluetooth module with the system so that we can easily control the system by smart phone application. Bluetooth module, DC motors are interfaced to the Microcontroller. The controller acts accordingly on the DC motors of the Robot. In achieving the task the controller is loaded with a program written using Embedded 'C' language. Related reference articles implementing wireless control of robots have been studied as mentioned in  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] . Few things like cost-effectiveness and simplicity in design, lowprofile structure etc. have been kept in mind before designing the project. c) Here the focus is on the latest technology of android and robot also called as 'Mobot'. g) The Bluetooth module is connected with the Arduino UNO board for the connection with the user. h) Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. The system consists of following parts: a) Arduino UNO (ATMEGA 328P) b) Bluetooth module (HC-05) c) Smart phone d) Motor driver (l293D) e) Arduino software (version 1.8.1) The basic building blocks of the project have been described below: Microcontroller will act as the brain of the robot. Generally our transmitter will be smart-phone and receiver will be Bluetooth module (  Figure  2 ). Here we the Bluetooth RC Controller application (  Figure 3 ) as the operating remote of this system. The novelty lies in the simplicity of the design and functioning. It is also interfaced with the microcontroller  (Figure 4 (a) ) and with circuit connections  (Figure 4 (b) ). Here we use programming language 'C' for coding. The program for executing this project has been written in C language. The program is stored in the EEPROM of the microcontroller, which is present in the Arduino board. Motor driver switches accordingly the data bit, if the data bit is low then the corresponding pin of the motor driver doesn't work else highbit then thecorrespondingpinof the motor driver is on. We have used Arduino IDE version 1.8.1 for writing program for Arduino. Second loop part where the program runs continuously. The working principle is kept as simple as possible. As seen from the  Figure 6 . The microcontroller, thereby, sends instructions, which when executed, helps in functioning of the motor driver. Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. The Bluetooth module is connected with the Arduino UNO board for the connection with the user. Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. In future we would try to extend the range using Internet of Things (IoT)  [12] . The Bluetooth module can operate below the 10 m range, which we would try to extend in future. Here we are using four 12 V, 200 R.P.M DC motors and a 9 V DC battery as main power supply of this system. Figure 8  shows the snapshot of the whole Bluetooth Based Smart Phone Control Robot Project.
paper_333	 Food security is a challenge in many developing countries like Ethiopia. Nations in our country are still struggling to make use of available resources so as to combat hunger. This is commonly known as false banana, the Ethiopian banana or the Abyssinian banana. Cheesman. As a matter of facts, visual or manual detections may have defects in terms of accuracy in detection along with lower precision. The remaining part of this paper is organized as follows. In Section III, we present the architecture of proposed system. Experimental results are reported in Section IV. Section V presents the conclusion and discusses of future works. Several types of diseases are known to affect Enset plants under field conditions. So far a number of fungal, nematode, viral and bacterial diseases were reported to cause damage at different degrees of intensity that was mainly explored by Quimio research finding since 1992. The occurrence, distribution and the incidence level also indicated to vary from one Enset-growing locality to the other. Figure 2  shows the architecture of the proposed system  A. Dataset Partitioning The images of the dataset are collected from different parts of southern Ethiopia. From all those disease category a total of 460 Enset leaf images are collected from which 368 is used for training and 92 images are used for testing. B. Different results were found by using different multiclass support vector machine classifiers. In order to measure the performance of the classifiers K-fold (in this case k is 10) cross validation method was used. If there is a shortage of experts in the area of those disease identification the system is going to be a replacement for the experts. Better results is achieved if the system is used by large number of dataset. It is a good option to use other machine learning techniques like Artificial Neural network and the hybrids of the other learning algorithms.
paper_389	 Word probability, the paper explores the probability characteristic of word location. Experiments on the corpus show that the introduction of the word position probability feature has improved the accuracy, recall and the value of Fl. For example, "O" can be used to represent individual Chinese characters, "B" means Chinese characters appear in the head, "I" means Chinese characters appear in the middle or the end of the word. Enter the sentence as "This is Wuhan." We first set three candidate marks "O", "B" and "I" for each Chinese character and add a start node "BOS" to the head, An end node "EOS". The feature that appears on each node is then calculated, using the feature weights to compute the most probable of all paths from "BOS" to "EOS". According to the meaning of "O", "B" and "I", the rules are summarized as follows: 1, the sentence of the first Chinese character tag can not be I, the last character of the mark can not be B. Because the mark B must be followed by a mark and only the mark I, and must be marked in front of the mark I, and marked as B or I. 2, marking O cannot appear behind the mark I, can only be O or B. 3, Mark B can only be followed by the mark I. Finally, according to the mark of each Chinese character, the mark result is "OOBIO", so the result is "这 / 是 / 武汉 /." The probability of the i-character of the position as the prefix: pY> 95% 昨狭抨第竞耽 3 Z The probability of the i-character as the suffix: pZ> 95% 丸役袄侣丸瞰 4 R The probability of the i-character individual word of the position: 85% <= pR <= 95% 吧枚刘磊躺却 5 U Position of the i-character as a prefix of the probability: formula_0 The probability of the i-character as the suffix: 85% <= pV<= 95% 貌型胁帘午岸 7 D The probability of the i-character position of the individual word: pD <= 5% 言辽改信申仪 8 E The probability of the i-character of the position as the prefix: pE <= 5% 业络王场姆杨 9 F The probability of the i-character as the suffix: pF<= 5% 增隐晋香浓伊 In order to deal with the long-distance information, this paper takes the context distance as 2. The probabilities of the position probabilities in  Table 3  are extracted from the training corpus and the probability of each position is calculated according to the following probability formula: P (individual word) = number of occurrences of the individual idiom of the word / total number of occurrences of the word * 100% (1) P (prefix) = number of occurrences of the word as a prefix / total number of occurrences of the word * 100% (2) P (suffix) = number of occurrences of the word as a suffix / total number of occurrences of the word * 100%  3 After many experiments and comparisons, the probability of more than 85% or probability of less than 5% of the word as a location probability feature, but also on the selected word for some filtering, the elimination of some unnecessary words, such as At the same time, select the probability of greater than or equal to 85% of the word is subdivided into the probability of 85% <p <95% and p> 95% of the two sets. Appropriate increase of p> 95% of the number of occurrences is in order to improve the characteristics of the sample expectations, and achieved good word effect. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. The standard corpus form used here dictates that each line in the corpus contains only one word, and the information associated with the word is followed by a tab stop followed by the word. The corpus is from 1950 to 2005, and the scale is 2564168000 sentences. The corpus content mainly comes from newspaper news. Word segmentation accuracy, also known as word segmentation accuracy, is the core performance index system. The performance of Chinese automatic word segmentation is evaluated by the following three indexes: correct rate (P), recall rate (R) and F value. Where, P refers to the accuracy of word segmentation; R refers to the word recall rate; F value refers to the P and R integrated value. The formula is as follows: Correct rate P = number of words correctly recognized / total number of system output words * 100% Recall rate IP correctly identify the number of words / test words in the total number of words * 100% F value F=*P*R/(P+R)*100％ The speed of word segmentation is another important index of word segmentation performance. The main factors that affect the speed of word segmentation are the structure of word segmentation dictionary and word segmentation algorithm. The query speed of the word dictionary depends on the organization structure of the dictionary. As the automatic word segmentation of the various types of knowledge to be obtained from the word dictionary, the system in the word processing needs frequent query word dictionary, word dictionary query speed will directly affect the speed of the word segmentation system. The results of "+ feature template 3" model are obviously better than that of "+ feature template 2" model, that is, under the condition of adding feature template 3, F-score is 4.4% higher than that of feature template 3, Played a better effect.
paper_391	 Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The data for this study were the Tetanus Toxoid data set from the Ethiopian Demographic and Health Survey (EDHS) 2011, and analyzed using the Knowledge discovery process of Selection, Processing, Transforming, mining, and interpretation. The WEKA 3.6.1 tool was used for classification, clustering, association and attribute selection. Single data instance test using Naïve Bayesian was done by creating test 1, test 2, test 3, and test 4 data test instance, three of them are correctly predicted but one of them incorrectly classified. But, in the class attribute, it is 0.72. And the major cause of early infant deaths in many developing countries is often due to failure to observe hygienic procedures during delivery. Tetanus is caused by a toxin produced during the anaerobic growth of Clostridium tetani. Infection is acquired through environmental exposure of any broken skin or dead tissue such as a wound or when the umbilical cord is cut to the spores of the bacteria. Immunizing the mother prior to childbirth with TT protects both her and her newborn against tetanus and antenatal care is the main programmatic entry point for routine TT immunization. Five doses of TT can ensure protection throughout the reproductive years and even longer. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The information is rich and massive. In this study, we have used several data mining techniques; classification, clustering, association and outlier detection techniques over the nominated Tetanus Toxoid immunization dataset of the EDHS 11. This problem caused the loss of time and effort in the healthcare system environment and spending lots of efforts and costs without having evidence-based information for planning and intervention. The standards used are a percentage of accuracy and error rate of every classification techniques used. Selection phase generates the target data set from the whole data set of EDHS 2011. Those patterns that remain represent the discovered knowledge. The EDHS 2011 was conducted by the Central Statistical Agency (CSA) in collaboration with Measure DHS and ICF International under the auspices of the Ethiopian Ministry of Health (EMoH). For this particular study, the dataset was requested and accessed from DHS website https://dhsprogram.com after formal online registration and submission of project title and detail project description. It has a dimension of 7033 rows and 12 columns. Only 80% of the overall data is used for training and the rest 20% was used for testing the accuracy of the classification of the selected classification methods. csv" file formats and stored as an ". arff" file format. How does this classification work? Data classification has two steps; Firstly, consisting of a learning step that is, where a classification model is constructed. The approaches are; (a). Decision tree (J48) approach It is a flowchart-like a tree structure. Where each branch represents an outcome of the test, each internal node denotes a test on an attribute, and each leaf node holds a class label. And the topmost node in a tree is the root node. This approach uses divide and conquers algorithm to split a root node into a subset of two partitions till leaf node that occurs in a tree. The training tuples are described by m attributes. Each of the tuples represents a point in an m-dimensional space. (c). (d). Multilayer preceptor Multilayer preceptor is a simple two-layer neural network classifier with no hidden layers. Understanding them will make it easy to grasp the meaning of the various measures. Training and testing are performed k times. "How does the k-means ( ) algorithm work?" For each of the rest objects, an object is assigned to the cluster to which it is the most similar, based on the Euclidean distance between the object and the cluster mean. The k-means algorithm then iteratively boost the within-cluster variation. The InfoGainAttributeEval with Ranker T used with respect to the class attribute and visualization was done in the attribute selection. As explained in the    The literacy status of the mother has high information gain (0.046) and followed by the literacy status of the mother (0.041) and the least information gained was head of household (0.00000147). (Figure 3 ) In this study, the data mining tool and algorithm like (J48, k-nearest, and Bayes) is used for selecting the training and test data, for classification, and K-means approach for clustering, single-dimension association rule to identify the best association. Our work extends to utilize the implementation of the dataset for data mining tool present in all sections to achieve a better rate of accuracy and improve the efficiency when analyzing the large dataset.
paper_402	 Mainly, ANN was utilized to model the nonlinear behaviour of fatigue and creep of Reinforced Concrete (RC) members  [5] [6] [7] [8] . Recently, research interest has revolved around the development of ANN models to interpret the behaviour of structural materials such as steel, concrete, and composites  [9] [10] [11] [12] [13] [14] . [24] . Ghafari et al. Despite the statistical advantages of ANN, it has been long regarded as a black box that evaluates functions using input covariates and yielding outputs. This technique makes use of ANN's complex computation and allows the SFS tool to select and remove the influential and redundant parameters, respectively. There are two types of SFS classes -mainly filter method and wrapper method  [28] , where Zhou et al. [29]  used the Markov Blanket with a wrapper method to select the most relevant features of human motion recognition. The total number of features was reduced rapidly, where this reduction helped the algorithm demonstrate better recognition accuracy than traditional methods. Moreover, Rodriguez-Galiano et al. There are two types of ANN models: (1) feed forward; and (2) feed backward. The input neurons are responsible for containing the independent parameter presented by the user, the wires represent the randomly generated matrices called weights that manipulate the function's slope or steepness, the hidden neurons map the weights variables using an activation function, and the bias units control the output function's shift, either upward or downward. Equation  1 shows the linear combination of mapping weights from each input neuron, via wires, to the hidden neurons. There are two types of search algorithms: sequential forward selection and sequential backward selection. Figure 1  shows the algorithm SFS uses when performing forward selection. The increment started from one neuron and ended with 15 neurons, where the model was analyzed 10 times, for each increment, because the Levenberg-Marquardt algorithm locates the local, and not the global, minimum of a function. Hence, for each neuron tested, ten NMSE values will be stored in a column vector, where each column vector will be averaged and plotted against its corresponding number of neuron(s). Figure 2  shows the plot of all the scenarios with the minimum point circled at 11 neurons. The SFS algorithm was run 200 times to capture all possible combinations of independent features when using ANN. Figure 3       The selected features, using SFS, were analyzed by the previous BPNN model. Table 3  shows the statistical measurements calculated for both cases. It was observed that the r 2 and NMSE before and after selection yielded 81.6% and 99.1%, respectively, and 0.0594 and 0.026, respectively. As a result, the ANN model with the relevant features was capable of predicting 89.6% of its values within the aforementioned boundaries, as opposed to the ANN model with all the features which predicted 58.7% of its values within the boundaries. Table 4  shows the coefficient values, with their corresponding symbols, for each UHPC constituent with the statistical measurements of the LSR model. Using Fly Ash quantities that range between 0-200 kg/m 3 and Silica Fume quantities that range between 40-160 kg/m 3 while fixing the quantity of cement at 1400 kg/m 3 and water at 175 kg/m 3 , several plots showing the variation of strength of UHPC were generated as shown in  Figure 5 . It is observed from  Figure 6  that there is noticeable increase in the compressive strength of UHPC with the increase in Flay Ash and more noticeable with the increase in Silica Fume. The correlation coefficient (r 2 ) before and after the use of SFS improved from 81.6% to 99.1% while the NMSE improved from 0.0594 to 0.026, respectively. 3) The ANN model with the selected relevant input parameters also showed a lower deviation (89.6 %) than the ANN model with all the features (58.7%).
paper_418	 Most of the existing studies have focused on how to choose between additive and multiplicative models with little or no regards on mixed model. For equation  (1)  it is assumed that the error term t e is the Gaussian white noise ( )  For equation  (2)  it is equally assumed that the error term t e is the Gaussian white noise ( )  An additive model is based on the assumption that the sum of the components is equal to the unadjusted data. They do not depend on the level of the trend  [3] . The multiplicative model was adopted when the magnitude of the seasonal pattern in the data depends on the magnitude of the series. In other words, the magnitude of the seasonal pattern increases as the data value increases and decreases as the data value decreases level of the trend. The additive model was adopted when the magnitude of the seasonal pattern does not change as the series goes up and down while the additive model was adopted when the magnitude of the seasonal pattern does not change as the series goes up and down  [5] . Iwueze and Akpanta  [6]  pointed out that an additive model is appropriate when the seasonal standard deviations show no appreciable increase or decrease relative to any increase or decrease in the seasonal means while a multiplicative model is usually appropriate when the seasonal standard deviations show appreciable increase/decrease relative to any increase or decrease in the seasonal means. This is an indication that the seasonal variation equals a certain percentage of the level of the time series. Sometimes the seasonal component is a proportion of the underlying trend value. Oladugba et al  [9]  gave brief description of additive and multiplicative seasonality. This is an improvement over what is in existence. The method adopted in this study is the Buys-Ballot procedure in descriptive time series. 3. These properties of row, column and overall averages and variances of the Buys-Ballot table are what could be used for estimation and assessment of trend parameters, estimation and assessment of seasonal indices and choice of the appropriate for time series decomposition. That is when 0 b =  (Table 1)  For mixed model, we obtain using the expression in  Table  1  ( ) ( )  formula_8 when there is no trend. formula_10 When there is no trend ( 0 b = ) we obtain from  (Table 1)  . . Table 3  that when there is no trend i.e. The data of 120 observations has been arranged in a Buys-Ballot table as monthly data (s = 12) and for 10 years (m = 10). The actual and transformed series are given in figures 3.1 and 3.2.
paper_428	 The present article examines the influence of thermal radiation on two-dimensional incompressible magnetohydrodynamic (MHD) mixed convective heat transfer flow of Williamson fluid flowing past a porous wedge. The solutions of the resulting nonlinear system are obtained numerically using the fifth order numerical scheme the Runge-Kutta-Fehlberg method. In Non-Newtonian fluids, the most frequently encountered fluids are pseudoplastic fluids, and Navier-stokes equations alone are insufficient to describe the rheological properties of these fluids, therefore, to overcome this defect, several rheological model like Ellis model, Power law model, Carreaus model and Cross model are presented, but little attention has been compensated to the Williamson fluid model and estimated to explain the rheological properties of pseudoplastic fluids. In this model both maximum viscosity μ (viscosity as shear rate tends to infinity) and minimum viscosity μ (viscosity as shear rate tends to zero) are to be taken. Williamson analyzed the flow of pseudoplastic materials and presented model to described the behavior of pseudoplastic material and explain convenient importance of plastic flows, and also recognized that viscous flows are very varied from plastic flows  [1] . Hayat et al. Nadeem et al. developed a model for the transport of Williamson fluid in an annular region  [10] . Khan et al. Hayat et al. Azimi et al. Srinivasacharya et al. Merkin et al. Jackson et al. Fu et al. Jafari et al. Bég et al. Porous wedge is a very important characteristic in science and engineering that can be illustrated as a material having a minute opening and the opening is almost filled with fluid. E 0 = E 6 , E′ 0 = 0, F 0 = 1 (9) E′ ∞ = 1, F ∞ = 0 Where E 6 is injection/suction parameter. The system of ordinary differential equations 7 and 8 subject to the boundary conditions 9 and 10 is first reduced to a system of first order ordinary differential equations using the substitutions E K =`, $ K = a, F K = b. This With the boundary conditions E 0 E 6 , $ 0 0, F 0 1, $ ∞ 1, F ∞ 0 (13) The resulting system in Eq. (11) (12) (13)  is solved numerically with the help of 5 th order Runge-Kutta-Fehlberg method. The transformed governing equations  (11) (12)  subjected to boundary conditions  (13)  are solved numerically by employing the fifth order Runge-Kutta-Fehlberg method. The influence of all pertinent parameters on flow and heat transfer are graphed and discussed in  Figures (1-8) . 1. Figures (1-2)  illustrate the influence of wedge angle parameter < with on velocity and temperature profile. Figure 6  drafts the non-dimensional velocity E′ for different values of suction parameter E 6 . Figures 7-8  illustrate the behavior of thermal radiation and Prandtl number on fluid flow region with M= E 6 = O = < =1. The important conclusions of the analysis are 1.
paper_432	 The Cotangent Bundles * of manifold d provides the basic model of a symplectic manifold. In case M is model on Euclidean space we have ≈ and so we want to assume that * ≈ * . Generically, the corresponding Hamiltonian system `(%) =& H '%, % ). 1, for any 0 ∈ , the tangent space at 0, denoted , is the space of linear derivations on 1 2, , that vanish on 3 2, , . Thus,T 5 M can be identified with (1 2, , /3 2, , )* the space formula_0 , is called the cotangent space at 0; it is isomorphic to the dual * , of M . Observe that if = 06 ∘ 8, as  (,)  , and ( 1 2, (,) / 3 2, (,) ) ** described above, DC corresponds to the linear map in T 5 * (M) . We see that(E > ) ,…, (E ) is a basis of T 5 * (M). < H , J = >= < = . When we take the basis vectors H =  The Cotangent Bundle Τ * contains the following classes of Lagrangian submanifolds; The fibers of Τ * . Since  formula_3  formula_7 which is the change of variables formula for integrals. b) Let exp ∶ → 3 > be given by exp(% =J •" … ∈ 3 > . For the mapping.∅: & → | to have a lift ‹ satisfying the condition ( ‚ ) =J ‚ , where ‚ ∈ &,J ‚ ∈ H, 0 (J ‚ ) =d 0 =( ‚ ), it is necessary that∅ (π n (&, ‚ )) ⊆0 (a n (H,J ‚ )) (3. For cOEOE . 1. 1)  Figure 2 . From which the required inclusions follow easily. 8 ' ( K , n, ' ) = 8 ' ( K, n ). ' , for K ∈ , n, ' ∈ . (n, ) = (n`n, ). these are easily seen to be free and proper. [n, ] = [n,`n, ] . Now consider a G on a manifold. šJ% ± ∈ , with isotropy subgroup = ² . A slice theorem (or tube theorem) is a theorem guaranteeing the existence of a tube under certain conditions. Note that ∅ is "injective mod '', meaning that ∅(± 1 ) =∅(± 2 ) if and only if ± > =n.
paper_444	 Traditional resource constrained project scheduling problem research takes into account achieve management goal in certain environment. However, there are many uncertainties in practical projects due to the uncertain factors, which leads to the change of resource availability. The other constraint is the resource constraint in which the demand of resource shall not exceed the total supply of resource for each resource type at any time. However, the majority of the above studies focus on RCPSP in deterministic environment, and suppose that the resource availability is a real number. In real-word projects, there may be some uncertainty phenomena, for example, overdue materials, the decrease in the number of workers at Grain in Ear season, etc, which result in the resource availability may be changed. Ying  [8]  proposed a schedule model of flexible work-hour constraint, in which the human resource was dealt with a new constraint to the classical RCPSP and the increased quantities of human resource were real-value variables. Its goal was minimizing the expected weighted sum of the absolute deviations between the planned and the actually realized activity starting times. Its constrains were the resource and priority rules. When the indeterminacy does not behave neither randomness nor fuzziness, a new tool is required to deal with it. Uncertainty theory based on uncertain measure founded by Liu  [12] , and its a branch of axiomatic mathematics for modeling human uncertainty. (4) No interruption is allowed for each activity in progress. " For some time-intensive and heavy-duty projects, managers tend to be completed as quickly as possible and cost to be minimized. Managers usually increase resource availability to improve construction efficiency and shorten construction time, but it often makes the projects with high costs. As to a pair of activity and 2, activity 2 start after its predecessor activity is finished. [16]  Let G be an uncertain variable with regular uncertainty distribution H. If the excepted value exists, then +IGJ = K Φ MA (<) A = <. (2) For instance, let G be a linear uncertain variable, it has inverse uncertainty distribution H MA = (1 − <)N + <O. If the function Q(R A , R P , ⋯ , R ) ) is strictly increase with respect to R A , R P , ⋯ , R S and strictly decreaseing with respect to R STA , R STP , ⋯ , R ) , then formula_2 holds if and only if formula_3 To solve the model (1), according to the operational law of uncertain variables, we transformed it into its equivalent form, as shown below. Model (1) is equivalent to the following model. is formula_6 By Theorem 1, we know that the excepted value of ? is formula_7 Since ℳ6∑ ∈8 9 ≤ + ; ≥ < = , then, ℳ6∑ ∈8 9 − − ≤ 0; ≥ < = . By Theorem 2, we have ∑ ∈8 9 − − Φ MA (1 − <) ≤ 0，  (12)  i.e. formula_10 Therefore, the model (1) is equivalent to the model (2). In this section, we give a project example which is the decoration of engineering in the bonded areas data center to illustrate the reasonability of the model. The constrains are recourse constraint and precedence constraint. The information of the activities. Cost  Preceding activity  1  0  0  0  0  2  1  5  1200  0  3  5  24  28800  2  4  27  16  103680 3  5  30  24  172800 3  6  22  24  126720 4  7  19  9  41040  4  8  24  6  34560  3  9  12  4  11520  8  10  22  20  105600 7  11  7  8  13440  6  12  20  8  38400  7  13  24  17  97920  5  14  12  6  17280  4  15  16  4  15360  7  16  11  6  15840  6  17  10  8  19200  14  18  8  6  11520  2  19  24  11  63360  18  20  13  4  12480  6  21  9  4  8640  12/15  22  3  4  2880  21  23  4  4  3840  7  24  0  0  0  22  With the above demand, we can present the following model:  In the real-life project, due to the influence of uncertain environment, managers should consider the trade-off between the completion time and cost. In future research, We can also focus on more types of project scheduling problems based on uncertainty theory.
paper_462	 Since 1998, the postgraduate education for master's degree in clinical medicine has been piloted in China. Chongqing Medical University was founded in 1956. It was founded by the former Shanghai First Medical College (now Shanghai Medical College of Fudan University) and moved to Chongqing. Since 1979, it has enrolled doctoral and master's degree students. In 1981, it was approved as the first authorized unit of doctoral and master's degree in China. In 1998, it became the first batch of doctoral and master's degree in clinical medicine in China. Pilot units for professional degrees. Since 1998, the school has vigorously developed professional degree postgraduate education  [1] . Since 2009, the school has been focusing on reforming the training mode of clinical master degree postgraduates (hereinafter referred to as clinical master). In view of the main problems existing in clinical master education, the school has carried out a series of reforms and explorations on the organic link between clinical master education and standardized resident training (hereinafter referred to as standard training). To solve these problems, the Academic Degree Committee of the State Council officially launched the pilot work of clinical medicine professional degree in 1998. At present, China's postgraduate education has entered a critical period of structural adjustment. To vigorously develop professional degree postgraduate education and improve the quality of training high-level applied talents is the focus of the current degree and postgraduate education work. In order to effectively improve the quality of clinical master training and strengthen the management of clinical rotation process, the school has set up postgraduate management offices in clinical colleges, implemented the system of professional degree tutorial group, and established three-level management systems of schools (graduate schools), departments (graduate management offices) and clinical departments (tutorial groups)  [6] . To ensure the quality of clinical master's training, the school has set up a guidance group composed of the first tutor and rotating responsibility tutor. The tutor who applied for the postgraduate examination is the first tutor (defense tutor). Each clinical rotating Department of postgraduate clinical rotation appoints a teacher qualified as a professional master's tutor as rotating responsibility tutor to ensure that clinical master's is in clinical rotation. Effective management during the transition period. Requirements for the first stage of training. Students who have obtained the certificate of resident standardization training are directly transferred to specialist training (i.e. the second stage of standardization training), which not only saves the time for graduate students to carry out clinical rotation, but also ensures the quality of training, and achieves seamless docking in the rotation cycle. The main reason for this phenomenon is that since the trial implementation of clinical medical professional degree, the state has not evaluated the quality of the pilot units and lacked an effective monitoring mechanism. There are mainly "pre-job training", "centralized departure assessment", "stage assessment" and "annual assessment"  [13] . The Interim Measures for the Implementation of the Regulations on Academic Degrees of the People's Republic of China promulgated in 1981 stipulates that before the postgraduate's reply, the degree-granting unit shall employ one or two experts from the disciplines concerned with the thesis to review the thesis. If the reviewers are hired, some experts will evaluate the papers according to the requirements of scientific degree papers. In order to pass the papers evaluation smoothly, postgraduates will increase the difficulty of scientific research, which directly affects the students' clinical rotation. At the same time, the cost of training clinical masters has increased substantially. It stipulates that clinical master can apply for a degree only by publishing a review or case analysis. Fourth, direct link the workload of tutors' guidance to professional degree postgraduates with the promotion of their professional titles, so as to improve the enthusiasm of tutors' guidance to professional degree postgraduates. The new "5+3" training mode built by the school covers all aspects of training, such as curriculum system, assessment system, award system, award system, rotation system, tutor system and management mode. It fully guarantees the quality of clinical master training and brings up a large number of high-level applied medical talents for Chongqing and even the whole central and Western regions, which has produced remarkable economic and social effects. Benefit. The school has continuously innovated the training mechanism and formed a "modular" curriculum system and a quantitative assessment system for clinical competence. The school has reformed the single tutor system and explored the establishment of clinical master tutor group system. The main results are as follows: The reform breaks through the restrictions of relevant industry policies on the training objectives and modes of clinical master, and provides innovative modes for brothers to learn from. It has been highly valued by the Ministry of Education and the Health Planning Commission, and has the realistic conditions for its promotion and implementation throughout the country. The employment rate of graduates has been guaranteed to be 100% for a long time. The article "Construction and Practice of Quality Assurance System for Master of Clinical Medicine Degree Postgraduates" published in "Degree and Graduate Education" won the second prize for Excellent Thesis of "Degree and Graduate Education", and it is also the only prize-winning thesis in the field of medical science. There are 9694 open beds in each affiliated teaching hospital, 57 resident standardized training bases, covering all the specialty categories of clinical medicine. At present, five affiliated hospitals of our university have been appraised as model planning training bases of the State Health Planning Commission (8 hospitals in Chongqing). Professor Jin Xianqing and Professor Xie Peng were appointed members of the National Medical Graduate Education Steering Committee in 1998 and 2010 respectively. Professor Xie Peng, Professor Huang Ailong and Professor Wang Zhibiao were appointed experts of the Discipline Review Group of the Academic Degree Committee of the State Council. The "four certificates association" should be implemented, namely "licensed physician qualification certificate", "standard training qualification certificate", "diploma certificate" and "degree certificate"  [17] . Through a series of reforms, it has achieved relatively ideal results and accumulated rich experience in reform. It hopes that through continuous exploration and reform, it can play a role in attracting more brothers.
paper_476	 Under drastic competition, major express companies have increased their daily delivery frequency to improve customer satisfaction and market share. In addition, it is important for the logistics enterprises to consider the constraints such as delivery resources and consumer satisfaction to achieve the appropriate decision of delivery frequency. at various stages in the delivery system, which will more likely to result in uneconomical performance. Therefore, it is necessary to analyze the quantitative relationship among the above-mentioned factors to ensure that the express company can achieve high delivery economy while realizing multi-frequency and fast delivery to improve the customer satisfaction. Fan Xuemei et al. Jesus et al. It proposed an assessment framework for joint delivery. Some other researches tried to improve the delivery efficiency and reduce the delivery cost through delivery center location optimization  [6] [7] [8] , delivery vehicle route optimization and scheduling  [9] [10] [11] [12] , and delivery resource integration  [13] [14] [15] . The system dynamics has a good applicability in analyzing the delivery efficiency, and some researches have achieved a series of results. Lin Wanting [22]  considers the phenomenon of unfilled orders, information leakage, and product damage which exists in crowdsourcing delivery, then built system dynamics model from five dimensions of reliability, responsiveness, assurance, empathy and security, which provide decision-making advice for enterprise delivery efficiency management. First, it analyzes the boundary and causality of its delivery system. It provides reference for express companies to determine the delivery frequency. As the first step, this paper defines the research boundary of JDL delivery system. Secondly, it constructs subsystems of cost and resource operation efficiency for the delivery activities in the boundary. Therefore, the storage and ferry crossings are negligible in this paper., Only the section within the dotted line in  Figure 1  is considered in this paper. The delivery cost mainly includes the equipment usage costs and labor costs in the three stages of sorting, transportation and terminal delivery. The equipment usage costs can be divided into two parts: the fixed costs and the variable costs. The fixed costs occurred in the use of equipment in the three links. The variable costs occurred due to the volume of orders, which include operating cost of the sorting equipment, rental cost of the site, units fixed cost of transport vehicle, fuel costs, toll and so on. Labor costs are composed of fixed wages and performance wages for employees in the three links mentioned above. The number of employees is affected by factors such as order quantity, transportation efficiency, delivery frequency, and the number of transport vehicles. Performance wage is determined by the actual amount of work performed during the delivery process. Sorting staffs consist of sorters and on-site logistics personnel. The sorter is the person who operates on the sorting equipment. The on-site logistics personnel is the person who puts the sorted goods into a mail packages and places them on a pallet and then sends it to the transportation vehicles. The number of sorting staffs is influenced by factors such as the amount of cargo, the number of sorting equipment, the worker's efficiency, the sorting time requirements, the area of sorting venues, the number of logistics personnel in the yard, the delivery frequency and so on. The usable area of the site consists of public area and working area. At the same time, the factors such as sorting time requirements, proportion of per-order batch, efficiency of the delivery personnel, and unit fuel consumption of the vehicle, will have a direct impact on the delivery frequency. Based on the surveys of JDL and interviews with related professionals, this paper summarizes 55 influencing factors on delivery frequency. These factors present a complex, nonlinear, and inverse relationship between each other. Therefore, this paper uses the causal loop method of system dynamics to analyze the relationship between the factors. Among them, the positive loop represents that there is a mutually reinforcing relationship between the factors, while the negative loop indicates that there is a balanced relationship between factors (Such as: Delivery frequency --> + Consumer demand response ability --> + Impact of demand response capacity --> + Consumer satisfaction --> + Order quantity due to change in satisfaction --> + Total daily order quantity --> + Per batch delivery order quantity-->+ Sorting center cargo volume-->+ Number of on-site logistics personnel--> -Onsite logistics personnel operating time-->+On-site logistics personnel costs-->+ Sorting personnel costs-->+ Sorting costs-->+ Total cost --> Total profit --> + Delivery facility input --> + Delivery frequency). This model passed the mechanical error checking and dimension consistency testing by VENSIM. On the other hand, the cost index was calculated based on the summed number of shipments as JDL adopts single-batch delivery. Second  Third  Fourth   1  60  30  40  20  2  30  30  20  20  3  10  30  30  20  4  -10  10  30  5  ---10  Table 4 . When the volume of orders increased from 1496 to 5300, the increased percentage of total delivery cost showed a downward trend followed by an upward trend. Regarding on the sorting efficiency equipment, the operation time of sorting equipment became longer and the sorting cost increased. The increased percentage behaved a negative growth trend followed by a positive growth trend as the order increased. The main reason is that the increase in the delivery frequency improved the demand for the number of delivery personnel and caused the delivery time of delivery personnel to be shorter in a single batch. As a result, the overall efficiency of the delivery staff was reduced. The reason is that the proportions of the sorting orders taken by the three sorting centers were different. This is because that the ratio between the order quantity and the actual number of delivery personnel at the delivery site was not equal. In the same delivery link the demand for delivery resources can be different due to different delivery operational capacities. Therefore, JDL should combine the simulation results with the actual situation before making the delivery frequency decision.
paper_479	 A forensic document examiner is saddled with the task of document authenticity. The L R theory takes its stance on odds-form from Bayes ' theorem. One proposition is 'The suspect is the author of the document in question'. A counter argument may be' The defendant is not the author of the document in question'. The propositions involved should be relevant and the latter case does not seem to be applicable. The oddity of the methodology depends on producing reproduced composing samples from an accumulation of composing samples acquired from a realized source to frame a database for evaluating the dissemination related with the numerator of a L R . While the utilization of probability proportions in the previous circumstance is currently rather entrenched spotlight on the insightful setting still remains rather past contemplations by and by. This method is genuinely direct for the score-based L R numerator, which involves creating a list of scores obtained by combining proof objects from the same source. Back Propagation Neural Network (BPNN) served the sake of supervised learning of the system. The rationale behind the choice of this algorithm is due to the fact that it is a supervised network and a supervised network will have a target, so the BPNN is a network that has a target. We target is set for each character in the handwriting. The target will help to know which handwriting is original and disguised. The neural network is a parametrized system because the weights are variable parameters. Transfer function can add non-linearity to the network. Each character variable has a weight W i which shows its contribution in the training process. The feature information extracted from student handwriting through LBP was entered in NN through input layer and the participation of each class of character variables is calculated by the hidden layer of the network using: formula_1 net j is ith type of factor, n is the number of factors in net i , & ' ! weight-recognition factor with weight ! " . The weight from input points i and two hidden unit j is ! " and ! Weight from second hidden unit i and output unit j is ! ) . Weight of additional edge for each unit is bias− θ, where input unit and output vector from the hidden layer are expanded with a 1 component as seen in  Figure 2 . Step 2: Backpropagation to the output layer This research looked for the first partial derivatives 12 1 ! ) If L R value greater than 1 H p is true If L R value less than 1 H p is false. Table 1  shows the results of our Estimated Likelihood Ratio (ELR) for a writer against any other author in a collection of appropriate databases where the upper confidence interval (CiU) and the lower confidence interval (CiL) are indicated.
paper_492	 Based on known functions of neuroscience the neural network that performs serial parallel conversion and its inverse transformation is presented. This function is the behavior of so-called short-term memory. Furthermore retroactively, although there is a quantitative difference in each part between brain of ape who does not speak the language and our brain, but our brain is consisted of same material. In Chapter 2, the circuits by combining the basic functions of neural circuits for serial parallel conversion and inverse of the conversion are shown. Although for learning process Hebb rules is used on the circuits, operations such as back propagation and Markov process are not used. The nervous system related to short-term memory is activated in synchronization with environmental events, but the nervous system involved in long-term memory is highly layered to form an image corresponding to past and future events. Acceptance and generation of time series data of the nervous system of long-term memory is carried out in consistent with short-term memory. The same is true for the recognition process. Deductive logical development is desired. The divided subsequence is defined the basic subsequence. The neural circuit corresponding to each basic subsequence can be easily configured in the neural network, leading to the realization of processing by hardware of general time series data. The dividing is done by the following procedure. (1) The first element is the beginning of the first subsequence. When the following conditions are true, the element allocate to the top of new subsequence. (2) If the same element exists in already divided subsequence. In this example a6 is the concerned element. (3) If the maximum length of subsequence is defined, a new subsequence is allocated after the subsequence that reaches the maximum length, add new element to the concerned new subsequence. The subsequences divided by above procedure are defined as the basic subsequence. Figure 2  shows the affinity with the neural circuit. For the next data c 1 additionally activates the elements which has been activated by the first data c 0 . Because the elements activated by c 1 is randomly connected to input, not all elements activated by c 0 are additionally activated. By receiving c 2 , c 3 and so on the activated portions become narrow and narrow. The output of the element holding the activity when receiving the last c 4 of the time series data is the recognition result of the time series data c 0 c 1 c 2 c 3 c 4 . Its output is the result of the serial parallel conversion, it is the result of AND logic of the output of the activated elements. that can be said a conversion of parallel to serial triggered by the first data. On the flow direction of data, this parallel to serial conversion is upside down with the serial to parallel conversion described above, but the basis of neural network operation is the same. When the state transition diagram of the  Figure 3  is seen as a serial parallel conversion, the bottom is inputs connected to such sensory organs. On the other hand, when the state transition diagram of the  Figure 3  is seen as a parallel serial conversion, another waiting state is needed. The essence of the neural network which performs the parallel serial conversion and the neural network which performs the serial parallel conversion is the same except that the conversion result comes out downward or upward. On other words, both operations of the serial parallel conversion and the reverse conversion are essentially the same on the point both operations are triggered by the first data and proceed waiting for input state change. For each basic subsequence a neural network that accepts and generates the basic subsequence is considered. The movement will be mentioned in the next chapter. The nervous system which is involved in the imitating function is called mirror neurons. If there is a problem in the episodic memory, it causes difficulties in social activities. Figure 5  is an illustration for showing the state change of each part in the neural network. The upper part shows the part related to episodic memory, and the lower part shows the part related to the short-term memory. Stimulus from the sensory organs from the bottom becomes time series data and is transmitted to the upper part. The part shown in red is a part that is particularly activated, the lower red disk is the part activated by the visual data of the chocolate plate that placed in front, the upper red disk is the part that is activating by recalled episode about the chocolate plate. Next, we introduce a function called functor between different categories. The object and morphism that make up the source category are connected by a function to the object and morphism of the target category. The process of migration can be described mathematically using free functor or free construction functor. It has been vaguely thought that the nervous system responsible for long-term memory may be in a different place from the nervous system responsible for short-term memory. Consistency is required when the objects in which both memories are involved are the same. The process of taking the consistency between the two memories was explained using the idea of a free functor in category theory. From among the random connections, the necessary connections for the desired operation are selected and enhanced, and the target function is realized. This process is close to the rehabilitation process of the brain that has had a stroke.
paper_507	 Landslide is defined as a slow to rapid downward movement of instable rock and debris masses under the action of gravity. Landslides are one of the major natural hazards that account for hundreds of lives besides enormous damage to properties and blocking the communication links every year. They are the catastrophic phenomenon taken lives of many a hundred and destroyed the hard earned money, disrupting the communication facilities. As per the official figures of United Nations International strategy of disaster reduction (UN/ISDR) and Centre for Research on Epidemiology of Disasters CRED for the year 2006, landslide ranked 3 rd in terms of number of death among the top ten natural disasters  [2] . As far as Indian scenario is concerned approximately 0.49 million km  2  or 15% of land area of country is vulnerable to landslide hazard and 80% is spread over Himalayas, Nilgiris, Ranchi plateau and eastern ghats  (GSI 2006)    [3] . The traditional practice of Landslide prevention is enabling people with Landslide Hazard Zonation Maps. 2. 3. Uttarkashi falls under the physiographic division (s) Rohilkhand plains, Nepal Himalayas, Ganga, Yamuna daob, Siwalik range, Kumaun Himalaya, Dhaoladhar range. slope, aspect, lithology, rainfall, land cover etc. DEM (Digital elevation model) was obtained from BHUVAN. The ANN is a black box model is a multi-layered neural network, which consists of an input layer, hidden layers, and an output layer. Each node is a simple processing element that responds to the weighted inputs it receives from other nodes. The arrangement of the nodes is refer-red to as the network architecture (  figure 18 ). Figure 18 . Architecture of neural network (source:  (Lee, 2009) ). The dataset is categorized into 60% training and 40% validation. The data is categorized into training, testing and validation, and all the six factors namely soil depth, soil type, rock and a excel database is created. The dataset is categorized into 60% training and 40% validation. In these points we categorized them into two categories landslide prone and non-landslide prone. Since ANN does not understand the 'landslide prone' and 'non-landslide prone' region we explain it by giving value '1' and '0' respectively. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer (20), and between the hidden layer (20) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). For a new dataset the weights are unknown. Most of the training datasets met the 0.01 RMSE goal. slope, aspect, lithology, rainfall, land cover etc. Slope 2. Soil depth 3. Soil texture 4. Height 5. The dataset is categorized into 60% training and 40% validation. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer  (15) , and between the hidden layer (15) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). The regression performance was 2.03, the accuracy for the training data was 0.99409, for testing and validation are 0.41565 and 0.18369. The most contributing factor is slope carrying 93% and the least one is soil depth. The reliability of ANN is high over other methods.
paper_1	 Clustering is a descriptive task of data mining. Clustering uses unsupervised learning technique in finding similarities between data according to the characteristics found in the data and grouping similar data objects into clusters. Its objective is to get groups of objects such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groups  [1] . It can be applied in various fields for instance taxonomy of living things, information retrieval from a document, identification of areas of similar land use in an earth observation database, discovering distinct groups by marketers in their customer bases for development of targeted marketing programs and identifying groups of houses according to their house type, value, and geographical location  [2] . A number of techniques can be used to do clustering. Clustering is the concept that was used in this research to create groups from social media data which can be used for learning on electronic learning platforms. The system design methodology used was incremental prototyping. In incremental prototyping, the whole requirements are broken down into building blocks which are incremented each time a new component is integrated based on an overall design solution. Several activities were performed to come up with the system. This generally dealt with preliminary processing of the data collected from the users to do away with any inconsistencies and outliers  [11] . The third step involved using the data already preprocessed above to train the prototype. The fourth step was testing the system. The prototype was then subjected to testing using the test data. They are part of the data that was used to train the system but its results are already known. They were used to confirm that the system indeed accurately did the classification given some data items. Finally, the model was used to classify a new user into a group. This involved picking the details of a new student from twitter and trying to predict the class hence group that he should join. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . This is summarized in the chart below. This matrix consists of the following parameters: TP, TN, FP and FN, which are defined below. Precision and recall were however average. These measures can be improved if large amounts of data are used to train the classifier before being used to do actual classification. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. They therefore come with a lot of challenges including time wastage. Through social media a better and more efficient way can be used to enable online learning generally and group formation specifically. It was able to extract tweets from various social media accounts based on a given hash tag (task) and then pass them to a Naïve Bayes classifier as input. Most of the information that passes through social media was being used majorly for social interaction. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. They therefore come with a lot of challenges including time wastage. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_2	 Current traffic light systems use a fixed time delay for different traffic directions and do follow a particular cycle while switching from one signal to another. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Junction timings allotted are fixed. Sometimes higher traffic density at one side of the junction demands longer green time as compared to the standard allotted time. The junction is a link to three roads which include; Effurun-Sapele road, Jakpa road, and PTI road. The conventional traffic light system based on fixed time is employed to control the traffic in this area. The signal timing changes automatically on sensing the traffic density at the junction. The design had a provision for pedestrians to request for crossing the road as and when required by pressing a switch. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Junction timings allotted are fixed. Sometimes higher traffic density at one side of the junction demands longer green time as compared to standard allotted time  [4] . In a bid to overcome this challenge  [4]  adopted an approach whereby a camera is placed on the top of the signal to get a clear view of traffic on the particular side of the signal so that it will capture the image. However, this system has the disadvantage of the controller since it depends on the preset quantification values for fuzzy variables  [6]  conducted a cross sectional study targeting traffic control in the city of Nairobi's Central Business District and its surroundings. The simulation runs results showed that the adaptive algorithms can strongly reduce average waiting times of cars compared to the conventional traffic controllers. The top down design approach was adopted here. It comprises of a 0V to 12V transformer which is required to step down the 220V AC supply to 12V AC. Below are the ratings of the transformer  The bridge rectifier consists of four single phase rectifier diodes connected together in a closed loop to form a circuit that is capable of converting AC voltage to DC voltage  [8] . A capacitor is needed to effectively carry out the filtering of the rectified AC signal to eliminate ripples  [9]  and can thus be calculated using the equation below. The PIV of the diode is calculated using the relation formula_1 Where Vp = peak voltage of transformer Vrms = root mean square voltage The Vrms of the transformer is given as 12v formula_2 Recall PIV ≥ 2Vp Therefore, PIV of the selected diode is given as 2 x 16.9706 = 33.9412v For current, the maximum current of the transformer I (max) = 500mA The forward current of the diode I f is given as:   formula_3 The reason for using PIC16F876A microcontroller over ATMEGA microcontroller is that, the former is cheaper and more readily available. Another purpose of an optocoupler is to prevent rapidly changing voltages or high voltages on one side of a circuit from distorting transmissions or damaging components on the other side of the circuit. The Transformer steps down the 220 v AC supply to 12 v AC. IR sensors are placed on the intersections on the road at fixed distances from the signal placed in the junction. The time delay in the traffic signal is set based on the density of vehicles on the roads. The IR sensors are used to sense the number of vehicles on the road. According to the IR count, microcontroller takes appropriate decisions as to which road is to be given the highest priority and the longest time delay for the corresponding traffic light. The Vero board is also called a strip board. It is a widely used type of electronic prototyping board characterized by a 0.1-inch rectangular grid holes with parallel strips of copper cladding running in one direction all the way across one side of the board. The PIC Microcontroller was programmed using Embedded C language. The codes are as shown in the Appendix. Simulation was done via Proteus software. On these days traffic rules are usually violated because of the complex traffic situation. The sensors help in keeping count of vehicles entering roads and the microcontroller subsequently allots time delay thereby giving accurate priority to each road. This new design would further reduce time delay, improve efficiency and reduce accidents by incorporating the following modifications: (1) The Airport -PTI road should have a two -way traffic light to pass the vehicles coming from Airport road to PTI road. There should also be a two -way traffic light between Effurun -Sapale road and Jakpa road to pass vehicles coming from Effurun -Sapale road to Jakpa road.
paper_3	 The aim of this article is to present the general architecture trends of Web-based Adaptive Educational Hypermedia Systems (AEHSs) and to give a complete description of architecture of the AEHS MATHEMA. Also, the formative evaluation of AEHS MATHEMA by students of the Department of Informatics and Telecommunications of the University of Athens, Greece, has shown, with the exception of other things, that all its functions are useful and easy to use. Adaptive Hypermedia Education Systems (AEHSs) can be considered as the answer to the problems of traditional hypermedia systems. They build a personalized model of the learner and apply this model to adapt the content and/or the appearance of the hypermedia, according to the learner's specific characteristics, such as the cognitive goal, level of knowledge, pre-existing or prior knowledge, interests, preferences, stereotypes, cognitive preferences and cognitive or learning style  [4] . (b) Adaptive Presentation: It adapts the content presented in each hypermedia node according to specific characteristics of the learner. (c) Adaptive Navigation Support: It adapts the link structure in such a way that the learner is guided towards interesting and relevant information, kept away from nonrelevant information either by suggesting the most relevant links to follow or by providing adaptive comments to visible links. (f) Intelligent Analysis of Learner's Solutions: It uses intelligent analysers that not only tell the learner whether the solution is correct but also it tells him/her what exactly is wrong or incomplete. (g) Example-based Problem Solving Support: It helps the learners in solving new problems, not by articulating their errors, but by suggesting them relevant successful problem solving cases, chosen from their earlier experience. (h) Adaptive Collaboration Support; Adaptive Group Formation and/or Peer Help: These techniques support the collaboration process either just like the interactive problem solving support systems assist an individual learner in solving a problem, or they use knowledge about possible collaborating peers in order to form a matching group relatively to the kind of the collaborative task. The general purpose of the AEHS MATHEMA is to support learners through an interactive and constructivist educational environment in order to construct their knowledge in Physics and Technology, individually and/or collaboratively, and to overcome their misconceptions and/or learning difficulties AEHS MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and meta-adaptive navigation, interactive problem solving, and adaptive group formation. More specifically it supports: (1) adaptive presentation according to learner's learning style; (2) four adaptive navigation techniques which are direct guidance (uses a "next" button), link hiding (hide nonrelevant links), link annotation (use colors to indicate relevance), and link sorting (like in search engines); (3) a meta-adaptive navigation technique in order for the learner selects the more appropriate navigation technique that best suits him or her; (4) an interactive problem-solving method through personalized and collaborative problem-solving activities that exploits the teaching approaches of experimentation through simulations, guided inquiry and discovery, investigations and peer collaboration; and (5) a method of adaptive group formation for peer matching. The rest of the paper is organized as follows: In the section 2 related works for various models of architecture of AEHSs is presented. In the section 3 a summary evaluation of the functions of the AEHS MATHEMA is presented and at the last section a summary is done and a discussion is taking place. Most of the Internet based systems use a variation of conditional reasoning where a decision is made to organize the text differently from fragments or select from a group of whole pages or even groups of pages. engine performs the actual adaptation by adapting or dynamically generating the content of nodes and the destination and class of links in order to guide each individual user differently. For each type of relationship (e.g., prerequisites) some generic adaptation rules can be defined. A key point in AHAM is that the adaptation rules are used to translate user actions into user model updates (as well as the presentation and adaptation of content). Such permanent information which exists at the conceptual level also does not belong in the withincomponent layer because that layer deals with implementation-specific elements. Today almost all AEHSs follow the general architecture of the AHAM reference model (but possibly with a different kind of rule or reasoning language and engine). ADAPT  [7]  is an European Community funded project that aims to rectify the situation described in the introduction, by investigating current adaptive practices in various AEH environments and identifying the design patterns within them. The ADAPT project has identified high level design dimensions for AEHSs, which are: (1) context of use (CU), (2) content domain (DM), (3) instructional strategy (IS), (4) instructional view (IV), (5) learner model (LM), (6) adaptation model (AM), and (7) detection mechanism (DE). Thus the prototype involves four main modules: (1) The Domain Model (DM) that represents the domain knowledge of the system. (2) The User Model (UM) that represents the particular user's knowledge of the domain as well as his individual characteristics; both these models comprise the Data Model of the system. 3 The Adaptation Module (AM) which interacts with DM and UM in order to provide adaptive navigation to the course, and (4) The RTE Sequencer that is triggered by course navigating controls and interacts with DM and delivers the appropriate educational content to the learner. The adaptive navigation techniques have been applied to it via Java servlets and Java Server Pages (JSP). These adaptation rules involve the use of Learning Object (LO) metadata, which are independent of any learning style. However, they convey enough information to allow for the adaptation decision making (i.e., they include essential information related to the media type, the level of abstractness, the instructional role, etc.). Next the Web page is composed from the selected and ordered LOs, each with its own status (highlighted, dimmed or standard). [12] proposed a framework for AEHS (  Fig. 4 ) based on JSP, Java servlets, and Javabeans. It enables to recognize the student's learning style automatically in real time by means of Multi Layer Feed-Forward Neural network (MLFF). The course player in ULUL-ILM enables the system to adaptively presents the content with various teaching strategies towards each of student's learning style. AEHS MATHEMA architecture is based on Web technologies similar to client/server distributed computing architecture of MIT and it is mainly supported by Apache Tomcat Server 5.5 and MySQL 6.0 relational database server. For supporting features, it uses the (JSP) pages, Java servlets, JavaBeans, JavaScripts, and Apache Struts technologies. MySQL is the database of choice since there is extensive experience on Campus and provides the ability to implement adaptation rules. This is an interface for Java that standardizes database connections, queries, and results of relational databases. Java servlet technology and JSP pages are server-side technologies that have dominated the server-side Java technology market; they have become the standard way to develop Web applications. A servlet allows a programmer to utilize whatever functions a programmer needs including conditional branching and loops. It is light weight, which means that it does not require a large amount of processing power on creation of the servlet instance, and it accesses databases using JDBC which offers a secure way of accessing many wellestablished database brands. Servlets are capable of eliciting student responses on more than one question and analyzing them to find out the strengths and weaknesses of that student to direct them towards remediation. The benefits of using JavaBeans components to augment JSP pages are the following  [10] : (1) Reusable components: Different applications will be able to reuse the components. (2) Separation of business logic and presentation logic: You can change the way data is displayed without affecting business logic. (3) Protects your intellectual property by keeping source code secure. The Apache Struts is a formalized framework for the architecture style design pattern Model View Controller (MVC). This framework is best used for complex applications where a single request or form submission can result in substantially different-looking results. It also defines the relationships between the concepts (prerequisites) and the level of difficulty of the concepts (see part of them in the  Table 1 ). The characteristics that the system maintains in learner model for each learner are: name, gender, username, password, learning style, level of knowledge for each cognitive goal (general performance), level of knowledge for each basic meaning of the cognitive goal (quantitative and qualitative characterization), concepts that have been successfully evaluated for each cognitive goal, current page, current teaching method, current navigation technique, navigation history, meta-adaptive navigation status, initial Web experience and knowledge level (pre-existing knowledge) in the current cognitive goal, ways of presenting feedback messages, preference for assistance by Advisor in terms of its navigation, course sequence, and phases of the guided dialogues in interactive problem solving process in which each learner participates. The learner during the study selects the cognitive goal he or she wishes to study, following his or her personal course in the educational content, exploits the system's assistance, navigating and studying, is informed about the information that the system maintains, who can change them by intervening and directing the system's adaptation in this way, in terms of curriculum sequencing, adaptive presentation and navigation and adaptive group formation, selects his or her learning style, selects the navigation technique, activates and deactivates the curriculum sequencing, selects the way that the feedback messages are presented, changes his or her knowledge level for each basic concept of the cognitive goal, and activates-deactivates the Advisor. It also takes into account the decisions of the data collection and monitoring unit of the learner's actions. The adaptive navigation techniques that it supports are: direct guidance, link hiding, link annotation, and link sorting. It includes the content of the JSP pages, Java servlets responsible for feedback and creation of guided dialogs in problem-solving, Java servlets for creating evaluation questionnaires, and so on. It is responsible for what will be presented to the learner as a result of the processing of the information that arrives from the adaptation engine, the meta-adaptation engine, adaptive group formation module, and application module. In this module there is the basic servlet, which deals with the presentation of the training material and other JSP pages and servlets, such as the page and the corresponding servlet to inform the learner about the characteristics of all learning styles, the servlet for the curriculum sequencing, the servlet for selecting and presenting the concept that the student selects to study according to his or her learning style, etc. This module is responsible for obtaining information about the data the learner entries in the system and for monitoring his or her actions (interaction with the system). More specifically it: (1) initializes and updates the Student Model; (2) collects and stores the data that the learner answers to open-ended questions, entries in tables, registers in notes, etc; (3) monitors the interactions between the learner and the system to get information about his or her choices in terms of the questionnaires, the phases of guided dialogs, the links, the options for meta-adaptive navigation, the visited pages, and more. Following a study  [13]  conducted on the formation adaptive groups in the MATHEMA, it was decided that: when the system creates a priority list of candidate peers for an interested learner, in the first and second position of the priority list, the algorithm will place the candidate collaborators with a concrete or abstract style and with the same active or reflective dimension of their learning style as follows: If the learner has a concrete learning style, then the algorithm will place the candidate collaborators with a concrete learning style in the first position, and in the second position, the candidate collaborators with an abstract learning style. If the learner has an abstract learning style, then the algorithm will place the candidate collaborators with an abstract learning style in the first position, and in the second position, the candidate collaborators with a concrete learning style. The collaboration protocol that uses the synchronous communication tool is as follows: (1) The learner declares willingness to collaborate either by selecting his or her partner from the priority list of candidate collaborators or by declaring a desire for collaboration so that others who would like to work with him or her can choose it, while activating the synchronous communication tool. The  Figure 11  shows a snapshot of a dialog between the learners Giannis and Mary taking place via the Synchronous Communication Tool (chat-tool). The evaluation of an educational tool is important and it must be ensured that the correct methods and techniques are used. Research has shown that high school students increase their performance by studying through the AEHS MATHEMA  [13] . Generally, the students felt that almost all of the system's functions are quite useful (83.7 to 100 percent) and easy to use (58.1 to 93 percent). Regarding adaptive and intelligent techniques of the AEHS MATHEMA mentioned above, the students having evaluated them consider that the adaptive presentation of different teaching strategies is quite useful (93.0 percent) and fairly easy to use (86.0 percent), the different ways of navigating are quite useful (100.0 percent) and handy (90.7 percent), the meta-adaptive navigation is quite useful (88.4 percent) and fairly easy to use (86.0 percent), the adaptive group formation of collaborative teams to assist them in collaborative problem solving is useful (83.7 percent), but less user-friendly (58.1 percent). Most of AEHSs presented in this paper are based on JSP, Java servlets, and Javabeans technologies to implement their functions, as implemented by AEHS MATHEMA. Furthermore, the presented AEHSs above have at least three main modules, such as Student Model, Domain Model, and Adaptation Model as the AEHS MATHEMA also has. MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and metaadaptive navigation, interactive problem solving, and adaptive group formation. The evaluation of the AEHS MATHEMA was encouraging and rewarding since the evaluators considered that almost all system functions are quite useful and easy to use. Taking into account the observations and recommendations of the evaluators, some system functions, such as this offered by the asynchronous communication tool, have been improved.
paper_21	 The proposed scheme ZSISMP is validated on all BCH codes of known minimum distance. A comparison with several known powerful techniques proves its efficiency in giving more accurate results in short time. The use of this efficient local search had yield to determine the error correcting capability of many BCH codes of length 1023 and 4095. An efficient solution of this problem is the use of error correcting codes. The error-correcting capability of these codes is directly related to their minimum distance. The remainder of this paper is organized as follows: The next section presents the main related works. The section 3 presents the proposed scheme ZSISMP. The section 4 presents the main results. For these codes, only a lower bound is known but the true value is still unknown for large codes. This section summarizes the most important ones. The existence of solutions to this system is a necessary condition to the existence of codewords of weight w in the code. The use of this method has finished the table of BCH codes of length 255. [15] . The artificial intelligence Simulated Annealing presented in  [16] , Tabu Search  [17] , Hill-Climbing  [18] , Genetic Algorithm  [18] [19] , Ant Colony Optimization  [20] , Metropolis Algorithm  [21] , was shown to be useful to attack the difficulty of the minimum distance search problem for BCH Codes. This section presents the proposed scheme for finding the lowest weight in BCH codes. For finding the minimum distance of BCH codes. This section presents also a comparison between the proposed scheme and previous work on minimum distance for BCH codes. All results have been done using a simple configuration machine: Intel (R) Core (TM) i3-4005U CPU @1.70GHz RAM 4GO and are made by running the considered algorithm in 1day for each code. The table 3 presents a comparison between the proposed scheme and MIM-RSC method  [24] . It shows that the proposed scheme greatly passes the MIM-RSC method.
paper_31	 Through the ZigBee wireless sensor network, this system sends the physiological parameters collected by various medical sensors to the intelligent medical system, and innovatively proposes semantic matching algorithm to solve the queuing problem of data transmission, to ensure the accuracy of data transmission. This system employs the improved spatial vector model to process the data uploaded, and uses AES encryption algorithm in the process of data transmission to ensure the security of data transmission. This passive recording method greatly raises the economic cost and time cost of patients with chronic diseases. In-depth interviews with 91 employees in different positions from 11 grassroots medical and health institutions since July 2019 show that the efficiency of mobile medical care in the surveyed areas has appeared  [2] . And some scholars combined Bluetooth technology with GSM short message to realize the collection and remote monitoring of physiological parameters such as blood pressure  [12]  and pulse. Traditional phone calls were adopted as the main way of communication between patients and doctors. This is the second stage of the development of WITMED. Most data were transferred by satellite and ISDN (integrated service digital network). This is the third stage of the development of WITMED. Energy-efficient Zigbee-based wireless sensor network (WSN) occupies a major role in emergency-based applications  [6] . Kumar establishes an enhanced shortcut tree routing-based geographic location (ESTRBGL) protocol  [7] . Haji Bagheri Fard present methods based on deep learning algorithms to achieve high classification accuracy  [8] . Zigbee has been implemented on the Health Care Profile. There are some defects of the current WITMED (Wise Information Technology of med) system as follows: (1) The inadequate reliability of data transmission. (2) The problem of power consumption. (3) The problems of unitary monitoring data. (4) The problem of data processing. The current WITMED system focuses only on data acquisition instead of processing uploaded data and returning the result. The growing trend of WITMED system shows that a variety of health signs data will be monitored, transmitted, processed and analyzed in real time, the system can process the uploaded data. The traditional prevention and treatment greatly increase the economic cost and time cost of patients. Alibaba founded Alibaba health  [15] ; Tencent launched the first AI+ medical product Tencent Meiying in 2017; many large enterprises integrated medical resources through mergers and acquisitions to lay out the intelligent medical industry chain. Then the coordinator transmits the data collected to the intelligent medical system. The process of reading and transmitting data is a loop. The data acquisition structure charts  After receiving the relevant data, the data analysis platform needs to analyze the corresponding data and feedback the processed results to relevant users. The related knowledge base in  Figure 3  can be regarded as an expert system. So the key to this research will be how to keep the integrity and effectiveness of each node's information and then how to process it. The patients' vital signs data will be transferred into the data analysis system by the sensor, and then the data will be analyzed by the big data system, various signs data will be compared with the characteristic values of related diseases and the results will be returned. Therefore, it is crucial to guarantee the security of data transmission in the process of information transmission. This system adopts AES (Advanced Encryption Standard) encryption algorithm which is widely used at present. AES encryption process is shown in  Figure 4 : (3) column Mixed Operation The column mixed transformation is realized by matrix multiplication. The state matrix after row shift is multiplied by the fixed matrix to obtain the confused state matrix. The column mixed operation formula is shown as  Figure 6 : (4) key plus calculation Key plus calculation operates the 128 bit key is XOR bit by bit with the data in the state matrix. The process of key plus calculation can be regarded as the result of Bitwise XOR or byte level or bit level operation. The calculation is shown as  Figure 7 :   After logging in the system, enter the sign input interface and put on the sensor, the system will automatically input the relevant data collected, as shown in  Figure 8 : After inputting the data of relevant signs, the system will automatically generate diagnosis & treatment files and health assessment according to the relevant data. Data acquisition in  Figure 8  forms the health records in  Figure 9 . Through the big data platform, the system intuitively displays the backstage analysis data in the form of reports and trend charts as shown in  Figure 9 . In this study, a design scheme of intelligent medical system based on semantic matching is proposed, with the front-end data acquisition which adopts ZigBee technology, and a semantic matching algorithm is innovatively proposed to solve the queuing problem of data transmission to ensure the accuracy of data transmission. Through ZigBee wireless sensor network, the system sends the physical parameters collected by various medical sensors to the information control terminal, and the improved spatial vector model is employed to process the uploaded data in the process of data transmission by the intelligent data analysis platform, and the security of data transmission is ensured by adopting the AES encryption algorithm in the whole process. The intelligent system realizes data acquisition, data encryption and processing, and big data analysis. The semantic matching algorithm and the space vector model algorithm proposed by the system can be widely used in the data acquisition of clinical big data to provide theoretical and technical support for artificial intelligence to assist disease risk prediction. The intelligent system realizes the data collection of the whole process of outpatient service, examination and treatment, which can provide data support for the precise treatment of diseases.
paper_38	 The study adopted the descriptive research design. Pearson Product Moment Correlation (PPMC) Coefficient and Multinomial Logistics Regression (MLR) were the statistics used to answer the four research questions used. It concluded that the use of OL and UTME as instruments is not enough to select candidates for admission and therefore recommended that other instruments such as senior secondary school mock examinations need to be included as part of the entry requirements in the admission criteria. It is a progressive development of knowledge and skills of examinees through stages of teaching and learning at various levels  [1] . The number of undergraduate population in Nigerian Universities has increased from 103 in 1948 to an estimated population of 600,000 in 2018  [4] . There have been various perspectives presented by researchers and psychologists about what is academic performance and its importance. In Nigerian universities, an academic performance frequently is defined in connection with semester examination performance. In this study, the academic performance is categorised by the entire performance each year, which culminates in a Cumulative Grade Point Average (CGPA). Formula 1 is used for calculating the CGPA. As such, the CGPA is considered to be a good predictor indicator of a student's academic performance. The objectives of this study are to: i. Investigate the relationship between the students' performance of their entry qualification and the academic performance in the 100 level CGPA examinations. The following research questions directed the study: 1. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? 2. How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? 3. What is the relationship between OL results, UTME scores and Post-UTME scores of students and their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? 4. How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? The numerous ways of reporting academic performance include raw scores, percentages, transformed scores, or even as categorical variables such as Excellent, Merit, Very Good, Pass, First Class, Distinction, A1, B2, C4, F9, and others. Students' academic performance which is a function of many variables, could be classified as a student, home, school, teacher, cultural and legal factors  [8] . The underlying assumption made in such selection is that those admitted by satisfying the admission criteria will be successful in the successive academic activities attached to their studies. Several studies have criticised the use of UTME and PUTME as an imperfect instrument for predicting academic performance of students. Wide disparities have cited between UTME and PUTME scores and the progress/performance of students especially those with exceptionally high UTME scores. A sample of 214 students records was used for data collection. The authors tested their nine hypotheses using an independent samples t-test and two-way analysis of variance. Graduates with the best academic performance from the preliminary programmes were those admitted through the certificate programme. The author's study found that the use UTME score was a very poor predictor of students' final grades and thereby recommended that less emphasis should be placed on UTME scores as a criterion for admission of candidates into universities. The study by  [13]  found a significant relationship between students' scores in three examinations, namely: UTME, PUTME and 100-Level Psychology course, Faculty of Agriculture, Federal University of Agriculture, Makurdi, and thus concluded that the UTME has predictive validity for performance in the university. The article recommended that JAMB should embark on a more realistic review of the content of the UTME to enhance its predictive validity. The sample population consisted of students who were admitted during the 2010/2011 academic sessions but have graduated at the end of the 2012/2013 academic session. The author concluded that the entry qualification or the entrance examination performance could not individually predict Mathematics performance at the CoE. The author concluded from his findings that the use of PUTME is beneficial for selection of candidates for admission and also that candidates who had a high-performance level in the UTME have a positive effect on the academic performance in the university. This section discussed in detail the methodology employed such as research design, sample of study, instruments used, the procedure for the collection of data and data analysis. He explained that there are two types of ex-post facto research designs namely the correlational and the casual comparative. Full-time degree programmes: Biochemistry, Biological Sciences, Chemistry, Computer Science, Geography, Industrial Chemistry, Microbiology, Mathematical Sciences, and Physics. The instrument used to derive the data for this study were: the JAMB UTME scores from 2010 to 2014 and OL grades in the five relevant subjects which were the pre-admission criteria and first-year CGPA results obtained from the semester examinations from 2010/2011 to 2014/2015 academic sessions. It was used in this research study. The results of the data analysis are presented in tables according to the research questions that guided the study in this section. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? In  Table 6 , the likelihood ratio Chi-Square of 18.723, 17.661 and 12.401 for Computer Science, Mathematics and Physics programmes with a significant value of 0.227, 0.281 and 0.414 tells us that the model as a whole does not predict the dependent variable, i.e., CGPA. Among the classification of degrees, each of the five subgroups for each programme, that is Pass, 3 rd Class, 2 nd Class Lower, 2 nd Class Upper and 1 st Class, are contrasted with the baseline group of 'fail' degree. These showed that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. Besides, the slopes (B) of UTME in the CGPA categories of 'Pass' and '1 st Class' are positive while the rest are negative. For Mathematics and Physics students the relative strength of OL, UTME and PUTME on CGPA performance are statistically insignificant except for the slope (B) of OL in the CGPA category of '1 st Class' for Mathematics students, which statistically significant. What is the relationship between OL results, UTME scores and Post-UTME scores of students and their firstyear CGPA across the academic session, 2010/2011 to 2014/2015? PPMC was used to analyse the data for this research question. The summary of  Table 8  shows the correlations of OL and CGPA scores, UTME and CGPA scores and PUTME and CGPA scores for all the programmes used for the analysis. How well do OL results, UTME scores and Post-UTME Sa'adatu Abdulkadir and Francisca Nonyelum Ogwueleka: Predicting Students' First-Year Academic Performance Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria scores of students predict their first-year CGPA across the academic session, 2010/2011 to 2014/2015? As shown in the Likelihood Ratio Test results in  Table 9 , the likelihood ratio Chi-Square of 37.446, 19.938, 46.141, 14.349 and 11.167 for 2010/2011, 2011/2012, 2012/2013, 2013/2014 and 2014/2015 academic sessions which has the following as significant values of 0.001, 0.174,.000, 0.499 and 0.741 tells us that the model for students admitted during the 2010/2011 and 2012/2013 academic sessions predicts the CGPA, which is the dependent variable while the other academic sessions does not. These show that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. Furthermore, the slopes (B) of UTME in the CGPA categories of the '3 rd Class' is negatively signifying that the relative strength of UTME is lower than those with a CGPA category of 'Fail' and the rest are positive which signifies otherwise. Conclusively, the slope (B) of PUTME in the CGPA category of '3 rd Class' is positive and the rest negative. This indicates that the relative strength of UTME is higher than those with a CGPA category of 'Fail' and the rest of the CGPA categories are negative which indicates otherwise. The relative strength of OL, UTME and PUTME on CGPA performance of students admitted in the 2010/2011 session is not statistically significant except for the slope (B) of OL in the CGPA category of '2 nd Class Upper', which is statistically significant. As for the students admitted in the other sessions, 2011/2012 to 2014/2015, the relative strength of OL, UTME and PUTME on CGPA performance are statistically insignificant except for the slope (B) of UTME in the CGPA category of '2 nd Class Upper' and '1 st Class' for those admitted in the 2012/2013 academic session, which is statistically significant. Based on the analysis and results using MLR and PPMC for each programme and each academic session, it is evident that OL, UTME or PUTME could not individually significantly predict the academic performance of students in Faculty of Science. However, by combining all the criterion variables, that is OL, UTME, and PUTME, as one variable and performing the PPMC and MLR, findings show that OL is a good predictor on the dependent variable for academic performance with a weak correlation of 0.068 which is statistically significant at 0.04 level. This predictor holds true especially for students who are in the CGPA category of '2 nd Class Lower' and '1 st Class' respectively.
paper_57	 Crossover and mutation operations have been adapted for use with ant generated strings which still have to provide feasible solutions. During the search process each ant sets off from ant colony (start position) and moves to search food (destination). On their way back they use the same way from which abundant loops has been removed, but the amount of pheromone (1) ∆τ k ij (t) they produced is inversely proportional to the tour length L k (t). The probability p ij (t) of choosing the particular arc (i, j) depends on pheromone τ ij  (t)  and the heuristic η ij values which are associated with the arc (4). formula_4 Symbols α and β are weight parameters and represents balance between ant's gathered knowledge and the user preferred area. Disadvantages of ACO algorithms are (i) many user parameters and (ii) the selection pressure. The mechanism for prevention of quick convergence (i) is based on pseudo-random proportional rule  [3] , but the tunable parameter q 0 is dependent on algorithm iteration  (6)   where NC is the current iteration and N max is the termination iteration. The stagnation avoidance mechanism (ii) is based on the comparison of a randomly generated quantity q ∈ (0,1) with probability p k ij (t) of selected arc. This occurs in later stages of the search process, where pheromone values tend to be high, and thus the chance of further exploration is low. Genetic algorithms (GA) were proposed by  Holland (1975) . The four main components of GA are representation (i), mutation (ii), crossover (iii) and selection (vi) mechanism. 1 . 2) . 3 ). Since optimization process is primary done by ants cooperative behavior, the selection process has purely random concept and genetic operations serve just for selection pressure decrease. Prior the genetic operations all loops are removed from the tours. After all mutation operations are performed, crossover operations are applied. Since genetic operations may produce strings with loops, in ACO framework prior and immediately after each genetic operation a loop removal procedure is performed. After all genetic operations are executed fitness evaluation and pheromone update are scheduled. Genetic operations do not have to be necessarily feasible. Feasibility of genetic operations depends on the graph and generated tours. For this purpose ACO GO algorithm has embedded user feedback which represents a ratio between accomplished and required genetic operations. 4) . Node coordinates x,y fall in range <0,1> and arc's values c ij represent the arc lengths. The task is to find the shortest path between start node n s = 1 and end node n e = 80. Variable parameters were set to determine the influence of the genetic operations quantity on algorithm performance and effect of distribution of mutation operations between paths. Their color map was set to show green -blue when the results are worse than reference value and yellow to red otherwise (  Fig. 5 -7) . 5 ). 7) . I.e. with no crossover pair certain amount of mutation operations should be spread out among more paths, but with 2 crossover pairs concentration of mutation operation on less paths tends to perform better. It does not have the highest value on edges of the surface where the highest amount mutation operation is. Genetic operations where nearly always feasible; ratio accomplished / required mutation operations is 100% and for the crossover operation over 99%. In order to determine the effect of crossover operation crossover rate was let to grow up to 100%  (Table 3) . 8) ; the highest output was gained for 60% of crossover rate. 9 ). This is caused by the search space dimension. It has been proved that genetic operations increase ACO algorithm performance. Limit of crossover is 60% of crossover rate. Mutation operation causes better results than crossover operation. This can be explained by the nature of the mutation operation which creates new paths whilst crossover operation can only combine already existing solutions. The higher amount of mutation operations the higher the performance gain is. No limit for amount of mutation operation was found during the simulation. Without crossover operation distributed mutation operation has better performance, but with two crossover pairs concentrated mutation operation on less paths tends to perform better. Further research and more experiments are needed to determine the distribution and optimal amount of mutation operation with respect to the number of ants and length of the paths.
paper_78	 Glycyrrhiza uralensis is an endangered medicinal plant species and mainly distributed in North China. These formations were main communities dominated by Glycyrrhiza uralensis in North China. Fuzzy C-means clustering is based on fuzzy set theory which has been developed by numerous scientists, and is now applied in various fields of sciences  [2, 3] . Like licorice, most medicinal plants are harvested in the wild and the extended use has led to some medicinal species endangered and tending to be extinct, and the protection of such medicinal species has been urgent  [12, 13] . Fuzzy C-means clustering is a soft classification technique  [5, 7]  in which a plot can belong to more than one cluster, and associated with each plot is a set of membership levels. The Fuzzy C-means clustering attempts to divide a finite collection of n elements (plots) formula_0 into a collection of C fuzzy clusters with respect to some given criterion. Given a finite set of data matrix (species × plots), the method calculates a list of C cluster centres  formula_1 formula_2 Where i = 1, 2, …, N = the number of plots; j = 1, 2, …, C = the number of clusters; U = {U ij } = the matrix of membership values, U ij is the membership of plot i in cluster j; V is a matrix of cluster centers; m is fuzzifier which determines the level of cluster fuzziness (1 ≤ m < ∞). (dA ij ) 2 is the distance index: formula_3 X i is the vector of attribute measurements in plots, usually a vector of ordination scores; V j is the centre of cluster j, if A is a unit matrix, then formula_4 Based on the algorithm above, the procedure of fuzzy C-means clustering is as follows: (1) Selecting a common ordination method, we use Detrended Correspondence Analysis (DCA), and do ordination analysis for species data. For a plot, any value can be given as its membership value in cluster j, but the sum of memberships for a plot must be equal to 1:  2  is calculated using equations (2) and (3). formula_5 formula_7 Based on the new membership values U, we go back to the fourth step and calculated the next turn V j , (dA ij )  2  and U ij iteratively, and until the membership values become approximately stable. Form. Glycyrrhiza uralensis + Stipa bungeana. Its disturbance intensity is medium and heavy. The community has a total cover of 75%, a shrub layer cover of 20% and an herb layer cover of 65%. Form. Glycyrrhiza uralensis + Polygonum bistorta. Its disturbance intensity is medium and heavy. The community has a total cover of 70%, a shrub layer cover of 5% and an herb layer cover of 70%. VII. Form. Glycyrrhiza uralensis + Ephedra przewalskii + Cancrinia discoidea. Its disturbance intensity is medium and heavy. The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 60%. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 59500 ha -1 . VIII. Form. Glycyrrhiza uralensis + Artemisia frigida. Its disturbance intensity is medium and heavy. The community has a total cover of 65%, a shrub layer cover of 10% and an herb layer cover of 60%. IX. Form. Glycyrrhiza uralensis + Carex pediformis +Stipa sareptana. Its disturbance intensity is heavy. The community has a total cover of 60%, a shrub layer cover of 5% and an herb layer cover of 58%. X. Form. Glycyrrhiza uralensis + Astragalinae triloa + Stipa sareptana. Its disturbance intensity is heavy. The community has a total cover of 75%, a shrub layer cover of 10% and an herb layer cover of 70%. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 6000 ha -1 . XI. Form. Glycyrrhiza uralensis + Aneurolepidium chinense +Stipa sareptana. Its disturbance intensity is heavy. The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 65%. XII. Form. Glycyrrhiza uralensis + Festuca logae + Stipa sareptana. Its disturbance intensity is heavy. The community has a total cover of 80%, a shrub layer cover of 10% and an herb layer cover of 75%. Theoretically, fuzzy C-means clustering is the only one soft clustering method and can deal with much imprecise and incomplete information and has advantages in solving non-linear problems and in studying complex system  [2, 17] . These communities are important not only for protection of medicinal plant species but also for conservation of ecosystems and their environments in semi-arid and arid regions in North China  [23, 24] .
paper_96	 This humane technology innovation is the specific embodiment of environmental science and technology aesthetic theory in the field of scientific and technological innovation. It has a positive and promoting role to the development of transportation and blind-man welfare in China. During last summer, I went blind orphanage as a volunteer, and personally experienced the inconvenience and hardship of the blind when they travel. I think that if electronic guide dogs guide them to avoid obstacles such as vehicles, the blind people are able to avoid such accidents, aren't they? The research on "warning system of embedded electronic guide dog avoiding obstacles" in this paper is an important part of intelligent electronic guide dog system, the study of which is mainly based on the embedded system, anti-collision warning system, and the USB technology is applied to warning system of embedded electronic guide dog avoiding obstacles. The design of obstacle-avoiding early warning system of embedded electronic guide dog is an inheritance and innovation, based on the design of traffic information collecting system, which is generally used in vehicle anti-collision at present. Nowadays, the main types of the design of traffic information collecting system, generally used in vehicle anti-collision, are shown as follows, that is, using single chip microcomputer for data analysis, PC-centric (industrial control computer), using DSP for data analysis and so on. In this way, the design of obstacle-avoiding early warning system of embedded electronic guide dog has the following advantages. Sound will produce auditory stimulus for the blind. The hardware of the obstacle-avoiding early warning system of embedded electronic guide dog can reference the Figure 1:  Figure 1 : Schematic diagram of hardware system structure  [4]   In the anti-collision early warning system, referenced in the design of obstacle-avoiding early warning system of embedded electronic guide dog, there mainly exits four steps, that is, information collection, information processing, information judgment, and warning information. Information processing is mainly to analyze the collected information, usually using ARM, MCU and DSP microprocessor, etc. [5]  And information judgment is based on the information, like the model of the guide dog, opportunity, weather, the distance between the guide dog and the obstacle, relative acceleration, relative speed and so on. Presently, the types of the generally used anti-collision warning systems mainly are radar anti-collision warning system, ultrasonic anti-collision warning system, laser anti-collision warning system, infrared anti-collision warning system, machine vision anti-collision warning system, and interactive intelligent anti-collision warning system. The design of obstacle-avoiding early warning system of embedded electronic guide dog also adopts the generally used embedded processor. Presently, the generally used embedded processors mainly are ARM, Power PC, MIPS, Motorola 68000 series, etc. The full name of ARM is Advanced RISC Machines. The ARM architecture follows the principle of reduced instruction set computer (RISC). It has a variety of merits, like small size, high performance, low power consumption, and cheap cost. It extensively uses the registers with a fast speed of instruction execution. In this way, the embedded operating system in embedded system is more like a set of function library. Besides, its other main features are universal Linux API, core file less than 512KB, core+file system less than 900KB, and complete TCP/IP. Also, it supports a large number of other network protocols and various file systems, including NFS, EXT2, ROMFS, JFFS, ms-dos, FAT16/32, etc. In addition, the design of obstacle-avoiding early warning system of embedded electronic guide dog also makes a full use of USB. The traffic safety problem of the blind is an urgent issue to solve. This humanized technology innovation is the embodiment of environmental science and technology aesthetics theory in the field of science and technology innovation practice.
paper_134	 New information and communication technologies, as well as decision support technologies, can be very effective in providing timely, accurate, and relevant information to users by collecting, storing, evaluating, interpreting, analyzing, retrieving and disseminating information to specific users  [1] . Data mining simultaneously utilizes several disciplines such as artificial intelligence, machine learning, neural networks, statistics, pattern recognition, and science-based systems. Dielectric properties are one of the most important physical properties of agricultural and food products. When in a country where milk is eaten up by 30 percent as a result of a rise in the price of a few hundred tons, what else would you expect from a 60 to 80 thousand Riels per liter consumption of lubricants  [9] ? The great benefit of mixing vegetable oils with lubricant makes it hard to find the type of product. Knowing the original oil is also not an easy task that anyone can handle. The second pertain to data mining algorithms; third part related to samples and used methods in the article. Lizhi et al. From the result of this study it can be seen that the dielectric spectrum can be used to detect fake oils with different types of oils with a percentage of their mixture below 5%  [12] . Soltani et al (2010) used a non-destructive capacitive method to detect the amount of banana fruit juice. The results of the prediction were satisfactory and the correlation coefficient between the actual yield and the expected value of the fruit was 0.85  [13] . Reggie et al. (2006) predicted egg quality parameters using its capacitive properties. They used 4-volt sin voltage with a frequency of 120 MHz and a rectangular parallel capacitor, which obtained capacitive capacitors (DCs) and voltages (DVs). Experiments were carried out on the day of laying, on the third day, on the sixth day, on the ninth and twelfth days after the laying. This parameter had a good relationship with all the quality properties of eggs (elevation of the cell, height of albumin, etc.). Soltani et al. (2015) reported a research about egg grading using image and sensor processing. They used a double-layer neural network with two inputs (large diameter and small diameter), a hidden slab and an outlet (egg volume) to predict the volume of eggs. Then different classification algorithms by MATLAB software and various techniques such as support vector regression were done and finally output dates were processed. Samples of olive oil provided from Khorramshahr Oil Company and produced at Rudbar oil plant located in Manjil. The microcontroller is an electronic application chip that can increase the speed and efficiency of the circuit versus reducing the volume and cost of the circuit. Arduino has a variety of ports, controls, side panels, and screen connectivity, while it may be smaller, less expensive, and possibly faster than a microcomputer system. The AD8302 phase detection chip provides a simple method for measuring the input ratio of the domain and the phase difference of the two signals simultaneously (22). (Figure 1 .) Support Vector Machine Regression (SVR) aims at finding a linear hyper plane, which fits the multidimensional input vectors to output values. One of the new ideas in identifying and categorizing patterns is a backup vector machine or SVM. Using this technique, the amount of RMSE for the samples were equal to 3.56, 2.45, 3.05 and the amount R 2 was equal to 0.92, 96 and 94, respectively. According to the results, Most R 2 related to Olive-Canola (0.90) and the least amount of R 2 pertain to Olive-Sunflower (0.86). Also, in regard to the amount of RMSE, result has shown 4.81, 2.54, and 4.38 for Olive-Sunflower, Olive-Canola and Olive-corn, respectively. The results were predicted and modeled using regression methods. In this article two factors (gain voltage and phase shift voltage) were measured. Figure 2  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn Using this technique, the amount of RMSE for the samples were equal to 3.65, 2.46, 3.31 and the amount R 2 was equal to 0.92, 96 and 93, respectively. Also device used can classify samples and use for other oils.
paper_139	 We introduce the correct graph theory based KTMIN-JAK-MAXAM algorithm filters out the redundant link. Web mining is categorized into three group Web Content mining  [6] , Web structure mining, Web usage mining. Traditional web mining algorithms handle with structured document  [7] [8] [9] [10] [11] [12] [13] [14] [15]  than the advanced methodology of mining algorithm can dealthe entire heterogeneous document comprises of images  [9] , graphs, videos  [16] , etc. Once the input query is requested, the search engine generate the document with multiple web pages along with the links, the user will be unaware about the content of the web pages, the extracted web documents contain multiple web pages either be redundant or not. Update 'u' to U. Step3C: Update the value of degree for all adjacent vertices of 'u'. Iterate throughall adjacent vertices if possible. So pick any one of the node among the 3 adjacent nodes. 4.2 Search the node B, the unvisited adjacent node is from B as 2 and 7. Now the set U consists of the nodes A, B, 2. 4.3 Now travel around the node 2, the unvisited adjacent node is from 2 as 1 and 4. After inclusion of the node 1 the set U consists of the nodes A, B, 2, 1. 4.4 Discover the node 1, the unvisited adjacent node is from 1 as only 3. Now the set U consists of the nodes A, B, 2, 1, 3. After Applying The Proposed KTMIN-JAK-MAXAM ALGORITHM To G 2 , we get  After applying the proposed KTMIN-JAK-MAXAM ALGORITHM to the above graph G 3 Here notice that, Regular connected network G 3 , after applying the proposed KTMIN-JAK-MAXAM ALGORITHM to G 3 , we get path of length 9 in figure 7. In search engine generate relevant information but most of the time the information is not redundant. We propose a graph theoretical based algorithm for detecting and eliminating redundant links. Also we observed derived linked graph need not be unique but this approach will provide the optimized cost analysis report in future in data science field. Future work aims to create a finite automata tool to produce only relevant and without redundant information of web documents in data mining.
paper_145	 Among them 44.3 percent were overweight and obese. With the increase in age significant increase in prevalence rate of obesity was observed. Higher prevalence of obesity was noted among females. The prevalence of overweight and obesity has increased rapidly over the last decades especially in developed countries  [1] [2] [3] . In 2016 WHO estimated that globally approximately 1.9 billion adults (age > 20 years) were overweight and more than 650 million adults were obese  [4] . The major contribution of obesity is to lead the increase in the prevalence of chronic diseases and cancers  [7] [8] [9] [10] . It had been observed in some research findings that youth who do not meet guidelines for dietary behavior, physical activity and sedentary behavior have greater insulin resistance than those who do meet guideline  [12] . For this reasons, World Health Organization considers the epidemic a worldwide problem which requires public health intervention  [13]  that act on different factors associated with overweight and obesity as well as technological changes that have lowered the cost of living of the people so that people can avail sufficient food with required protein. Efforts are needed to improve the economic, political, social and environmental conditions so that congenial atmosphere prevails in the society for maintaining healthy life of the people. Even government and public health planners remain largely unaware of the current prevalence of obesity which is the cause of many diseases  [5] . The information regarding blood sugar level and blood pressure level were also noted down according to the latest measurement by doctors/diagnostic centers. All variables were transformed to nominal form by assigning numbers to do the factor analysis. The analysis was done by using SPSS [version 20.0]. The level of obesity was measured by BMI [weight in kg /(height in m)  2  ] and it is a most commonly used measure of level of obesity  [18] . The respondents were classified as underweight  [BMI < 20] , normal [BMI,  20 -25] , overweight [BMI < 30] and obese [BMI 30+]. Factor analysis was done to identify the factors for obesity. Besides the factor analysis, the association of different socioeconomic variables with level of obesity were investigated. Significant association was decided by chi-square test with p-value< 0.05 and odd ratio was calculated for respondents who were overweight and obese compared to normal group. Obesity was observed among 15.3 percent people and overweight respondents were 34.0 percent. In each level of obesity the majority were from urban area. However, urban people were more exposed to overweight and obesity by thirty two percent compared to rural people [O. R.= 1.32]. There were 58.9 percent males among the respondents and 47.2 percent of them were normal. The corresponding figure among females were 37.3 percent. They were in more risk of overweight and obesity by 51 percent compared to males [O. R.= 1.51] Obesity and severe obesity were observed almost similar among Muslims and Non-Muslims [  Table 3 ]. But more Muslim respondents (43.8%) were normal compared to Non-Muslim respondents (38.8%). Among the investigated respondents 92.6 percent were currently married and 43.1 percent of them were normal  [Table  4 ]. Similar normal group was noted among the other group of respondents. The present study also indicated similar result [  Table 11 ]. Among the smokers 47.2 percent were normal and 37.2 percent were overweight. Table 11  Among the smokers 47.2 percent were normal and 37.2 percent were overweight. The association between smoking habit and level of obesity was significant [P(χ2 ≥ 20.189) = 0.0.002] and smokers were 21 percent more exposed to overweight and obesity compared to non-smokers [O. R. = 1.21]. This was done by factor analysis. The significant multiple regression analysis using one of the included variable as dependent variable and others as explanatory variables also justified the inclusion of the variables for factor analysis. From the analytical results it was seen that all the p-values were less than 0.05 which indicated that the inclusion of variables for factor analysis were justified. From the results of the communality it could be concluded that the variable marital status was more important followed by gender and education. From the results of the communality of the variables it could be concluded that 85.6 percent variation of the variable marital status would be explained by the extracted factors  [22] . These three variables were more important for the variation in the level of obesity. These coefficients were observed from the first component. This component explained 25.733 percent variation in the data of obesity. This component explained 10.86 percent variation of the obesity. Around 50 percent respondents were overweight and obese. Similar finding was also noted in another study  [21] . The prevalence of overweight and obesity were significantly associated with age, religion, education, occupation, marital status, income and smoking habit. Around 50.6 percent people of urban area were overweight and obese. This result was also similar as was observed in another study  [21] . Most the respondents were in normal and overweight groups. [3] . Among the respondents 84.1 percent were of the age 40 years and above and among them 42.8 percent were overweight and obese. This finding is similar to that observed in both home and abroad  [23] [24] [25] . The following actions are very important to reduce the prevalence of obesity. These are: a) Halt the rise in body weight by encouraging people so that they can take healthy home made food and avoid restaurant food / first food.
paper_212	 Mathematical modeling of disease has been an indispensable tool in accounting for disease transmission dynamics as well as disease spread. The models developed however faced considerable challenges ranging from inaccurate representation of natural data for deterministic models, to methods of forecasting such as statistical extrapolation which assumes that current conditions will prevail which is not always the case. This algorithm is able to give a statistically correct of the course of a disease with initial conditions to begin with and propensity functions to update the system. We conclude that the simulated model reflects reality. There are several channels in which the virus can be transmitted such as inter-species transmission, vector transmission, direct transmission or environmental transmission. In 1984, several HIV and AIDS cases were documented in Kenya. 170 of 1999. Mathematical modeling of HIV is the use of statistical tools and procedures to recognize the general pattern in the transmission of HIV and to translate a problem into a statistical form for subsequent analysis. This study employs mathematical modeling tools in the transmission probability of HIV and analyses done on how the cumulative number of infected individuals responds as well as the AIDS death probability and how the cumulative cases of removed individuals responds to this probability. In this model, there were no forms of intervention. The transmission parameter was held constant for all stages of HIV. The formulated model was used to forecast the number of PLWHA. There is need now more than ever to develop the SIR model since its application is going beyond epidemiological application such as how cues influence behaviour in a social setting and the spread of ideas  [5] . In their works they build a disease spread prediction model based on the SIR model and applied parameter values to a stochastic model based on Gillespie's algorithm. This study seeks to incorporate a stochastic aspect in the deterministic SIR epidemiological model. The transmission and infection rates were considered to be variant. The initial conditions changed over time and demographics not being included such that change over time was described as; (1) formula_0 The Kermack-McKendrick theory was later developed to a version where they tackled the problem of endemics  [14] ,  [15] . denotes the birth rate and death rate denotes the infection rate denotes the recovery rate t denotes time point Model development The Gillespie algorithm was used to simulate a statistically correct trajectory given initial SIR conditions. The model explored how altering transmission dynamics affected the model as a whole. The death rates were distinguished such that one death event led an individual out of the model while the other death event led an individual into a different classes. The SIR model explained how the epidemic manifests in all the compartments. The stochastic SIR model. , the state change vector defined as , , where is the change in state i caused by one event. Assuming that the resulting state is . A propensity function is the probability of one event occurring in the time interval ! , ". A Markov chain model is one where the probability of the next event depends on the probability of the present state. The wait times between events can either assume an exponentially distributed wait time or the rate of flow between compartments can assume any of the following distributions depending on the results. A stochastic SIR model was simulated with a mean step size of 0.006336446. 537 tau steps were made in the model. Variables in the model were S = 3507162, I = 45820, R = 4597, parameters in the model are crude birth rate of 0.06, non-AIDS death rate of 0.025, transition rate of 0.1 and AIDS death rate of 0.48. Curves produced are illustrated below. The critical value was 47.4. Therefore, the conclusion is that the simulated data model fits the natural data model.
paper_214	 To create simulation envolves changing strategic economical indicators and keeping restrains on others so they reflect reality and the business environment. Solving the problem is to build decision tables or decision trees, from which it selects the best alternative. Tabelelele decision highlights a possible alternative schematic characteristic information. Decision trees, in addition to decision tables, chart highlights the relationship between the variables of the problem, making it possible representation of complex situations. Apply mathematical programming problems which lead to the formalization of a mathematical relationship between decision variables and purpose. It is the only method that can be applied to unstructured problems. Such testing may be actions that can be made explicit in the framework model; enable better decision-making structure problem, allowing exploration of information flows and operational procedures without interfering with the actual operation of the system; using cybernetic control system, which underlies decision making in practice; There are a large number of parcel simulation program. These are translated into algorithms which are executed by a computer system. This led to consideration of simulation as one of the most powerful tools in decision making. The solution offered is one spot that has no counterpart in the real system. Decision making process, conducted with the help of tools, methods and techniques, conduct to the scenarios constructed according to a definite objective. The activity of the coordinations of inputs is done in most cases with specialized software systems that create analytical databases or modeling languages. Evaluation of search results depends on the method of presenting results and depends on the facilities of component dialog with users that provide inputs. In complex cases, the problem breaks down into subproblems more manageable, easier structured. Modeling takes expression of reality by means of abstract entities possessing quantitative and qualitative attributes. The choice of solution is closely linked to proper evaluation of the results of said solution. The assessment in turn depends on the search method. Structural problems use mathematical formulas and analytical method to achieve an optimum solution. For complex problems, solving is carried progressing from one situation to another, until a final statement, which is the solution. Basically successive tests are performed, the search progressing from a solution to another. Assisting decision states that the decision is the responsibility of the user. Basically, it helps the decision maker during operation and defining the problem, generating satisfactory solutions and retention strategy. In current systems, the company's intranet, are increasingly present data accessible through web browsers and multimedia items such as maps, images, sounds. The data source, internal or external, data is extracted and managed by a management database. It is the component that differentiates interactive decision support systems to other systems. The models are domain-specific and models can be classified into strategic, tactical and operational models models. Database management system allows creating new models models using programming languages, update and modify existing models, establish interdependencies between models. Extract information in order to obtain information for decision making. Facts associated table. The advantages of using databases for Decision Support Systems in businesses involves creating numerous queries by the decision maker and also allow to create many scenarious and variants to choose from. In the business area is crucial to store data into a warehouse such as a star modell and create many queries and reports so the decision maker can see the trends of the economical indicators and helps him to make a decision that can improve those numbers or it can be useful to implement a strategy for a further development  [1, 3] . The databases permits flexibility in creating queries by choosing columns from dimension tables and measures from the fact table and so the decision maker may choose what economical indicators is useful to analyse and also permits to create different types of graphs such as columns, pies, scatter points or webs. By simply analysing these reports and graphs helps the decision maker to create different scenarious by changing target indicators and corelate with the economical indicators that can't be changed  [2, 5] . The methods specific to the databases such as SQL language and the Business Intelligence tools allow businesses to explore data and to create alternatives that helps to choose the optimal variant according to the economical restrictions that came from the business environment.
paper_216	 The Black-Scholes model is a well-known model for hedging and pricing derivative securities. Amongst the non-parametric approaches used to improve the accuracy of the model in this study is the Wavelet-based pricing model. This study made an attempt to improve the accuracy of option price estimation using Wavelet method and it improves the accuracy due to its ability to estimate the risk neutral MGF. The MSE and RMSE of Wavelet model is 0.208546 and 0.456669 respectively which is much lower than that of Black-Scholes model and therefore in conclusion, Wavelet model outperforms the other model. Equity, commodity, bond or currency, stocks, interest rate, exchange rate or any other financial variables of interest to the researcher could be the underlying asset. Derivatives includes; Forwards, futures, options and swaps. An option is a type of derivative that gives the option holder the right but not obligation to buy or sell an underlying security at a specified price, at any time on or before a given date in the future as agreed on. According to the right to sell or to buy, we distinguish the two types of options. The holder of a put option has the right but not the obligation to sell underlying security under some predefined terms of an agreement which includes; strike or exercise price, maturity date and option volume. European-style options can be exercised at the maturity date only while American options can be exercised at any time prior to or on the maturity date. American options need more complex pricing methodology due to the extra feature of early exercising. In the Kenyan market, derivatives are yet to be developed. Variables used in this model are observable, for example the time to expiration, exercise price and the closing price except the volatility which is not directly observable. This research therefore compared the performance of Wavelet based pricing model and Black-Scholes model in the valuation of a European option. The remainder of the paper is structured as follows: Section 2 discusses the existing literature; Section 3 presents an overview of the Black-Scholes model and Wavelet based pricing model. Lastly, section 5 concludes the study. Their formula can be used to obtain the following parameters; the spot price, the exercise price, interest rate, time to maturity and volatility. A lot of improvements have been done to the original Black-Scholes formula since the paper of  [2] . The derivation of the pricing formula is independent on the parametric form of the underlying security's price dynamics and hence the success of this model depends on the accurate capture of dynamics of the underlying security's price process. According to the researches as far as pricing derivative securities is concerned, Wavelet based option pricing model is the latest option pricing model in the literature  [6] . In comparison between the risk-neutral MGF and the implied risk-neutral PDF, the risk-neutral MGF has a number of advantages even though between them, there is a one to one relationship. The following are some examples of these advantages; when there is presence of jumps in the underlying process MGF is more tractable, the option's obtained MGF is a continuous function, statistical moments of the asset distributions can be obtained using MGF and the risk neutral MGF can be used to estimate the out of sample options that has maturity dates which are different. Algorithm has a superior performance in de-noising financial data with high frequency by using the first principal component to de-noise frequency variations of a business cycle with wavelets  [12] . Where;  formula_0 From the Black-Scholes formula above, we see that the strike price (K), risk free interest rate (r), time to maturity (T), the volatility (") and the current price of the stock (S0) are required. c is the call option price, p is the put option price and N (.) ʆ ! The underlying asset dynamics and investor expectation in option prices is captured by the MGF Θ !$ + of the logarithmic returns /0 7 . On this criterion, the Franklin hat function performs very well because it is symmetric, smooth and piecewise continuous. The data sets consist of simulated stock prices, the exercise price, the time to matureness, interest rate and volatility. These data is divided into three, In-The-Money options, At-The-Money options and Out-of -The money options. This is evident from the values of the RMSE and MSE, whereby MSE of wavelet model is lower than that of Black-Scholes model. In this study, we also focused on pricing European call options and therefore we recommend an extension of the approach to pricing more complex options like American options which have no general closed form analytical solution. Other complex options include; Bermuda options and exotic options.
paper_219	 quality of service (QoS). The resultant minimax value correlates to the lowest performing attributes of the framework. The minimax decision model is chosen to meet the design philosophy that little advantage to the overall enterprise network performance will result from further investment in high performing attributes prior to balancing performance across all three model attributes. The presented framework offers decision support tools to enable agencies to allocate limited resources towards improving the performance of their net-centric service offerings to the enterprise network. When discussing the theoretical, it is easy to dismiss the challenges of limited resources to implement new policies such as schedule, budget, manpower, etc. Successful policy implementation amongst other factors requires system analysis methods that assist acquisition agencies in targeting limited developmental resources to areas of greatest impact to the overall mission objectives. Not all data is the same; some is more relevant to the users' needs when compared across all the data. Section 2 of this paper provides a brief overview of the origins of net-centric and net-ready policies. [2] . The DoD introduced four criteria that must be satisfied for "Data Sharing in a Net-Centric Environment" via DoD Directive 8320.02 in 2004  [4]  which later in  [5]  expanded to the seven listed in this section. In the years following, the Chairman of the Joint Chiefs of Staff Instruction (CJSCI) 6212.01 was released that set-forth the procedures for development and certification of a Net-Ready (NR) Key Performance Parameter (KPP); the NR KPP process later being subsumed into the Joint Capabilities Integration and Development System (JCIDS) process  [3]    [7] . The NR KPP specifies the attributes required of data sharing Information Technology (IT) introduced into the net-centric operational environment: (1) IT must be able to support military operations (SMO), IT must be able to be entered and managed on the network (EMN), and (3) IT must effectively exchange information (EI)  [3] . The access process should be via the "network" using "commonly supported access methods"  [6] . Making data understandable requires alignment of terminology, data protocols, data formats, and data meaning between produced and consumer  [6] . Net-centric operational tasks are those that "produce information, products, or services for or consume information, products, or services from external IT"  [3] . non-generic); the required performance of the connections be identified by quantifiable and testable measures of performance (MOPs); and the connectivity must be managed by a structured methodology  [3] . Fig. Thus to avoid bias consider unequal sampling of the user population by using login authentication to identify users to form strata of homogenous users. A novel application to networks of a common survey practice is proposed to use the Horvitz-Thompson (HT) estimator  [10]  for determining the value of the total score of the population of size N. HT is commonly used because of its versatility as an unbiased estimator of the total for a sampling/sample with or without replacement. For a sample of size n consider v strata with sample s k in each stratum, i.e. ∑ i=1 to v n i = n and s = U k=1 to v s k . where n ≤ N and s = {1, 2,…, n}. Where it is not easy to design a stratified sampling plan a stratification post collection of n samples without replacement can be constructed. The sample is organized into their various strata with the number of elements in each total number of strata k, in the total population N. The HT estimator for stratification becomes formula_2 and estimate of the stratified mean is formula_3 where m k = ∑ i in s k y i /n is the arithmetic mean of strata k. With each attribute having its MOS value using the stratified estimate in (3). The originating data can vary from content on webpages; chat room information; geolocation data; audio; and data from electro-optic, infrared, or radar sensors. QDS is a subjective rating from the perspective of the end user  [11] . In the video standards of  [14]  and  [15]  they present a series of methods of modelling with objective measurements to predict the subjective ratings. The prevailing method for assessing the quality of a still image is based on the ability to perform certain levels of object recognition with scoring defined by the national imagery interpretability rating scale (NIIRS)  [17] . In  [17]  and  [15]  there are equations to estimate the interpretability for still imagery and video, respectively. QoS is in essence an engineering optimization problem where the objective is to maximize users' satisfaction while minimizing cost of delivery of the supporting network services. User satisfaction is traditionally associated with network metrics: delay, jitter, throughput, packet loss, order preservation. And the service level agreement (SLA) is the users' agreement with network provider(s) on acceptable ranges for the metrics. User-satisfaction provides a true gauge of a network QoS  [11]  and the subjective assessment of that satisfaction is provided by QoE. However there are a number of challenges to QoE discussed in  [19]  and  [20] . These models of QoE based on QoS objective measures are often referred to as "QoE\QoS" correlators  [21]  and  [12] . Models have been developed to correlate the QoS with QoE for multimedia applications  [12] . For both models  [21]  showed QoE had an exponential model in terms of their QoS objective metric(s). 1 ). The three principal elements of data relevance and their respective effects on the QoE for data relevance are now discussed, as depicted in  Fig. 2 . Intrinsic data relevance represents the relative value (i.e. Enterprise data systems can offer multiple forms of data to the consumer (e.g. still imagery, motion imagery, acoustic signal, electronic signal, radar) with each revealing different aspects for a given target of interest. The data's spatial properties relate to the location of the data collection sensor relative to the target of interest (e.g. overhead, side-view, rear-view, distant, near). Temporal properties of the data pertain to the time of data collection relative to the actions and conditions of the target of interest (e.g. target while in port, target while in open ocean, target when first detected, target after engagement). The measure of tagged data relevance corresponds to how accurate the data producer was in expressing the intrinsic data relevance through the application of metadata that is understandable, relatable, and unambiguous to the data consumer. A poorly performing taxonomy would be one that prompts extraneous definition of detail or that includes terminology with such subtle variation as to lack the mutual exclusivity necessary to select between one term or the other. We first discuss the design philosophy for the overall assessment of the data quality for the enterprise: (1) The enterprise data quality is determined by the attribute with the lowest user satisfaction (QoE). (2) A high user satisfaction for the enterprise data quality can only occur when all three attributes have high QoEs. By focusing on all three attributes we can reduce resources required in one attribute based on the overall value of data quality of the enterprise. In game theory, the minimax value of a player is the smallest value that other players can force without knowing the player's actions. The same value is also the largest guaranteed value for that player with knowledge of the other player actions. Formal minimax definition  [24]  is formula_4 where a(i) denotes the actions of the i-th player of n players, a(-i) is actions of all other players except the i-th, and v i is the value function of player i. The cells in  Table 1  consist of a left value which is the games' pay-off for player one and a right value which is the games' pay-off for player two. Rows in  Table 1  represent actions of player one and columns in  Table I  represent actions of player two. Player one has three action options U, D, N to choose from. Going through each action of Player one with knowledge of player two actions L, R we have maximum payoff for player one for U of 5, for D of 5, for N of 4, making a minimax pay-off action of player one of N. Player two takes each action along columns for action L maximum payoff is 4 and 3 for action R, resulting in minimax action for Player two of R with payoff of 3. The minimax strategy in game theory inspired the decision theory approach of Abraham Wald's minimax model  [25] . formula_5 Thus, for each d in D, starting with d=QDS, a state vector s max (d) is found that produces max s in S(d) f d (s) where f d (s) is a discrete function formed by a predictive model for QoE or in the absence of a model formed from direct user sampled MOS value calculated using (3), i.e. f d (s) = u HT . 3 , as formula_6 In  Fig. 3 , a particular value of enterprise data quality is given at the start, but from start one could continue to improve QoS and QDS under Option A with no increase in the value of enterprise data quality, whereas Option B modifying the system delivering DR does increase the overall enterprise data quality.
paper_241	 Transformers are the key equipment in electrical power transmission. This paper is concerned with the differential current protection scheme of transformer using a differential relay technique with arduino Uno microcontroller as a differential relay responsible for comparing the differential current values and sends trip signal to the relay (acting as circuit breaker) to open the circuit when there is fault in the protected zone as a result of imbalance in the differential current values. The system is efficient in transformer protection, gives better isolation, has accurate fault detection and quick response time to clearing faults The demand of electricity in our modern day as led to advancement in electrical power systems, which is reflected in the development of all the power system device generators, different sizes of transformers, transmission lines and the protection equipment. C transmission, power transformer is one of the most important equipment. Due to advancement in technology and daily use of electrical devices by industries, organizations and individuals, there is an increase in electricity demand which most likely results systems overload, reducing its efficiency and can cause damage to the transformer  [1] . Typically, Power systems are built to allow continuous generation, transmission and consumption of energy. The system is capable of sustaining a variety of environmental and operating impacts that resemble normal operating conditions which may include lightning striking the transmission lines, excessive loading, deterioration or breakdown of the equipment insulation resulting to power systems experiencing occasional faults such as phase to phase or phase to ground faults, over current, over voltage or even temperature based faults. differential current) and can be prevented using differential protection and microcontroller based relay protection. At all times, the Arduino senses the condition of the transformer. [4] Ochieng designed a microcontroller based power transformer protection system which uses a current sensor as the interfacing instrument between the current transformer and the pic16f690 microcontroller. This paper is concern with the implementation of digital differential current protection of a single phase transformer using Arduino Uno microcontroller as a mechanism of differential relay. A power transformer functions as a node to connect two different voltage levels  [3] . The power transformer to be protected is connected to the main AC supply and delivered power (P=IV) to the output terminal. The power transformer primary and secondary currents are measured by the current transformers, which are to be protected by the Arduino and the circuit breaker. The current transformers are each connected to the primary and secondary side of the step-up transformer and are used to reduce the currents to a measureable value that can be sensed by the current sensors. The current are first connected in series and then in parallel to the secondary side of the step-up transformer to display. Variac is introduced in the system to vary and show the characteristics of the differential protection scheme when differential current is below zero and above zero respectively. The LCD display is used to display the differential current values of the transformer and to display the voltage and current level of the system as well. The state of the system was divided into No load, faulty and normal conditions and were displayed on the 20*4 LCD display with a delay of 3 seconds as programmed on the arduino. For the load with both the 200W and 60W bulbs the current values and difference were larger than with each connected separately. At No load the secondary current is very close to zero as a result of open circuit at the secondary. The microcontroller based relay is a technological advancement or development as compared to the use of conventional relays which improves the efficiency and reliability of the system which could be beneficial to both the society and its economic growth.
paper_251	 Clouds provide an infrastructure for easily usable, scalable, virtually accessible and adjustable IT resources that need not be owned by an entity but can be delivered as a service over the Internet. The cloud concept eliminates the need to install and run middleware and applications on users own computer by providing Infrastructure, Platform and Services to users, thus easing the tasks of software and hardware maintenance and support. Heterogeneity in the computational requirement, dynamic choice and infrequent usages types of resources of the users in modern era has main challenge for service provider (application developer and hardware manufacturer). Secondly, Now computing power or connected computing power (with network) has more demanding and significant role in almost all areas of epoch including market analysis, searching, map, accounting, medical, trading, shopping, rescue operations and many more, the list is endless. More comprehensive concept about cloud computing has been narrated and drafted by National Institute of Standard & Technology (NIST): According to NIST-"Cloud computing is a paradigm for facilitating expedient, on-demand network access to a shared cluster (pool/collection) of configurable computing power and resources (like applications, services, networks, servers, and storage,) that can be expeditiously provisioned and exemption with least management endeavor or without service provider interaction. All the scheduling and monitoring assured the high reliability, automatic scalability, fault tolerance services in secure manner. Proposed agent based approach has provides the efficient and accurate solutions for efficient scheduling and monitoring in cloud computing. In the cloud computing. social in nature, mobile i.e. can roam in the network, perform the task at remote stations and send back the results to source platform (where they been originated), agents are also clone themselves and one of the core property of the agent is autonomy i.e. They are following with respective functionality in the proposed system 1. SaaS (Software as a service). 2. Platform as a Service (PaaS). 3. New Relic -To develop the core functionality of the proposed system. i.e. These are following 1. Elasticity optimization 3. Better Provisioning of the SaaS 4. The main lacking point in the article  [1]  and  [2]  is validation of the proposed mechanism. Additionally the requirements for such fast provisioning of the cloud has been discuss in the recent year in the article  [3] . Our main research work is to enhance the agent based model for SaaS delivery in the cloud as depicted in the  [1]  and  [2] . Detail Objective of the proposed agent based SaaS Service. 2. 4. Model for Proposed Work Our proposed work is to schedule and monitor cloud SaaS application onto the cloud and evaluate the performance of the same using proposed agent based. Time to execute the task 3. Availability 4. Hence proposed system performs better than  [2]  and any other methods as shown in figure 3. This paper presents the enhanced agent based solution to ensure better elasticity and monitoring solution. Following few areas has been chosen as future work as derivative of the proposed multi agent based solution where the current work can be taken further. 1. 2.
paper_272	 Since many years up to date now, we are still constructing the vacuum circuit breaker by classical design, but the interacting among the characteristics inside each vacuum interrupter must be scientifically analysis as a high values of the general specification which must be thoroughly understood before the breaker can be applied with safety confidence. Moreover the switching process associated with power electronics offers many modern benefits as a static switching system when joints together by using a snubber circuits for damping chopping current and transient over voltages or other facilities  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] . This paper focuses on the understanding of both characteristics and load specification relative to vacuum circuit breaker to generate precisely parameters  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] . In addition the inside electroplates some characteristics can be described as followings: Non-sustained disruptive discharge voltages In general, the above characteristics are not involving associated with only vacuum switchgear itself a lone, but when connects the switchgear with power distribution transforms or synchronous motors, there are also many negative interactive processes for instance hysteresis currents, eddy currents and high voltage capacitive inside each transformer (Parasites electrostatics charged) feedback to the interrupter process. The name vacuum arc is really incorrect, indeed, it's a contradiction  [2]  "If there is a vacuum there is no arc, and if there is an arc there is no vacuum". J=2*10 6 (A/m 2 ) Joule heating as thermionic emission Where Ø is the thermionic work function. K is Boltzmann's constant (1.37x10-23 Joules/K) The constant A is approximately 6x10 5 for most metals According to the Harris model and classification who was classified "the positive ions initiated from ionization process in an Anode plate and generates joule heating to a cathode plate surface during switching process"; i. Ionization zone ii. The total above processing time was measured by Harris approximately (20-250µs)  [15] . The term multiple re-ignitions refers to the series of alternate re-ignitions and high frequency (typical several hundred KHz)interruptions usually resulting in an increasing train of voltage peaks; this overall phenomena is usually defined as voltage escalation  [3, 4] . If the high frequency accompanying re-ignitions and voltage escalation in one phase couple into other two phases, the process of virtual current chopping can occur  [6] . It is important to appreciate that while current chopping and voltage escalation can occur in a single -phase circuit, virtual current chopping is specifically a 3-phase characteristic: the effects of normal current chopping, multiple re ignition and over voltage escalation in one phase can generate surge over voltages in the second and third phases. Pre-striking of the breaker in picking up a transformer load is somewhat similar to the multiple re-ignition event which occurs on opening a breaker  [7] . The process of current chopping is the premature suppression of 50Hz or 60Hz circuit current before normal current zero due to instability of the arcs in a vacuum interrupter  [6] [7] . When current chop occurs, the energy stored in the effective load inductance is transferred to the available loadside capacitance to produce the so called chop overvoltage, given by Ic√   According to the equation  1    EMTP which is called electromagnetic transient program or others such as SIMULINK/MATLAB are excellent numerical tools that allow for depth studies of switching transients in industrial as well as utility power systems. Since the high frequency characteristics of the power system equipment are depend on stray capacitances that mentioned above and inductances also and they in turn depend on physical dimensions, equipment layout, materials, cable lengths, all these can be represented as follows  [17] ; The figure represents TRV & Re-ignitions computed in the system when switching off power transformer load that the transient voltage approximately (350µs-450 µs) and shows very high frequency sub transients both going in the negative direction of the voltage -Left side. The analysis shows that the effect of damping in an oscillatory circuit can be described in terms of a single parameter, designed ŋ, or its reciprocal ʎ, which is the ratio of the resistance to the surge impedance of the circuit: This fact permits the construction of generalized damping curves for different values of ŋ, from which the solutions to many practical problems can be extracted with about the same effort as one expand in using a table logarithms. We note that the only parameter involved is η. Where η =0.5, the sine function changes from a circular to a hyperbolic function. Zo = √L/C = 5X104 ohms η. = 2. The electrostatic charges -static charged -parasitism are almost depleted in the a mount current begin to decrease in the same time of first peak value which generates chopping currents, moreover the formula of the transition processing rate of du/dt or di/dt have to be considered. 2. The theory of the snubber electronic circuit and other power electronic applications offer more reliable static switching process in the field of medium voltage switching system  [16] [17] [18] .
paper_294	 The cultural heritage of any people can be redressed through the preserved cultural values tangibly stored and preserved in media and forms retrievable and usable. Audiovisual heritage are quite essential that the public library system must not afford to elude its storage not only for users but also for posterity. It was therefore, recommended that concerned policy makers should make policies that will allow public libraries to acquire store and preserve audiovisual materials on various indigenous knowledge for development to ensue; and for posterity; and that governments and other stakeholders should ensure that the public library sector is adequately funded and manpower is adequately provided as well. The cultural heritage of any people can be redressed through the preserved cultural values tangibly stored and preserved in media and forms retrievable and usable. The photographs people take, videos they shoot, speeches and music they record, capture in bits, every moment of their life, culture, event and times that inevitably speak volumes of their history. Audiovisual heritage are quite essential that the public library system must not afford to elude its storage not only for users but also for posterity. The Tiv had different management strategies of postharvest losses of cassava (Akom or Alogo) that includes dried cassava chips (kpor), imyorun or imorun (gari) processing, fermented cassava (akpu) and production of base (mtuhem) among others, which were potent managing postharvest losses of cassava. Apparently, if this is allowed to continue, the consequences cannot be foretold in the near future. Specifically, the study sought to: 1. Identify Tiv management strategies of postharvest losses of cassava 2. 1. What are the Tiv management strategies for postharvest losses of cassava? 2. 3. They are the venue through which the overall information resources are made freely available to all (Edoka, 2000; Assoh, 2011; Shidi, Aju & Ashaver, 2014). The term cassava is most likely derived from the Arawak word for bread, casavi or cazabi, and the term manioc from the Tupi word maniot, which French explorers converted to manioc  (Lebot, 2009) . Moreover, other varieties of cassava in Tiv land of Benue State include: Akpu from which akpu are processed, Yakpe, Genyi, Wari and Gyo-Akom among others. Cassava (Manihot esculenta Crantz, Euphorbiaceae, Dicotyledons) is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world, mostly in the poorest tropical countries. Cassava plays an essential food security role because its matured edible roots can be left in the ground for up to 36 months. The crop therefore represents a household food bank that can be drawn on when adverse climatic conditions limit the availability of other foods. Fresh cassava has a very short postharvest storage life  (Karuri, Mbugua, Karugia, Wanda & Jagwe, 2001 ). Postharvest encompasses the conditions and situations surrounding the state of the food after separation from the medium and site of immediate growth or production of that food. Harris and Lindblad (1978)  asserted that postharvest begins when the process of collecting or separating food of edible quality from its site of immediate production has been completed. A direct loss is disappearance of food by spillage, or consumption by insects, rodents, and birds. These include: 1. Peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips) 2. Peeling the cassava, slicing, sun drying it and grinding into flour known as Mwem ma Kpor (Cassava Flour) 3. Slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base) and 5. This would have been achieved by capturing of audio narrations of elderly cassava farmers by recording, as well as organizing custodians of the knowledge to shot informative video clips and snap shot for video slides aimed at educating the younger generation and storing the materials for posterity. These are the Benue state Library Board Headquarters in Makurdi and its branches in Gboko, Vandeikya, Katsina-Ala, Gungur, Aliade and Adikpo with about 33 workers  (Mngutyô & Amaakaven, 2013) . The 19 item questionnaire adapted a 4 point rating scale and respondents were asked to respond by ticking the correct or applicable responses (SA) strongly agree, (A) agree, (D) disagree and (SD) strongly disagree. These research assistants were asked to administer and retrieve the questionnaire through personal contact to avoid delays associated with mailing and multiple filling. Importantly, on each research question, data were collected on related items in the instrument. Demographically, data were collected from 680 Library Staffs, Cassava Farmers and Library Users, out of which 348 were male representing 51.2% while 332 representing 48.8% were female. What are the Tiv management strategies for postharvest losses of cassava? The collected data were analyzed and presented in  Table 1 . This means that peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips), peeling the cassava, slicing, sun drying it and grinding into flour known as Mwem ma Kpor (Cassava Flour), peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom, slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base), peeling the cassava, fermenting, sieving and squeezing it into lumps known as Akpu, peeling the cassava, grating, squeezing (demoisturizing) it, mixing with salt and other ingredients and frying with palm or groundnuts oil known as Kweesi or Akweesa Akom and peeling the cassava, grating, squeezing and sieving the starch and preparing it for food as Kamu Akom are the management strategies for postharvest losses of cassava. Table 2  shows the mean rating scores of all the items are below 2.50, which implies that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). The collected data were analyzed and presented in  Table 3 . Table 3 , it can be seen that the mean rating scores of all the items are above 2.50. 1. Based on the results in  Table 1 , the findings from the study revealed that processing cassava into Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom, Mtuhem (Base), Akpu, Kweesi or Akweesa Akom and Kamu Akom are the Tiv management strategies for postharvest losses of cassava. Table 2 , findings of the study revealed that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). 3. Management of public libraries should also ensure that initiatives on going from one community to another to record and shot films on indigenous knowledge are in place.
paper_298	 Four research questions guided the study. Survey research design was used, registered users of the University Library between 2016-2019 numbering 920 formed the population of the study where a sample of 272 (30% sample) was used based on 95% confidence level and 5% confidence interval of Sample Size Calculator. The instrument used for data collection was questionnaire. Majority of the respondents used the library for research work rather than for leisure and recreational purposes. Regular shelf reading should be done so as to establish right contact between library users and library materials. The library as a service oriented organization must provide the bibliographic resources and services channeled towards the fulfillment of its parent institution's goals and objectives. The library ensures that the resources acquired are well organized to allow easy access by the library users  [1] . OPAC has brought a changing relationship between libraries and their users in the changing ICT environment. Library OPACs first emerged in the late 1970s and early 1980s and have gone through several cycles of change and development all geared towards improvement. The purpose of using OPAC majority of the respondents 63.2% stated that they use OPAC to know the availability of required document. Kumar revealed that 26.99% respondents use the OPAC fortnightly and 22.84% weekly. Okorafor discovered that catalogue use in Latunde Odeku Medical Library was poor. The author attributed the reason for the poor usage to lack of user education programme  [9] . It is on this note that this study investigates the use of library catalogue by undergraduate student in the university library. The consequences of this, a user may end off scanning from one shelve to another in search of a document, which is often a waste of time or failure to locate and retrieve the needed material. The study is designed to achieve the following objectives: a. To find out the students awareness about library catalogue as access point to library collection. b. The total population of the study comprises is 920 undergraduate students that had registered with the University Library with effect from 4/1/2017 to 15/7/2019, covering 3 academic sessions. This section presents questionnaire collected on the basis of gender of the research respondents. The gender breakdown is presented on  Table 1 . The question on the demographic information of the respondents was on gender presented in  Table 1 . It shows that majority of the respondents 188 (69%) were male, followed by females 84 (31%). This shows that majority of library users in the Federal University of Kashere Library were male. Table 2  revealed that majority of the respondents 262 (96.3%) were aware of the existence of the library catalogue in the university library. This finding is a good development for the library because it shows that the methods they used for creating awareness of library yielded good results. Awareness of access points to library collections are the most important factors influencing the success of the retrieval and utilization of library resources. The awareness of respondents would depend on the way the library informs their users about the use and functions of library catalogue revealing the available resources and its location in the library. With 64% response rate it is obvious that the respondents are aware of the existence of the card catalogues as a retrieval tool for searching for information materials. 26 (10%) of the respondents were aware of the existence of OPAC as a retrieval tools in the library. Their responses were presented in the  Table 4  below. Table 5  reveals that 106 (40%) got their awareness through library staff, followed by those that knew it through the user education programme (GST) 88 (16%). Never  Total  F  %  F  %  F  %  100  20  40  28  56  2  4  50  200  14  32  20  45  10  23  44  300  24  44  20  37  10  19  54  400  42  75  12  21  2  4  56  500  58  100  0  0  0  0  58  Total  158  60  80  31  24  9  262  Key: F=Frequency, %= Percentage. Table 5  indicated that majority of the respondents 158 (60%) used the library catalogue regularly when searching for materials in the library. Total  F  %  F  %  F  %  F  %  F  %  F  %  100  10  20  0  0  6  12  28  56  6  12  0  0  50  200  10  23  6  14  10  23  16  36  2  5  0  0  44  300  10  19  6  11  14  26  12  22  6  11  6  11  54  400  14  25  0  0  36  64  6  11  0  0  0  0  56  500  2  3  0  0  54  93  2  3  0  0  0  0  58  Total  46  18  12  5  120  46  64  24  14  5  6  2  262  The finding indicated poor use of OPAC 12 (5%) at the university library. The finding in this case was surprising considering that majority of the respondents had indicated that they do not use the OPAC. The results are presented in  Table 8 . Table 8  focused on the challenges associated with catalogue use for information retrieval. This indicates that good of the respondents had difficulties using the library catalogue because the respondents lack sound ICT skills that could enable them use the OPAC. This finding corroborates with that of Ogbole & Morayo which revealed that (41.1%) of the respondents rarely use OPAC. Again with that of [Ogunniyi & Efosa whose study concluded that the problem of catalogue use is associated with lack of knowledge on how to use the library catalogue  [11] . The finding was completely different from that of Onuoha, Umahi, & Bamidele where respondents faced problems associated mostly with orientation and unstable power supply in UNAAB and also at RUN where the challenge was mostly associated with computer supply and lack of orientation for students  [13] . The study revealed that majority of the university library users were male. Higher number of them got their awareness through library staff and above average used the library catalogue regularly. Because most challenges associated with the use of the OPAC often have to do with lack of ICT skills on how to make use of it.
paper_305	 In this paper, we report on the results of a MSc. Generally, bank loan officers rely on traditional methods to guide them in evaluating the worthiness of loan applications. Furthermore, a loan officer's credit decision or recommendation for loan worthiness is subjective. After some experience, these officers develop their own experiential knowledge or intuition to judge the worthiness of a loan decision. Generally, loan application evaluations are based on a loan officers' subjective assessment. The general objectives of this study were to: 1) Implement the meta learning algorithm -LogitBoost to develop as system for evaluating credit applications to support loan decisions in Kenyan financial institutions 2) Outline some of the challenges of using the learning algorithm in the decision-making process for the banking industry in Kenya 3) Champion the applicability of Java as an open source software in business intelligence applications From time immemorial in the banking sector, banks have relied on the personal assessment of loan risks or on the traditional statistical methods to predict the default of loans instead of using a standardized evaluation tool. Although the classifier takes this into account through voting -in which those values that meet certain thresholds are promoted to either of the classification values, most of such incidences are minimal and can be handled through judgmental procedures by re-examining those peculiar cases and applying policies as laid out. The ongoing changes in the banking industry, in the form of new credit regulations, the need for innovative marketing strategies, the ever increasing competition and the constant changes in customer borrowing patterns; call for frequent adjustments to credit management in order to remain competitive. One of the earliest automated procedures uses statistical tools which have fallen short of the inherent challenge for today's commercial banks is their desire to understand large amounts of information and reveal useful knowledge to improve decision-making. Although the model was reported to perform relatively better than models developed using other approaches; as part of the limitations and recommendation, they suggested that such a model is usually a black box and more insight the model parameters was required to make it more effective. The solution to the problem was an adaptation of ensemble machine learning strategies where a 'weak' classifier, commonly referred to as a base classifier was boosted through a series of adjustments through weighting and re-sampling to develop a better learner which was an additive aggregate of individual learners. A decision stump is a decision tree with only a single root node. A accept classification for a loan decision meant pointed to a successful application while a reject classification pointed to the alternative. The model was specified in terms of K −1 log-odds that separate each class from the base class K. a) With K attributes , there are K different decision stumps to choose from b) At each stage of boosting i. given reweighted data from previous stage ii. Train all K decision stumps iii. Learn all K classifiers again, select the best, combine, viii. reweight ix. Repeat until you have T classifiers selected The development platform used for this project mainly included the following open source software products: The Java Development Kit (JDK) which is a Sun Microsystems product released under the GNU General Public License (GPL) was one of the packages used especially for the compilation of the source files. i. iii. iv. vi. Calculate an average performance. This strategy is similar to the use of two files as discussed earlier but relies on the learner to automatically partition a given data set into two given a split percentage Options: -F -R -I 15 Number of performed iterations: 15 Time taken to build model: 0.06 seconds Time taken to test model on training data: 0.01 seconds The results were interpreted along the following parameters for all the various training and testing strategies. 1.0. 0.9. 0.8. 0.7. 0.6. 0.5. <0.5. The value converted to 1 decimal place, these values indicate a perfect classification After a successful implementation of the stated system, the following were the key outcomes:  Three options were investigated for training the algorithm namely: a) The use of single file both for training and testing the model through stratified cross validation. The model accuracy using this procedure was 86.86% making it a fairly reliable strategy b) The use of separate training and testing data sets returned an accuracy of 95% making it a relatively better strategy c) The use of a ratio to determine the size of the training and testing files from one data set returned an accuracy of 88.64% Therefore, it implies from these findings that the use of separate files for training and testing of the model returns the best model accuracy and hence should be adopted. The trained model was subjected to 20 instances of unclassified data which had been carefully selected from a portion of the training and through analysis returned 19 correctly classified instances resulting in a predictive accuracy of 95%  Three suggestions are likely improve the model and hence the predictive accuracy of the learner: The training and testing procedures can be done severally with different input parameters and file sizes to settle on the most effective set for different learning processes. As a conclusion, the reported work indeed confirmed that: 1) Machine learning procedures can be applied in financial modeling applications to augment manual underwriting techniques 2) These procedures can greatly improve the efficiency of such techniques because of their ability to handle large items of data generating very useful statistics 3) This work can be improved through the use of enhanced data set pre-processing procedures, the use of a cost matrix as well as parameter tuning to settle on the most effective set for various data mining requirements.
paper_310	 Now, smart phone has become the most essential thing in our daily life. Android application based smart phones are becoming each time more powerful and equipped with several accessories that are useful for Robots. This project describes how to control a robot using mobile through Bluetooth communication, some features about Bluetooth technology, components of the mobile and robot. We present a review of robots controlled by mobile phone via moving the robot upward, backward, left and right side by the android application such as Arduino, Bluetooth. Bluetooth has changed how people use digital device at home or office, and has transferred traditional wired digital devices into wireless devices. We derived simple solutions to provide a framework for building robots with very low cost but with high computation and sensing capabilities provided by the smart phone that is used as a control device. Here in the project the Android smart phone is used as a remote control for operating the Robot. The controller acts accordingly on the DC motors of the Robot. Related reference articles implementing wireless control of robots have been studied as mentioned in  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] . have been kept in mind before designing the project. Our system aims to achieve the target to design a system that can provide following functionalities with a simple and easy-to-use interface: a) Develop an android application that will act as an remote of a robot. e) Mobile, robot and Bluetooth are the on-going technologies which can be used for the benefit of mankind. Arduino has it own programming burnt in its Read Only Memory (ROM). Cprogram is very easy to implement for programming the Arduino UNO. The smart phone is the transmitter of this circuit. It sends the data to microcontroller through Bluetooth module. It also helps to send the instruction of forward, backward, left, right to the microcontroller. Actually, the smart phone is used as a remote of this system. The advantage of this project is that the application software designed for android phones is kept simple but attractive with all necessary built-in functions. Here we use programming language 'C' for coding. By this software we put the data and instruction for forward, backward, left, right operation of this system. Motor driver switches accordingly the data bit, if the data bit is low then the corresponding pin of the motor driver doesn't work else highbit then thecorrespondingpinof the motor driver is on. There are two steps of the programming. As seen from the  Figure 6 . The microcontroller, thereby, sends instructions, which when executed, helps in functioning of the motor driver. When user sends any data to the Arduino board then the corresponding pin of Arduino goes to high state and switches the motor driver ic in the on mode. Bluetooth module. The instructions are sent by the smart phone. This is indeed a cost-effective and efficient project. The novelty lies in the fact that it is a cost-effective project with a simple and easy to use interface compared to existing ones. The robot can be used for surveillance.
paper_333	 There are several issues and diseases which try to decline the yield with quality. Particularly, diagnosis of potential diseases on Enset is based on traditional ways. The researcher selected two Enset leaf diseases viz. The proposed model demonstrated with four different kernels, and the overall result indicates that the RBF Kernel achieves the highest accuracy as 94.04% and 92.44% for bacterial wilt and fusarium wilt respectively. Food security is a challenge in many developing countries like Ethiopia. The government committed 15% to 17% of expenditures to the sector; it covers 41% of gross domestic product (GDP), over 90% of export value and directly supports 80% of the populations' livelihoods  [1] . This is commonly known as false banana, the Ethiopian banana or the Abyssinian banana. Cheesman. Ethiopia is one of the grand producer of Enset in African continent countries. There are several issues and diseases which tries to decline the yield with quality. Therefore, the implementation of IT based solution in the sector will have a paramount importance to facilitate the activities such as economic, social and ecologic development in the country by increasing efficiency in both quality and quantity of Enset crops production, to sustain dependability of customer preferences and to preserve the ecology  [2] . In our country few researchers found the promising solutions to different plant diseases diagnosis such as maize, rose flower, coffee and others using computer vision and machine learning techniques  [3, 4, 5] . The remaining part of this paper is organized as follows. The types of common Enest diseases are discussed in Section II. Experimental results are reported in Section IV. Section V presents the conclusion and discusses of future works. Several types of diseases are known to affect Enset plants under field conditions. So far a number of fungal, nematode, viral and bacterial diseases were reported to cause damage at different degrees of intensity that was mainly explored by Quimio research finding since 1992. Therefore, the damage inflicted by each disease also varied. As it is mentioned in the above section, for the experiment two diseases and one normal of Enset were identified. B. Test Result For the experimentation a total of fifteen experimental setups which is color, texture morphological and a combination of each and a multiclass Kernel support vector machine model: Linear Kernel, polynomial Kernel, RBF kernel and Quadratic Kernel was used. The detail classification result is shown in  figure 4 .
paper_389	 The experimental corpus has been tested by Changjiang Daily for many years. Experiments are carried out to analyze the influence of the choice of conditional random field model parameters and the selection of Chinese character annotation sets on the experimental results. Common Chinese character tagging method is based on the Chinese characters appear in the words of different locations marked different labels. For example, "O" can be used to represent individual Chinese characters, "B" means Chinese characters appear in the head, "I" means Chinese characters appear in the middle or the end of the word. Enter the sentence as "This is Wuhan." We first set three candidate marks "O", "B" and "I" for each Chinese character and add a start node "BOS" to the head, An end node "EOS". The feature that appears on each node is then calculated, using the feature weights to compute the most probable of all paths from "BOS" to "EOS". According to the meaning of "O", "B" and "I", the rules are summarized as follows: 1, the sentence of the first Chinese character tag can not be I, the last character of the mark can not be B. Because the mark B must be followed by a mark and only the mark I, and must be marked in front of the mark I, and marked as B or I. 3, Mark B can only be followed by the mark I. Finally, according to the mark of each Chinese character, the mark result is "OOBIO", so the result is "这 / 是 / 武汉 /." For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. Secondly, feature extraction is carried out to generate training corpus and test corpus which can be recognized by CRF model tool. Then the training corpus is trained to generate a CRF model, and some training parameters such as iteration number are added in the training process. The main corpus used in this paper is the training corpus and test corpus of Changjiang Daily. The coding method is GB code. The corpus content mainly comes from newspaper news. The format consists of a sentence segment consisting of words marked with spaces. The results of "+ feature template 3" model are obviously better than that of "+ feature template 2" model, that is, under the condition of adding feature template 3, F-score is 4.4% higher than that of feature template 3, Played a better effect. It can be seen from  Table 5  that the results of the CRF system are better than those of other models under the same conditions as the training corpus and the test corpus. And then use these tools and the corpus carried out a number of experiments. The experiment not only demonstrates the influence of the choice of conditional random field model parameters and Chinese character annotation set on the experimental results, but also verifies the validity of the new features and the feasibility of the new method. As the Yangtze River Daily corpus is from the newspaper news, for the news corpus in the special format, such as title, poetry, weather forecasting, etc., to our model training has a certain impact, so if the corpus in the handling of these disturbances, Our model should have better performance.
paper_391	 Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. Globally, tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually. The data for this study were the Tetanus Toxoid data set from the Ethiopian Demographic and Health Survey (EDHS) 2011, and analyzed using the Knowledge discovery process of Selection, Processing, Transforming, mining, and interpretation. The WEKA 3.6.1 tool was used for classification, clustering, association and attribute selection. But, in the class attribute, it is 0.72. Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. And the major cause of early infant deaths in many developing countries is often due to failure to observe hygienic procedures during delivery. Thus, women receive doses of tetanus toxoid to protect their birth against neonatal tetanus  [1] . Infection is acquired through environmental exposure of any broken skin or dead tissue such as a wound or when the umbilical cord is cut to the spores of the bacteria. Globally tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually, accounting for up to 25% neonate's death in some African countries  [3] [4] [5] . As of December 2012, maternal and neonatal tetanus persist as a public health problem in 30 countries, mainly in Africa and Asia  [2] . In sub-Saharan Africa, up to an estimated 70,000 newborns die each year in the first four weeks of life due to neonatal tetanus  [5] . Ethiopia has one of the highest neonatal tetanus morbidity and mortality rate in the world due to low tetanus toxoid immunization coverage coupled with some 90% of deliveries taking place at home in unsanitary conditions. In Ethiopia in 1999 WHO has estimated about 17,875 neonatal tetanus cases and 13406 NNT deaths which made the country to contribute to 4.6% of the global NNT deaths  [3] . Vaccination coverage with at least two doses of tetanus toxoid vaccine estimated at 70% in 2011 and an estimated 82% of newborns protected against neonatal tetanus through immunization  [3] . The TT vaccination schedule in Ethiopia for childbearing women follows the schedule recommended by WHO for developing countries  [6] . Immunizing the mother prior to childbirth with TT protects both her and her newborn against tetanus and antenatal care is the main programmatic entry point for routine TT immunization. The information is rich and massive. In this study, we have used several data mining techniques; classification, clustering, association and outlier detection techniques over the nominated Tetanus Toxoid immunization dataset of the EDHS 11. This problem caused the loss of time and effort in the healthcare system environment and spending lots of efforts and costs without having evidence-based information for planning and intervention. The standards used are a percentage of accuracy and error rate of every classification techniques used. The technique which is suitable for a particular dataset is chosen based on highest classification accuracy rate and less error rate. However, not all of the patterns are useful. The EDHS 2011 was conducted by the Central Statistical Agency (CSA) in collaboration with Measure DHS and ICF International under the auspices of the Ethiopian Ministry of Health (EMoH). The EDHS of 2011 dataset was used as a source for this study and WEKA 3.6.1 machine learning tools are used. Only 80% of the overall data is used for training and the rest 20% was used for testing the accuracy of the classification of the selected classification methods. csv" file formats and stored as an ". arff" file format. Such models, called classifiers, predict categorical class labels (nominal, ordinal). The classification has numerous applications, including fraud detection, target marketing, performance prediction, manufacturing, and medical diagnosis. How does this classification work? Data classification has two steps; Firstly, consisting of a learning step that is, where a classification model is constructed. The approaches are; (a). Decision tree (J48) approach It is a flowchart-like a tree structure. And the topmost node in a tree is the root node. This approach uses divide and conquers algorithm to split a root node into a subset of two partitions till leaf node that occurs in a tree. [8, 9]  (b). The training tuples are described by m attributes. Each of the tuples represents a point in an m-dimensional space. (c). This assumption is called class conditional independence. (d). Confusion matrix; is used in this study, include accuracy, sensitivity, specificity, and precision. Training and testing are performed k times. Thus, in this study, we have used five clusters. "How does the k-means ( ) algorithm work?" The InfoGainAttributeEval with Ranker T used with respect to the class attribute and visualization was done in the attribute selection. The 5680 of mothers were from rural Ethiopia, and more of them (3484) were in the age range from 25-34. Briefly described as follows: Cluster 0-This group have 1534 (27%) instances of which consist of mothers who live in the rural, no access to radio, no access to TV, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 1-This group consists of 1004 (18%) instances of which mothers who live in the rural, no access to radio, access to TV yes, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, head of household male, tt-vaccinated yes. Cluster 2-This group consists of 2063 (37%) instances of which mothers who live in the rural, no access to radio, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated No. Cluster 3-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 4-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, orthodox in religion, Amhara in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education no education, women age 35-49, mother marital status married, hh_head Female, tt-vaccinated yes. As explained in the    The literacy status of the mother has high information gain (0.046) and followed by the literacy status of the mother (0.041) and the least information gained was head of household (0.00000147).
paper_402	 As a result, the BPNN with the selected features was able to interpret more accurate results (r 2 = 0.991) than the model with all the features (r 2 = 0.816). The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. ANN emerged as one of soft computing paradigms that have been successfully applied in several engineering fields  [1] . Specifically, ANN has been used to solve a wide variety of civil engineering problems  [2] [3] [4] . Mainly, ANN was utilized to model the nonlinear behaviour of fatigue and creep of Reinforced Concrete (RC) members  [5] [6] [7] [8] . Recently, research interest has revolved around the development of ANN models to interpret the behaviour of structural materials such as steel, concrete, and composites  [9] [10] [11] [12] [13] [14] . The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete  [14] [15] [16] [17] . Afterwards, several authors began developing ANN models for the prediction of compressive strength of high performance concrete  [18] [19] [20] [21] . In this study ANN is employed with other machine learning techniques to identify the parameters that capture the compressive strength of UHPC using data collected from the literature. [24] . Ghafari et al. There are two types of SFS classes -mainly filter method and wrapper method  [28] , where Zhou et al. Four sets of open human motion data and two types of machine learning algorithms were used. Moreover, Rodriguez-Galiano et al. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. Table 1  presents the initial input variables together with their range (maximum and minimum values) and symbols for identifying them in this experimental program. Artificial neural network (ANN) is a machine learning tool that imitates the learning functions of a human brain by providing a robust technique in classifying and predicting certain outcomes based on the model's objective. There are two types of ANN models: (1) feed forward; and (2) feed backward. In this study, the feed backward ANN is used, where it is composed of input neurons, hidden neurons, bias units, wires containing randomly generated weights, and output neurons. The increment started from one neuron and ended with 15 neurons, where the model was analyzed 10 times, for each increment, because the Levenberg-Marquardt algorithm locates the local, and not the global, minimum of a function. Figure 2  shows the plot of all the scenarios with the minimum point circled at 11 neurons. Therefore, 11 neurons is, approximately, the number of neurons that is sufficient enough for BPNN to facilitate an accurate ANN model for the collected dataset. In this study, four variables (Cement, Sillica Fume, Flyash, and Water) were selected as the most relevant features for the prediction model. It was observed that the r 2 and NMSE before and after selection yielded 81.6% and 99.1%, respectively, and 0.0594 and 0.026, respectively. Using Fly Ash quantities that range between 0-200 kg/m 3 and Silica Fume quantities that range between 40-160 kg/m 3 while fixing the quantity of cement at 1400 kg/m 3 and water at 175 kg/m 3 , several plots showing the variation of strength of UHPC were generated as shown in  Figure 5 . BPNN was used and three major steps were executed: (1) verification of ANN; (2) application of both SFS; and NID, and (3) analysis of selected features using ANN and LSR. The SFS tool was used to select the relevant constituent that impacted have the most impact on the compressive strength of UHPC which are mainly Cement, Sillica Fume, Flyash, and Water. 3) The ANN model with the selected relevant input parameters also showed a lower deviation (89.6 %) than the ANN model with all the features (58.7%).
paper_418	 Time series analyses are statistical methods used to assess trends in repeated measurements taken at equally spaced time intervals and their relationships with other trends or events, taking account of the temporal structure of such data. An important aspect of descriptive time series analysis is the choice of model for time series decomposition. Results show that seasonal variances of the Buys-Ballot table is constant for additive model and a function of slope and seasonal effects for mixed model. An important goal in time series analysis is the decomposition of a series into a set of non-observable (latent) components that can be associated to different types of temporal variations  [1] . For equation  (1)  it is assumed that the error term t e is the Gaussian white noise ( )  For equation  (2)  it is equally assumed that the error term t e is the Gaussian white noise ( )  An additive model is based on the assumption that the sum of the components is equal to the unadjusted data. They do not depend on the level of the trend  [3] . The additive model was adopted when the magnitude of the seasonal pattern does not change as the series goes up and down while the additive model was adopted when the magnitude of the seasonal pattern does not change as the series goes up and down  [5] . An important aspect of descriptive time series analysis is the choice of model for time series decomposition. Linde  [7]  observed that, the differences between the Additive and Multiplicative and the models are (i) for the additive model, the seasonal variation is independent of the absolute level of the time series, but it takes approximately the same magnitude each year while in the multiplicative model, the seasonal variation takes the same relative magnitude each year. Gupta  [8]  observed that, the additive model assumes that all the four components of the time series operate independently of each other so that none of these components has any effect on the remaining three. According to him, this series are not independent of which other. While multiplicative, model assumes that the four components of the time series are due to different causes but that, they are not necessarily independent and they can affect each other. The method adopted in this study is the Buys-Ballot procedure in descriptive time series. 3. 4. The column variances of the Buys-Ballot table is constant for additive model, but a function of slope and seasonal indices for the mixed model. formula_5 Source: Iwueze and Nwogu (2014), Nwogu el al (2019). That is when 0 b =  (Table 1)  For mixed model, we obtain using the expression in  Table  1  ( ) ( )  formula_8 when there is no trend. formula_10 When there is no trend ( 0 b = ) we obtain from  (Table 1)  . Table 3  that when there is no trend i.e. The real life example is based on monthly data on number of registered road traffic accidents in Owerri, Imo State, Nigeria for the period of 2009 to 2018 shown in  Table A1 . The data of 120 observations has been arranged in a Buys-Ballot table as monthly data (s = 12) and for 10 years (m = 10). Results show that seasonal variances of the Buys-Ballot table is constant for additive model and a function of slope and seasonal effects for mixed model.
paper_428	 The present article examines the influence of thermal radiation on two-dimensional incompressible magnetohydrodynamic (MHD) mixed convective heat transfer flow of Williamson fluid flowing past a porous wedge. The effects of different pertinent physical parameter such as magnetic parameter, Williamson parameter, radiation parameter and Prandtl number on temperature and velocity distributions are observed through graph. Williamson analyzed the flow of pseudoplastic materials and presented model to described the behavior of pseudoplastic material and explain convenient importance of plastic flows, and also recognized that viscous flows are very varied from plastic flows  [1] . Nadeem     [6] . Hayat et al. studied the peristaltic motion of non-Newtonian fluid through asymmetric channels along porous wall by means of various phase and amplitude, and also studied the manipulation of different wave structures on the fluid flow model  [8] . and they studied the flow in wave framework which is moving with speed of wave  [9] . Nadeem et al. developed a model for the transport of Williamson fluid in an annular region  [10] . Khan et al. Khan     [19] . Electrically directing fluid flow has received the concentration of researchers due to its several applications in technology and science like MHD pumps, MHD power generators and purification of crude oil. Hayat et al. Azimi et al. Reddy discussed unsteady MHD transport of rotating fluid past a permeable surface confined by infinite vertical permeable plate and concluded that by increasing rotating parameter the velocity field is also increased  [23] . Misra     [32] . Merkin et al. Xu and Chen proposed two-layer model to simulate mixed convection flow in a room  [35] . Bau investigated the thermal convection in a saturated stratified medium bounded between two parallel eccentric cylinders with the help of a regular perturbation expansion along Daarcy-Rayleigh number; it was observed that the appropriate preference of eccentricity values can maximize the heat transfer inside annulus of various thermal insulators  [37] . Fu et al. Kaya found nonsimilar solutions of steady laminar mixed convection heat transfer flow from a perpendicular cone in a porous medium with influence of radiation, conduction, interaction and having high porosity  [40] . Jafari et al. studied unsteady combined convection flow in a cavity in presence of nanofluid  [41] . Bég et al. Deka   is wedge angle parameter. The system of ordinary differential equations 7 and 8 subject to the boundary conditions 9 and 10 is first reduced to a system of first order ordinary differential equations using the substitutions E K =`, $ K = a, F K = b. [78] , Yih  [79]  and Rashidi et al. [76]  in  Table. 1. Figures (1-2)  illustrate the influence of wedge angle parameter < with on velocity and temperature profile. It is observed that velocity increases by increasing the wedge angle parameter < , but the thermal boundary layer thickness is decreased. Furthermore an increase in λ may cause increase in temperature of flow. It is clear from graph that an increase in thermal radiation parameter leads to increase in temperature and thermal boundary layer thickness. The steady, incompressible two dimensional boundary layer flow of Williamson fluid past a porous wedge is analyzed numerically using the 5 th order Fehlberg technique. The important conclusions of the analysis are 1. The non-dimensional velocity profile increases by increasing the wedge angle parameter <.
paper_432	 And found some results; The theory of reduction of cotangent bundles developed playing an important role in solution of the general problem for reduction a single or bit type cotangent bundles for base manifolds, possibility study of Hamiltonian tubes when the simplistic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models. The Cotangent Bundles * of manifold d provides the basic model of a symplectic manifold. Then the Cotangent Bundle has a dual space * . In case M is model on Euclidean space we have ≈ and so we want to assume that * ≈ * . We are motivated in part by the role of such action a groups of a symplectics of Hamiltonian systems with cotangent bundles phase spaces. We can introduce the Legendre transformation we need some basic facts about the structure of the cotangent bundles * of a nd dimensional differentiable manifold . 1, for any 0 ∈ , the tangent space at 0, denoted , is the space of linear derivations on 1 2, , that vanish on 3 2, , . Thus,T 5 M can be identified with (1 2, , /3 2, , )* the space formula_0 , is called the cotangent space at 0; it is isomorphic to the dual * , of M . Observe that if = 06 ∘ 8, as  (,)  , and ( 1 2, (,) / 3 2, (,) ) ** described above, DC corresponds to the linear map in T 5 * (M) . We see that(E > ) ,…, (E ) is a basis of T 5 * (M). For the mapping.∅: & → | to have a lift ‹ satisfying the condition ( ‚ ) =J ‚ , where ‚ ∈ &,J ‚ ∈ H, 0 (J ‚ ) =d 0 =( ‚ ), it is necessary that∅ (π n (&, ‚ )) ⊆0 (a n (H,J ‚ )) (3. 5). 1)  Figure 2 . 8 ' ( K , n, ' ) = 8 ' ( K, n ). ' , for K ∈ , n, ' ∈ . (n, ) = (n`n, ). [n, ] = [n,`n, ] . A slice theorem (or tube theorem) is a theorem guaranteeing the existence of a tube under certain conditions. In this part we study the symplectic geometry of cotangent -lifted action induced by a smooth proper action of a Lie group on a smooth manifold. This model is known as the Hamiltonian tubes; it the basis of almost all the local studies concerning Hamiltonian of Lie groups on symplectic manifolds. In the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models. The first work studying symplectic normal forms in the specific case of cotangent bundles. The symplectic reduction of the cotangent bundles * Q has more structure than a symplectic manifold. The left -hand side has the reduced symplectic form corresponding to the canonical symplectic form on T * Q , and T * (Q G ⁄ ) has the canonical symplectic form. Conclude that the theory of reduction of cotangent bundles developed playing an important role in solution of the general problem for reduction a single or bit type cotangent bundles for base manifolds and found that the phase space is the cotangent bundle T * Q of a configuration space .
paper_444	 Resource constrained project scheduling problem is to make a schedule for minimizing of the completion time or total cost subject to precedence rules and resource constraints. Traditional resource constrained project scheduling problem research takes into account achieve management goal in certain environment. However, there are many uncertainties in practical projects due to the uncertain factors, which leads to the change of resource availability. In this paper, for better described the uncertain resource constrained project scheduling problem, we firstly consider the uncertain resource availability project scheduling problem based on uncertainty theory. The goals of the model are to minimize the completion time and the total cost which composed by the activity cost and the additional resource cost. The other constraint is the resource constraint in which the demand of resource shall not exceed the total supply of resource for each resource type at any time. The resource constrained project scheduling problem (RCPSP) takes into account the balance of makespan and total cost through resource allocation and reasonable activity schedules while the precedence constrains between the activities and resource constrains are all satisfied. In recent years, many scholars discussed different types of resource constrained project scheduling problems, such as multi-mode RCPSR  [1] [2] [3] , multi-project RCPSP  [4] [5] , robust RCPSP  [6] [7] , and so on. In real-word projects, there may be some uncertainty phenomena, for example, overdue materials, the decrease in the number of workers at Grain in Ear season, etc, which result in the resource availability may be changed. Lambrechts  [10]  established a stochastic project scheduling model in which the resource availability was a random variable in order to increase robustness. Its goal was minimizing the expected weighted sum of the absolute deviations between the planned and the actually realized activity starting times. Its constrains were the resource and priority rules. Uncertainty theory based on uncertain measure founded by Liu  [12] , and its a branch of axiomatic mathematics for modeling human uncertainty. Ji and Yao  [14]  recently considered the uncertainty of the duration times and the resources allocation times by assuming them are uncertain variables. Up to now, we have not yet found uncertain resource availability constrained RCPSP in uncertain environment, which is not either randomness or fuzziness. In this paper, we consider uncertain resource availability project scheduling problem based on uncertainty theory, in which the increased quantities of resource are uncertain variables, and the finish time of each activity is a decision variable. Then, we build a multi-objective model which is under resource and precedence rule constrained to minimize the resource cost and the project completion time. In order to solve the problem of time-cost trade-off in project scheduling, we consider a project which is described as an activity-on-the-node network ( , ) , where = {1, 2, ⋯ , } is the set of activities and is the set of pairs of activities with precedence relations. A multi-objective programming model is built under the resource restricts and precedence rules of activities constrains in order to balance the completion time and the total cost of the project. (4) No interruption is allowed for each activity in progress. " Constraint ○ 4 reflects that for any time # and each resource type , the demand for resources shall not exceed the total supply of resources with at least given confidence level < = . The one is original resources cost determined by resource employ. The other is additional resource costs, in which means the product of the cost per time unit of additional resource, project completion time and the increased quantities of resource . [16]  Let G A , G P , ⋯ , G ) be independent uncertain variables with regular uncertainty distributions H A , H P , ⋯ , H ) , respectively. If the function Q(R A , R P , ⋯ , R ) ) is strictly increase with respect to R A , R P , ⋯ , R S and strictly decreaseing with respect to R STA , R STP , ⋯ , R ) , then formula_2 holds if and only if formula_3 To solve the model (1), according to the operational law of uncertain variables, we transformed it into its equivalent form, as shown below. formula_4 Proof: Because is an uncertain variable with regular uncertainty distribution Φ , and formula_5 by the definition of uncertain variable  [16] , we know that ? is an uncertain variable, and the inverse uncertainty distribution of ? The manager tends to the demand of resource shall not exceed the total resource supply with at least given confidence level 0.9 for any time # and each resource type . The cost per time unit of additional resource ! Cost  Preceding activity  1  0  0  0  0  2  1  5  1200  0  3  5  24  28800  2  4  27  16  103680 3  5  30  24  172800 3  6  22  24  126720 4  7  19  9  41040  4  8  24  6  34560  3  9  12  4  11520  8  10  22  20  105600 7  11  7  8  13440  6  12  20  8  38400  7  13  24  17  97920  5  14  12  6  17280  4  15  16  4  15360  7  16  11  6  15840  6  17  10  8  19200  14  18  8  6  11520  2  19  24  11  63360  18  20  13  4  12480  6  21  9  4  8640  12/15  22  3  4  2880  21  23  4  4  3840  7  24  0  0  0  22  With the above demand, we can present the following model:  In the real-life project, due to the influence of uncertain environment, managers should consider the trade-off between the completion time and cost. By describing the increased quantities of resource as uncertain variables, an uncertain resource constrained project scheduling problem is discussed in this paper.
paper_462	 There are many problems such as insufficient practical ability training. "5 + 3" new training mode of training in combination. Pilot units for professional degrees. The reform has opened up a new path and realized the organic link between professional degree education and vocational qualification certification. In the long-term practice of training clinical master, Chongqing Medical University has gradually explored a new "5+3" training mode which combines clinical master education with regular training, and completed the educational practice of "Innovation and Practice of the Training Mode of Clinical Master Professional Degree in China"  [3] . It has effectively solved the problems of imperfect training mode of clinical master, lack of mature training mode of high-level clinicians, low overall training quality of clinical master, inability of clinical master to apply for the qualification of practicing doctor, difficulty of effective connection with regular training, difficulty of seamless connection between professional degree education and industry admission standards, and inadaptability of current management system and mechanism to professional degree postgraduates. Educational development and other key problems of clinical master training. The system reform of training mode for clinical master of Medical University has effectively solved the problem of insufficient practical ability of clinical master, and has realized the degree education and professional qualification recognition of clinical medicine specialty. The organic cohesion of syndromes has effectively improved the quality of clinical master training. To vigorously develop professional degree postgraduate education and improve the quality of training high-level applied talents is the focus of the current degree and postgraduate education work. The second is to allow the registration of licensed physicians from other places to the relevant departments of our university, which solves the problem of registering candidates who have obtained licenses for licensed physicians from different places in our school. Thirdly, we allow our clinical master not to take the entrance examination, but to be directly incorporated into the standardized resident training system, so that they can participate in the standardized resident training. The above policy support ensures that our clinical master can fully participate in clinical practice skills training and ensure the quality of training. The Graduate Management Department of the Department organically integrates the training of clinical master's degree, the training of seven-year students' master's degree, and the application of resident doctors for master's degree related to the work of degree award, and arranges the clinical training and clinical competence assessment of clinical master's degree as a whole. The tutor who applied for the postgraduate examination is the first tutor (defense tutor). Each clinical rotating Department of postgraduate clinical rotation appoints a teacher qualified as a professional master's tutor as rotating responsibility tutor to ensure that clinical master's is in clinical rotation. In order to consolidate the reform results of the training mode of clinical master, gradually standardize the management system and continuously improve the quality of training, the school continuously improves the rules and systems around the education of clinical master, covering the methods of re-examination admission, the quantitative assessment of the selection of tutors, the training program, the tutor group system, the curriculum, the professional course examination, the assessment of clinical ability, the regulation of publishing papers, the management of research funds and institutions  [8] . The deputy chief physician who meets the age requirement can recruit professional degree postgraduates, and more than two postgraduates must have one professional degree. In order to provide a common basis for the follow-up process, it is necessary to ensure that the postgraduate enrollment conditions are equal to the basic conditions required by the professional qualification certification. It is closely related to clinical practice. Thirdly, according to the requirements of standardized resident training, and with the cooperation of training bases, starting from medical ethics, medical ethics, laws and regulations, professional ethics and basic clinical skills, we will offer lectures on medical law, applied psychology, humanistic literacy and doctor-patient communication to comprehensively improve the comprehensive quality of clinical master. Therefore, the training object, the training teachers and the responsible subjects can be combined into one, which provides a common basis for the integration of training. Requirements for the first stage of training. For non-graduate students, we adopt the "fill-in" training method, that is, accurately record the clinical rotation time of clinical master, and add up the previous training time to meet the requirements of training time. Students who have obtained the certificate of resident standardization training are directly transferred to specialist training (i.e. System for Clinical Ability Clinical competence is the core of clinical master training. How to objectively and effectively assess the clinical competence of postgraduates is the key to ensure the quality of clinical medicine degree award. Therefore, according to their own characteristics, each unit has formulated a variety of assessment indicators system, and its level is uneven, resulting in different clinical ability of clinical master. The main reason for this phenomenon is that since the trial implementation of clinical medical professional degree, the state has not evaluated the quality of the pilot units and lacked an effective monitoring mechanism. There are mainly "pre-job training", "centralized departure assessment", "stage assessment" and "annual assessment"  [13] . The school has established a multi-level and whole-process clinical competence assessment system, which is suitable for clinical master's clinical competence assessment and regular training and graduation assessment. The Interim Measures for the Implementation of the Regulations on Academic Degrees of the People's Republic of China promulgated in 1981 stipulates that before the postgraduate's reply, the degree-granting unit shall employ one or two experts from the disciplines concerned with the thesis to review the thesis. The clinical master emphasizes on examining the clinical competence of postgraduates. Compared with academic degree postgraduates, the training objectives of professional degree postgraduates are obviously different. Fourth, direct link the workload of tutors' guidance to professional degree postgraduates with the promotion of their professional titles, so as to improve the enthusiasm of tutors' guidance to professional degree postgraduates. By promoting the combination of clinical master training and regular training, the problem of clinical master qualification certification has been solved, and a new "5 + 3" training mode has been constructed. This model is based on practice in an all-round way, does not need the national single enrollment index, does not need the national special allocation, has the characteristics of innovation, practicability, commonality, feasibility, and has the realistic basis for comprehensive promotion and implementation. It provides a mature reference model for the docking of professional degree postgraduate education and industry access standards in other fields in China. The new "5+3" training mode built by the school covers all aspects of training, such as curriculum system, assessment system, award system, award system, rotation system, tutor system and management mode. It fully guarantees the quality of clinical master training and brings up a large number of high-level applied medical talents for Chongqing and even the whole central and Western regions, which has produced remarkable economic and social effects. The two systems complement each other and organically combine to realize the training of clinical master and regular training. Secondary colleges have set up a special post-graduate management office to coordinate the implementation of the reform of the training mode of their departments. This model is in line with the reality of the development of medical and health care and higher education in China. They have achieved the ultimate goal of training "doctors who can really see a doctor". The employment rate of graduates has been guaranteed to be 100% for a long time. They have trained a large number of high-level applied medical talents for Chongqing and the whole central and Western regions, promoted the development of health undertakings in the central and Western regions, and produced remarkable economic and social benefits  [16] . In the 2011 "Forum on Reform and Development of Medical Education", the principal of the school presented the reform experience to the conference and won the unanimous praise of the broad masses of colleagues. The article "Construction and Practice of Quality Assurance System for Master of Clinical Medicine Degree Postgraduates" published in "Degree and Graduate Education" won the second prize for Excellent Thesis of "Degree and Graduate Education", and it is also the only prize-winning thesis in the field of medical science. The "5 + 3" reform of clinical medicine master's degree in Chongqing Medical University has established a "one goal", that is, to train "doctors who really can see a doctor", and established the basic principles of the reform of training mode of clinical medicine master's degree and the training of clinicians. The implementation of "three combinations", namely, the combination of enrollment and enrollment, training and training, degree award and physician access. It has solved the difficult problem of registering for medical examination for master of clinical medicine in our university and effectively connected the training of master of clinical medicine with the regular training of master of clinical medicine. The "four certificates association" should be implemented, namely "licensed physician qualification certificate", "standard training qualification certificate", "diploma certificate" and "degree certificate"  [17] . The problem of docking professional degree education with industry access standards has been solved. It has effectively solved the problems of imperfect training mode of clinical master, lack of mature training mode of high-level clinicians, low overall training quality of clinical master, inability to apply for the qualification of licensed physician, difficulty in effectively linking up with regular training, and difficulty in seamlessly linking professional degree education with industry access standards.
paper_476	 The inverse relationship of frequency with cost and operational efficiency becomes the key to the decision of delivery frequency. This paper uses JD Logistics as an example to quantitatively analyze the relationship mentioned above. The results show that: (1) The cost and resources operational efficiency are closely related to the order splitting ratio under the same delivery frequency; (2) The delivery frequency has different effects on the operational efficiency of the resources in different links. However, the increasing in delivery frequency will lead to changes in the workload of different resources (distributors, facilities, equipment, etc.) The transportation system is a very complex system with many different feedbacks and lagged responses between policy makers. System dynamics models not only offer a different Operational Efficiency: A Case Study of Jingdong Logistics perspective while whole system approach to transport planning, but also demonstrate to policy makers the importance of these feedbacks and lagged responses  [1] . Fan Xuemei et al. That research explored three scenarios, which are joint delivery, autonomous delivery, and third-party delivery, which also pointed out that, in order to improve delivery efficiency, enterprises should adequately consider relevant factors such as own resources, competitors' delivery strategies, and urban transport policies before determining delivery methods. Jesus et al. [5]  pointed out that joint delivery can effectively improve the efficiency of urban delivery. Some other researches tried to improve the delivery efficiency and reduce the delivery cost through delivery center location optimization  [6] [7] [8] , delivery vehicle route optimization and scheduling  [9] [10] [11] [12] , and delivery resource integration  [13] [14] [15] . Lin Wanting [22]  considers the phenomenon of unfilled orders, information leakage, and product damage which exists in crowdsourcing delivery, then built system dynamics model from five dimensions of reliability, responsiveness, assurance, empathy and security, which provide decision-making advice for enterprise delivery efficiency management. First, it analyzes the boundary and causality of its delivery system. Then, it establishes a simulation model for the operation of the delivery system on the base of system dynamics. Next, it uses the simulation model to study the effect of changes in delivery frequency on delivery costs and resource operation efficiency under different quantity of delivery orders. It provides reference for express companies to determine the delivery frequency. Next, it analyzes the interaction between delivery frequency, cost, and resource efficiency, and builds dynamics simulation model to get the equilibrium of cost and resource operation efficiency under different delivery frequency. Each delivery operation mainly includes: storage, ferry, sorting, transportation and terminal delivery. The variable costs occurred due to the volume of orders, which include operating cost of the sorting equipment, rental cost of the site, units fixed cost of transport vehicle, fuel costs, toll and so on. In addition, the number of working facilities and the operating time are affected by factors such as order quantity, delivery frequency, sorting equipment efficiency, and unit load of transport vehicles. Labor costs are composed of fixed wages and performance wages for employees in the three links mentioned above. The employees include direct employees who are vehicle drivers and indirect employees who are managers. The number of employees is affected by factors such as order quantity, transportation efficiency, delivery frequency, and the number of transport vehicles. The sorter is the person who operates on the sorting equipment. The on-site logistics personnel is the person who puts the sorted goods into a mail packages and places them on a pallet and then sends it to the transportation vehicles. The number of sorting staffs is influenced by factors such as the amount of cargo, the number of sorting equipment, the worker's efficiency, the sorting time requirements, the area of sorting venues, the number of logistics personnel in the yard, the delivery frequency and so on. This article considers the utilization efficiency for the facilities, personnel, and technology such as turnover rate of transportation vehicles, the utilization rate of delivery site, and the delivery personnel loading rate, etc., which are put into the sorting, transportation, and terminal delivery links. The utilization rate of transportation vehicles is calculated by dividing the actual traffic volume at each sorting center by the vehicle capacity. The utilization of the site space is obtained by dividing the actual leased area of the site by the available area. The usable area of the site consists of public area and working area. The personnel load rate in the process is calculated by dividing the actual workload by the authorized workload. The delivery frequency refers to the number of times of terminal delivery by the company in unit time (in days). At the same time, the factors such as sorting time requirements, proportion of per-order batch, efficiency of the delivery personnel, and unit fuel consumption of the vehicle, will have a direct impact on the delivery frequency. These factors present a complex, nonlinear, and inverse relationship between each other. Therefore, this paper uses the causal loop method of system dynamics to analyze the relationship between the factors. According to the influencing factors above, a simulation model of the system dynamics has been built as shown in in  Figure  3 . This model passed the mechanical error checking and dimension consistency testing by VENSIM. The extreme condition when the order quantity equals to zero was examined as well. Second  Third  Fourth   1  60  30  40  20  2  30  30  20  20  3  10  30  30  20  4  -10  10  30  5  ---10  Table 4 . Setting reason for each scheme. The minimum value of 3.9% appeared in the order volume of 3086-3154 units. The four scenarios showed a lower average unit cost when the order quantity was around 4,700 units indicating that this order quantity was a batch of economic orders. The main reason is that the increase in the delivery frequency improved the demand for the number of delivery personnel and caused the delivery time of delivery personnel to be shorter in a single batch. The vehicles utilizations of the three sorting centers in scenario 1 showed the first drop point when the order volume was 2530, 2567, and 2350, and the second drop point occurred at 5000, 5200, and 4800. The difference between the vehicle utilization efficiency in scenarios 2 and 3 increased with order volume until it reached 3600 units. This is because that the ratio between the order quantity and the actual number of delivery personnel at the delivery site was not equal. The change of delivery frequency has different effects on the resource operational efficiency in different stages of the delivery process. With the same delivery frequency, different order splitting ratios affect delivery cost and resource operational efficiency. Therefore, JDL needs to consider the increase in delivery frequency, the increase in delivery costs, the overloading of resources, and the ratio of orders and splits. Recommendation II: one should consider constraints such as delivery resources and consumer satisfaction to achieve the appropriate decision of delivery frequency. With different delivery frequency, JDL's delivery resources and consumer service quality are different. The increase in delivery frequency can, ease JDL's resource utilization in the links of sorting, transportation, delivery, and other aspects, and provide consumers with fast delivery services. In the same delivery link the demand for delivery resources can be different due to different delivery operational capacities. In this case, if scenario 2, 3, or 4 is adopted, one can increase the total number of order shipments by 2,400, 1,400, and 4,450 units respectively without increasing the number of transport vehicles. The delivery frequency has different effects on the resource operational efficiency in different delivery stages.
paper_479	 Handwriting is also a key element in document examination as it leaves a forensic document examiner with the task of determining who the writer of a particular document is and this is achieved through the likelihood ratio (L R) paradigm. It has been established that there are varieties of handwritten documents ranging from forgeries, counterfeiting, identity theft, fraud, suicidal note, contested wills. A forensic document examiner is saddled with the task of document authenticity. To determine whether a document is genuine, an examiner may attempt to confirm who created the document amongst other things. The likelihood ratio paradigm has been studied as a means for quantifying the strength of evidence for a variety of forensic evidence types in handwriting and other types of forensic evidence such as earmark, speech, footprint, fingerprint, glass fragments and DNA  [1] [2] [3] [4] [5] [6] [7] [8] . In ' competing, ' it can be said that they are mutually exclusive, but may not necessarily include all possible alternatives. Consider, as an example, a situation where the question is whether a specific writing belongs to a particular person. One proposition is 'The suspect is the author of the document in question'. A counter argument may be' The defendant is not the author of the document in question'. For instance, someone might have fabricated the writing with some special skills. The definition of proof analysis is then reflected in the proportion of probabilities. By doing so, the meaning of its value becomes clear, that is, how more (or less) the effects are frequent under the conditions of one proposal than under the conditions of the other proposal. Extended writing samples such as a paragraph of writing as well as signatures were considered. The task of verification, which is to determine whether two writing samples compared side-by-side, originates from the same person, was the principal problem addressed. Strength of proof serves an integral part of this problem. [15] [16] [17] [18]  estimated a L R for handwriting using Bayesian approach but in the presence of nuisance parameter, and their works had no underlying principle and model in which this L R was estimated. Back Propagation Neural Network (BPNN) served the sake of supervised learning of the system. The rationale behind the choice of this algorithm is due to the fact that it is a supervised network and a supervised network will have a target, so the BPNN is a network that has a target. Various things come together to form a BPNN, such as hundreds of single units, artificial neurons or processing elements (PE), connected to coefficients (weights), which represent a neural structure and are organized in layers. The feature information extracted from student handwriting through LBP was entered in NN through input layer and the participation of each class of character variables is calculated by the hidden layer of the network using: formula_1 net j is ith type of factor, n is the number of factors in net i , & ' ! weight-recognition factor with weight ! " . The weight from input points i and two hidden unit j is ! " and ! ( . Weight from second hidden unit i and output unit j is ! ) . Step 1: Feed-Forward Computation The vectors & " , & ( and & ) are computed and stored, evaluated derivative also stored. Step 2: Backpropagation to the output layer This research looked for the first partial derivatives 12 1 ! ) Base on decision law i.e. An inconclusive state is declared if and only if L R value = 1 With the BPNN algorithm to model handwriting pattern for each writer and L R estimation described in  [10]  this paper was able to deal with the problem of inconclusiveness as there was no inconclusive results reported in the investigation, determine who the writer of a questioned document is as well as eliminate the inclusion of parameters considered nuisance in investigation because each writer's handwriting profile could be ascertained due to the BPNN training of the Handwriting categories. Thus there must be agreement in sign otherwise there is disagreement. Table 1  shows the results of our Estimated Likelihood Ratio (ELR) for a writer against any other author in a collection of appropriate databases where the upper confidence interval (CiU) and the lower confidence interval (CiL) are indicated.
paper_492	 This function is the behavior of so-called short-term memory. The nervous system of long-term memory behaves freely but keeping consistency of the change in the environment. In parallel with advances in technology, metals, chemicals, and even semiconductors have emerged as a new material for new products. Furthermore retroactively, although there is a quantitative difference in each part between brain of ape who does not speak the language and our brain, but our brain is consisted of same material. In other words, the nervous system of animals called advanced higher animals is locally same as very primitive animal's nervous system. In Chapter 2, the circuits by combining the basic functions of neural circuits for serial parallel conversion and inverse of the conversion are shown. The nervous system related to short-term memory is activated in synchronization with environmental events, but the nervous system involved in long-term memory is highly layered to form an image corresponding to past and future events. Acceptance and generation of time series data of the nervous system of long-term memory is carried out in consistent with short-term memory. Even bacteria like animals in the early stages of evolution must have some eating behavior such as moving relying on light and smell to search for food and determine whether they can be eaten. The number of logical elements used may not differ much from the sum of nerve cells in insects or zooplankton. The same is true for the recognition process. The same applies to general figures. Deductive logical development is desired. The divided subsequence is defined the basic subsequence. The dividing is done by the following procedure. (1) The first element is the beginning of the first subsequence. In given example, the leading element is a1, followed by a7, a4, a6 and a6. (2) If the same element exists in already divided subsequence. (3) If the maximum length of subsequence is defined, a new subsequence is allocated after the subsequence that reaches the maximum length, add new element to the concerned new subsequence. The subsequences divided by above procedure are defined as the basic subsequence. For the next data c 1 additionally activates the elements which has been activated by the first data c 0 . Because the elements activated by c 1 is randomly connected to input, not all elements activated by c 0 are additionally activated. Four portions are activated in the  Figure 2 . By receiving c 2 , c 3 and so on the activated portions become narrow and narrow. The output of the element holding the activity when receiving the last c 4 of the time series data is the recognition result of the time series data c 0 c 1 c 2 c 3 c 4 . The number and their position of the activated elements is the conversion output corresponding to the serial input  [1] . Its output is the result of the serial parallel conversion, it is the result of AND logic of the output of the activated elements. The elements involved in the conversion are still activated at the time of output. that can be said a conversion of parallel to serial triggered by the first data. On the flow direction of data, this parallel to serial conversion is upside down with the serial to parallel conversion described above, but the basis of neural network operation is the same. After the first data reception, the connected elements are activated as described above. In the spontaneous operation (here by voluntary muscle) performed reactions such as changes of weight feelings from muscle or joint are accompanied by. The essence of the neural network which performs the parallel serial conversion and the neural network which performs the serial parallel conversion is the same except that the conversion result comes out downward or upward. For each basic subsequence a neural network that accepts and generates the basic subsequence is considered. Neuroscientist Damasio calls "image" the internal representation built in the nervous system by stimulation inside and outside the body. The organism would not yet be capable of subjectivity and would be unable to inspect the images in its own mind, execution of a movement; the movement would be more precise in terms of its target and succeed rather than fail. The nervous system which is involved in the imitating function is called mirror neurons. The object to be drawn is the change of all things, the joys and sorrows of livings, and their hope for the coming future. One of the factors that made the language possible to dramatically expand the range of expression is that it can be used as expression of object even if it is not near. On your birthday day, you may want to see the cake in front of you, identify the cake from store A, which was the one that was the subject of a conversation with your family the day before, and worry about the difference from the previous day's expectations. The upper part shows the part related to episodic memory, and the lower part shows the part related to the short-term memory. The part shown in red is a part that is particularly activated, the lower red disk is the part activated by the visual data of the chocolate plate that placed in front, the upper red disk is the part that is activating by recalled episode about the chocolate plate. The following is a description of  Figure 6  associated with the definition of the category shown in Tom Leinster "Basic Category Theory"  [4] . A category N consists of: [Def.1] a collection ob (N) of objects; In this paper, object corresponds to nerve cells in neuroscience and in neural networks corresponds to basic units (or combinations thereof) described in  Figure 6 . In neuroscience, the morphism corresponds to synapses and axons, and is responsible for the transmission of information between objects. [  formula_0 The composite of the coupling is considered to be a hierarchical connection of the objects, and the identity, which is considered a special morphism, can correspond long axons extending beyond hierarchies. According to the Hebb law, if there are two nervous system activated, the binding between the elements in the two categories will be enhanced. It has been vaguely thought that the nervous system responsible for long-term memory may be in a different place from the nervous system responsible for short-term memory. It is a short-term memory to be involved in the recognition and manipulation of the thing in the vicinity of the animal itself. Consistency is required when the objects in which both memories are involved are the same. The process of taking the consistency between the two memories was explained using the idea of a free functor in category theory. From among the random connections, the necessary connections for the desired operation are selected and enhanced, and the target function is realized. Therefore, even if the circuit is partially damaged, it is possible to supplement by learning the function of the peripheral circuit is lost. This process is close to the rehabilitation process of the brain that has had a stroke. There may be cases of errors in the accuracy of the operation compared to the circuit using the existing logic IC because there is a probabilistic part, but the bud of a new strategy might be hidden in the vicinity of the malfunction.
paper_507	 Landslides are one of the major natural hazards that account for hundreds of lives besides enormous damage to properties and blocking the communication links every year. The main aim of present study is to integrate the result of our study with spatial data, soil parameters, land inventory and used the output as a user friendly application using GIS which could predict the future susceptibility of region to landslide and% contribution of each factor for the same. Finally, an overlay analysis will be carried out by evaluating the layers obtained according to their accepted coefficient in final model.. As per the official figures of United Nations International strategy of disaster reduction (UN/ISDR) and Centre for Research on Epidemiology of Disasters CRED for the year 2006, landslide ranked 3 rd in terms of number of death among the top ten natural disasters  [2] . The recent landslide of 2012 took a heavy toll on life and property, many people lost their lives, and thousands of tourist were stranded due to disrupted communication services  [5] . These map divides the land into homogeneous areas or domain and their ranking according to degree of actual hazard potential caused by mass movement  (Guzzetti et. In the present study with the help of satellite imageries like DEM from cartosat-1 and topographic maps from GSI (geographical survey of India) a susceptibility map is prepared. 2. 3. Uttarkashi falls under the physiographic division (s) Rohilkhand plains, Nepal Himalayas, Ganga, Yamuna daob, Siwalik range, Kumaun Himalaya, Dhaoladhar range. There are 793 villages with area drained by major river (s), Yamuna, Ganga. Thus a number of thematic maps (referred to as data layer in GIS) based on the specific parameters which are related to occurrence of landslide viz. slope, aspect, lithology, rainfall, land cover etc. DEM (Digital elevation model) was obtained from BHUVAN. The back-propagation training algorithm is the most frequently used neural network method and is the method used in this study. An artificial neural network "learns" by adjusting the weights between the neurons in response to the errors between the actual output values and the target output values. The arrangement of the nodes is refer-red to as the network architecture (  figure 18 ). Figure 18 . Architecture of neural network (source:  (Lee, 2009) ). The dataset is categorized into 60% training and 40% validation. ANNs can be grouped into two major forward and feedback (recurrent) networks. The data is categorized into training, testing and validation, and all the six factors namely soil depth, soil type, rock and a excel database is created. The dataset is categorized into 60% training and 40% validation. As we have seen neural network can compute the output for a given input. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer (20), and between the hidden layer (20) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). In the training process we change the weights in that way in which the network output and the true values get closer and closer to each other. For a new dataset the weights are unknown. Most of the training datasets met the 0.01 RMSE goal. This study was concerned to the region of Uttarkashi due to the limitation of resources and time, we have been able to generate the results for a limited area Rishikesh Uttarkashi-Gangotri-Gaumukh route from latitude 78°19'55.14'' to 78°47'36.27" and longitude 30°32'30" to31°1'9.33". The results are compiled below. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area  [13] . Thus a number of thematic maps (referred to as data layer in GIS) based on the specific parameters which are related to occurrence of landslide viz. slope, aspect, lithology, rainfall, land cover etc. They are: 1. Slope 2. Soil depth 3. Soil texture 4. Height 5. Land use 7. The dataset is categorized into 60% training and 40% validation. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer  (15) , and between the hidden layer (15) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). The regression performance was 2.03, the accuracy for the training data was 0.99409, for testing and validation are 0.41565 and 0.18369. The most contributing factor is slope carrying 93% and the least one is soil depth.
