{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 36/36 [00:00<00:00, 39.18it/s]\n"
     ]
    }
   ],
   "source": [
    "sent_df = pd.read_csv('./data/test_sentence_embeddings.csv')\n",
    "sent_df['sent_transf_emb'] = sent_df['sent_transf_emb'].apply(ast.literal_eval)\n",
    "sent_df['word2vec_emb'] = sent_df['word2vec_emb'].apply(ast.literal_eval)\n",
    "\n",
    "with open('./data/categories.json', 'r') as f:\n",
    "    categ_id = json.load(f)\n",
    "\n",
    "paper_ids = []\n",
    "categs = []\n",
    "paper_doc2vec_embs = []\n",
    "paper_sent_transf_embs = []\n",
    "paper_word2vec_embs = []\n",
    "\n",
    "\n",
    "for p_id in tqdm(set(sent_df.paper_id.values)):\n",
    "    paper_df = sent_df[sent_df.paper_id == p_id]\n",
    "    paper_ids.append(p_id)\n",
    "    categs.append(paper_df.iloc[0].category)\n",
    "    paper_doc2vec_embs.append(list(np.mean(np.array([list(i) for i in paper_df.doc2vec_emb.values]), axis = 0)))\n",
    "    paper_sent_transf_embs.append(list(np.mean(np.array([list(i) for i in paper_df.sent_transf_emb.values]), axis = 0)))\n",
    "    paper_word2vec_embs.append(list(np.mean(np.array([list(i) for i in paper_df.word2vec_emb.values]), axis = 0)))\n",
    "    \n",
    "\n",
    "paper_df = pd.DataFrame({'paper_id': paper_ids, 'category': categs, 'categ_id': [categ_id[i] for i in categs],\n",
    "                         'word2vec_emb': paper_word2vec_embs}).sort_values(['category', 'paper_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7932863180239611"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = {}\n",
    "\n",
    "with open('./results/lsa_summaries.txt', 'r', encoding = 'utf-8') as f:\n",
    "    for i in f.readlines():\n",
    "        p_id = i.strip().split('\\t')[0]\n",
    "        summ = i.strip().split('\\t')[1]\n",
    "        summaries[p_id] = summ\n",
    "\n",
    "similarity_scores = []\n",
    "for paper_id in summaries.keys():\n",
    "    paper = sent_df[sent_df.paper_id == paper_id]\n",
    "    summ_text = summaries[paper_id]\n",
    "\n",
    "    paper_emb = paper_df[paper_df.paper_id == paper_id]['doc2vec_emb'].item()\n",
    "    summ_emb_list = paper[paper.sentence.apply(lambda x: x in summ_text)]['doc2vec_emb'].values\n",
    "    summ_emb = list(np.mean(np.array([list(i) for i in summ_emb_list]), axis = 0))\n",
    "\n",
    "    similarity_scores.append(cosine_similarity([paper_emb], [summ_emb]).item())\n",
    "    \n",
    "np.mean(similarity_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec + Equal clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8913951508956429"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = {}\n",
    "\n",
    "with open('./results/clust_summaries_word2vec_equal_clusters.txt', 'r', encoding = 'utf-8') as f:\n",
    "    for i in f.readlines():\n",
    "        p_id = i.strip().split('\\t')[0]\n",
    "        summ = i.strip().split('\\t')[1]\n",
    "        summaries[p_id] = summ\n",
    "\n",
    "similarity_scores = []\n",
    "for paper_id in summaries.keys():\n",
    "    paper = sent_df[sent_df.paper_id == paper_id]\n",
    "    summ_text = summaries[paper_id]\n",
    "\n",
    "    paper_emb = paper_df[paper_df.paper_id == paper_id]['doc2vec_emb'].item()\n",
    "    summ_emb_list = paper[paper.sentence.apply(lambda x: x in summ_text)]['doc2vec_emb'].values\n",
    "    summ_emb = list(np.mean(np.array([list(i) for i in summ_emb_list]), axis = 0))\n",
    "\n",
    "    similarity_scores.append(cosine_similarity([paper_emb], [summ_emb]).item())\n",
    "    \n",
    "np.mean(similarity_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec + Top N closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9008287849665815"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = {}\n",
    "\n",
    "with open('./results/clust_summaries_word2vec_top_n.txt', 'r', encoding = 'utf-8') as f:\n",
    "    for i in f.readlines():\n",
    "        p_id = i.strip().split('\\t')[0]\n",
    "        summ = i.strip().split('\\t')[1]\n",
    "        summaries[p_id] = summ\n",
    "\n",
    "similarity_scores = []\n",
    "for paper_id in summaries.keys():\n",
    "    paper = sent_df[sent_df.paper_id == paper_id]\n",
    "    summ_text = summaries[paper_id]\n",
    "\n",
    "    paper_emb = paper_df[paper_df.paper_id == paper_id]['doc2vec_emb'].item()\n",
    "    summ_emb_list = paper[paper.sentence.apply(lambda x: x in summ_text)]['doc2vec_emb'].values\n",
    "    summ_emb = list(np.mean(np.array([list(i) for i in summ_emb_list]), axis = 0))\n",
    "\n",
    "    similarity_scores.append(cosine_similarity([paper_emb], [summ_emb]).item())\n",
    "    \n",
    "np.mean(similarity_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Based Summ. results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_df = pd.read_csv('./results/query_based_summ_sent_transf.csv')\n",
    "summ_df['summary_by_closeness'] = summ_df['summary_by_closeness'].apply(ast.literal_eval)\n",
    "summ_df['summary_by_frequency'] = summ_df['summary_by_frequency'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9147987223628427"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_scores = []\n",
    "for paper_id in summ_df.paper_id.values:\n",
    "    paper = sent_df[sent_df.paper_id == paper_id]\n",
    "    summ_text = ' '.join(summ_df[summ_df.paper_id == paper_id]['summary_by_closeness'].item().values())\n",
    "\n",
    "    paper_emb = paper_df[paper_df.paper_id == paper_id]['doc2vec_emb'].item()\n",
    "    \n",
    "    summ_emb_list = paper[paper.sentence.apply(lambda x: x in summ_text)]['doc2vec_emb'].values\n",
    "    summ_emb = list(np.mean(np.array([list(i) for i in summ_emb_list]), axis = 0))\n",
    "\n",
    "    similarity_scores.append(cosine_similarity([paper_emb], [summ_emb]).item())\n",
    "    \n",
    "np.mean(similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8744386886213783"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_scores = []\n",
    "for paper_id in summ_df.paper_id.values:\n",
    "    paper = sent_df[sent_df.paper_id == paper_id]\n",
    "    summ_text = ' '.join(summ_df[summ_df.paper_id == paper_id]['summary_by_frequency'].item().values())\n",
    "\n",
    "    paper_emb = paper_df[paper_df.paper_id == paper_id]['doc2vec_emb'].item()\n",
    "    \n",
    "    summ_emb_list = paper[paper.sentence.apply(lambda x: x in summ_text)]['doc2vec_emb'].values\n",
    "    summ_emb = list(np.mean(np.array([list(i) for i in summ_emb_list]), axis = 0))\n",
    "\n",
    "    similarity_scores.append(cosine_similarity([paper_emb], [summ_emb]).item())\n",
    "    \n",
    "np.mean(similarity_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_df = pd.read_csv('./results/query_based_summ_word2vec.csv')\n",
    "summ_df['summary_by_closeness'] = summ_df['summary_by_closeness'].apply(ast.literal_eval)\n",
    "summ_df['summary_by_frequency'] = summ_df['summary_by_frequency'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9173872327014787"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_scores = []\n",
    "for paper_id in summ_df.paper_id.values:\n",
    "    paper = sent_df[sent_df.paper_id == paper_id]\n",
    "    summ_text = ' '.join(summ_df[summ_df.paper_id == paper_id]['summary_by_closeness'].item().values())\n",
    "\n",
    "    paper_emb = paper_df[paper_df.paper_id == paper_id]['doc2vec_emb'].item()\n",
    "    \n",
    "    summ_emb_list = paper[paper.sentence.apply(lambda x: x in summ_text)]['doc2vec_emb'].values\n",
    "    summ_emb = list(np.mean(np.array([list(i) for i in summ_emb_list]), axis = 0))\n",
    "\n",
    "    similarity_scores.append(cosine_similarity([paper_emb], [summ_emb]).item())\n",
    "    \n",
    "np.mean(similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8713999620499607"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_scores = []\n",
    "for paper_id in summ_df.paper_id.values:\n",
    "    paper = sent_df[sent_df.paper_id == paper_id]\n",
    "    summ_text = ' '.join(summ_df[summ_df.paper_id == paper_id]['summary_by_frequency'].item().values())\n",
    "\n",
    "    paper_emb = paper_df[paper_df.paper_id == paper_id]['doc2vec_emb'].item()\n",
    "    \n",
    "    summ_emb_list = paper[paper.sentence.apply(lambda x: x in summ_text)]['doc2vec_emb'].values\n",
    "    summ_emb = list(np.mean(np.array([list(i) for i in summ_emb_list]), axis = 0))\n",
    "\n",
    "    similarity_scores.append(cosine_similarity([paper_emb], [summ_emb]).item())\n",
    "    \n",
    "np.mean(similarity_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
