paper_1	 Tweets and other updates have become so important in the world of information and communication because they have a great potential of passing information very fast. They enable interaction among vast groups of people including students, businesses and their clients. This helps in solving the problem of losing vital information that is generated from the social media. Clustering is a descriptive task of data mining. In incremental prototyping, the whole requirements are broken down into building blocks which are incremented each time a new component is integrated based on an overall design solution. The solution is complete when all the components are in place. Several activities were performed to come up with the system. This generally dealt with preliminary processing of the data collected from the users to do away with any inconsistencies and outliers  [11] . These unwanted features are not very good because they can easily cause the system to perform irregularly. The third step involved using the data already preprocessed above to train the prototype. The fourth step was testing the system. The prototype was then subjected to testing using the test data. This involved picking the details of a new student from twitter and trying to predict the class hence group that he should join. The illustration of the proposed prototype is given below. The results reported in this paper were obtained from a series of evaluations that were done on the classifier on different parameters including functionality, usability, accuracy, precision and recall. This is summarized in the chart below. The Naïve Bayes Classifier was also tested to evaluate its accuracy, precision and recall  [9] . In experimenting with the Naïve Bayes Classifier, we relied on the NLTK module which provides functions for calculating these measures for the classifier. From this analysis the classifier performed above average with an accuracy of 71.4%. Precision and recall were however average. They therefore come with a lot of challenges including time wastage. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. They therefore come with a lot of challenges including time wastage.
paper_2	 This creates unwanted congestion during peak hours, loss of man-hours and eventually decline in productivity. The density of the vehicles is measured in three zones i.e., low, medium and high based on which timings were allotted accordingly. Junction timings allotted are fixed. The junction is a link to three roads which include; Effurun-Sapele road, Jakpa road, and PTI road. But the system only increases the level of traffic congestion during peak hours. The sensors used in this project are infra-red (IR) and photodiodes. The density of the vehicles is measured in three zones i.e., low, medium, high based on which timings were allotted accordingly. Most of the crossings handle the automated traffic signaling using fixed duration intervals between the Red, Yellow, Green and Pedestrian Pass Signal. Junction timings allotted are fixed. Fuzzy Logic offers the possibility to 'compute with words', by using a mechanism for representing linguistic constructs common on real world problems. Real world complex problems such as human controlled systems involve a certain degree of uncertainty, which cannot be handled by traditional binary set theory. However, this system has the disadvantage of the controller since it depends on the preset quantification values for fuzzy variables  [6]  conducted a cross sectional study targeting traffic control in the city of Nairobi's Central Business District and its surroundings. This system was broken down into different units as listed below: (1) Power Supply Unit (2) Control Unit (3) Sensor Unit   Power Supply: A power supply of +5V with respect to ground is required for the micro controller. The complete circuitry is operated with TTL logic level of 0-5V. It comprises of a 0V to 12V transformer which is required to step down the 220V AC supply to 12V AC. It is further filtered through a 1000µF capacitor and then regulated using 7805 regulator to get +5V. Since the peak inverse voltage of the diodes has to be greater than the peak secondary voltage of the transformer, the 1N4007 silicon diode with peak inverse voltage (PIV) of 1000 Volts was used in the circuit. 0.7 volts for silicon diodes). For convenience, a capacitor of 1000uF is used. As for the optocoupler, it is used to provide coupling while ensuring electrical isolation between its input and output  [12] . The Transformer steps down the 220 v AC supply to 12 v AC. IR sensors are placed on the intersections on the road at fixed distances from the signal placed in the junction. The IR sensors are used to sense the number of vehicles on the road. The Vero board is also called a strip board. The codes are as shown in the Appendix. The problem assumes a more worrisome dimension on Effurun Market days. In order to address this problem, an advanced traffic congestion control system is required. This new design would further reduce time delay, improve efficiency and reduce accidents by incorporating the following modifications: (1) The Airport -PTI road should have a two -way traffic light to pass the vehicles coming from Airport road to PTI road. There should also be a two -way traffic light between Effurun -Sapale road and Jakpa road to pass vehicles coming from Effurun -Sapale road to Jakpa road. (2) Solar energy should be used to support the mains power supply because of the highly erratic nature of power supply from (PHCN).
paper_3	 The aim of this article is to present the general architecture trends of Web-based Adaptive Educational Hypermedia Systems (AEHSs) and to give a complete description of architecture of the AEHS MATHEMA. In the beginning, a related work on the architecture trends of Web-based AEHSs is presented. They build a personalized model of the learner and apply this model to adapt the content and/or the appearance of the hypermedia, according to the learner's specific characteristics, such as the cognitive goal, level of knowledge, pre-existing or prior knowledge, interests, preferences, stereotypes, cognitive preferences and cognitive or learning style  [4] . The aim of this article is to highlight the main contributions of the architecture of the MATHEMA in the improvement of the functionality of AEHSs. The key idea is the decentralization of their functions. The rest of the paper is organized as follows: In the section 2 related works for various models of architecture of AEHSs is presented. In the section 3 the architecture of the AEHS MATHEMA is presented in detail. For the AEHSs various models of architecture have been developed so far on which the designers of these systems are based on. The AHAM architecture model proposed by  [6]  is shown in the  Figure 1 . The AEHSs fit so nicely in this model. The rules together form the adaptation model in AHAM. A key point in AHAM is that the adaptation rules are used to translate user actions into user model updates (as well as the presentation and adaptation of content). Also, the rules use the user model information together with the action information in order to determine the required user model updates. In order to perform adaptation based on the domain model and user model is needed to specify how the user's knowledge influences the way in which the information from the domain model is to be presented. The ADAPT project has identified high level design dimensions for AEHSs, which are: (1) context of use (CU), (2) content domain (DM), (3) instructional strategy (IS), (4) instructional view (IV), (5) learner model (LM), (6) adaptation model (AM), and (7) detection mechanism (DE). [8] presented a similar architecture in the Proper system based on the AHAM model. Proper's architecture is a combined architecture of SCORM LMS and AEHS. Thus its architecture is a typical of a SCORM compliant LMS. Moreover domain independent data of UM is stored into the database while at the same time, data about user knowledge is stored into Java Object Files. All the runtime data about user actions and performance is stored into Java Object Files via JSP and Java servlets. In the  Figure 3  the architecture of the WELSA  [14]  system is presented. The system then enables to present and recommends a variety of learning contents adaptively towards each of the student's learning style identified in the student model through the adaptation model. The result of that analysis is called domain model. AEHS MATHEMA architecture is based on Web technologies similar to client/server distributed computing architecture of MIT and it is mainly supported by Apache Tomcat Server 5.5 and MySQL 6.0 relational database server. For supporting features, it uses the (JSP) pages, Java servlets, JavaBeans, JavaScripts, and Apache Struts technologies. The Apache Struts is a formalized framework for the architecture style design pattern Model View Controller (MVC). Figure 5  shows the architecture of the AEHS MATHEMA. The didactic model uses the information contained in the learner model to provide the two basic services, namely, adaptive presentation and adaptive navigation. The model that supports the AEHS MATHEMA is the overlay model. The characteristics that the system maintains in learner model for each learner are: name, gender, username, password, learning style, level of knowledge for each cognitive goal (general performance), level of knowledge for each basic meaning of the cognitive goal (quantitative and qualitative characterization), concepts that have been successfully evaluated for each cognitive goal, current page, current teaching method, current navigation technique, navigation history, meta-adaptive navigation status, initial Web experience and knowledge level (pre-existing knowledge) in the current cognitive goal, ways of presenting feedback messages, preference for assistance by Advisor in terms of its navigation, course sequence, and phases of the guided dialogues in interactive problem solving process in which each learner participates. The Student Model is transparent to the learner (open learner model) and is verifiable by the learner, that is, the learner can access his/her model and change the content of some characteristics. Figure 6  shows a snapshot of the page showing the characteristics of the learner model where the learner can be informed about them but also modify them. This is an innovation on the architecture of AEHSs. Figure 7  shows a snapshot of the meta-adaptation result. Figure 7 . A snapshot of a meta-adaptation result. Figure 9  shows a snapshot of the page responsible for selecting preferences of the learner. Following a study  [13]  conducted on the formation adaptive groups in the MATHEMA, it was decided that: when the system creates a priority list of candidate peers for an interested learner, in the first and second position of the priority list, the algorithm will place the candidate collaborators with a concrete or abstract style and with the same active or reflective dimension of their learning style as follows: If the learner has a concrete learning style, then the algorithm will place the candidate collaborators with a concrete learning style in the first position, and in the second position, the candidate collaborators with an abstract learning style. If the learner has an abstract learning style, then the algorithm will place the candidate collaborators with an abstract learning style in the first position, and in the second position, the candidate collaborators with a concrete learning style. After creating the list, the system informs the learner that his or her most important candidate collaborator is at the top of the list, while the less important is at the end of the list. Figure 10  shows a snapshot of adaptive group formation in the MATHEMA. Regarding adaptive and intelligent techniques of the AEHS MATHEMA mentioned above, the students having evaluated them consider that the adaptive presentation of different teaching strategies is quite useful (93.0 percent) and fairly easy to use (86.0 percent), the different ways of navigating are quite useful (100.0 percent) and handy (90.7 percent), the meta-adaptive navigation is quite useful (88.4 percent) and fairly easy to use (86.0 percent), the adaptive group formation of collaborative teams to assist them in collaborative problem solving is useful (83.7 percent), but less user-friendly (58.1 percent). This article presents the trends of architecture of AEHSs so far and then an extensive description of the architecture of the AEHS MATHEMA. Most of AEHSs presented in this paper are based on JSP, Java servlets, and Javabeans technologies to implement their functions, as implemented by AEHS MATHEMA. The architecture of AEHSs becomes more complex as more and more functions are implemented. MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and metaadaptive navigation, interactive problem solving, and adaptive group formation. In order to support all these functions the architecture of the MATHEMA is more complicated from other AEHSs. The key idea is the decentralization of their functions.
paper_21	 The use of this efficient local search had yield to determine the error correcting capability of many BCH codes of length 1023 and 4095. In telecommunication and storage systems, the fundamental problem is the reproduction at one point exactly or approximately the selected data at another point. An efficient solution of this problem is the use of error correcting codes. The error correcting codes improve the reliability of such communication, notably on channels that are subject to noise, by adding redundancy in data. The error-correcting capability of these codes is directly related to their minimum distance. The remainder of this paper is organized as follows: The next section presents the main related works. The section 3 presents the proposed scheme ZSISMP. The section 4 presents the main results. The conclusion and possible future directions of this research are outlined in section 5. This section summarizes the most important ones. In  [9] , Augot, Charpin, and Sendrier presented an algebraic system constructed from Newton's identities. The existence of solutions to this system is a necessary condition to the existence of codewords of weight w in the code. The use of this method has finished the table of BCH codes of length 255. [15] . This approach consists in adding to all-zero codeword a level of noise and considering the minimum distance as the smallest level of noise from which the Soft-In decoder fails in correction. This section presents the proposed scheme for finding the lowest weight in BCH codes. The table 2 summarizes the obtained results. It shows that the proposed scheme greatly passes the MIM-RSC method. This table shows the height capacity of the proposed technique to find a minimum weight codeword.
paper_31	 According to China National Health and Nutrition Big Data Report 2018, 20% of Chinese suffer from chronic diseases, and chronic disease mortality was 86%. In-depth interviews with 91 employees in different positions from 11 grassroots medical and health institutions since July 2019 show that the efficiency of mobile medical care in the surveyed areas has appeared  [2] . Since the end of last century, some Chinese hospitals and universities have carried out some researches in this field. In the 1950s, American scholar Wittson had used two-way TV systems in medical field. At the end of the last century, the rapid development of semiconductor technology drove the revolution of information technology. This is the second stage of the development of WITMED. This is the third stage of the development of WITMED. Zigbee has been implemented on the Health Care Profile. There are some defects of the current WITMED (Wise Information Technology of med) system as follows: (1) The inadequate reliability of data transmission. (2) The problem of power consumption. Chronic diseases patients have to be long-term monitored, however, whether using a WIFI or GPRS, high power consumption will be arouse. (3) The problems of unitary monitoring data. Chronic diseases patients are always required to be monitored for multiple vital signs simultaneously. (4) The problem of data processing. The current WITMED system focuses only on data acquisition instead of processing uploaded data and returning the result. According to the Research Report on market development and investment trend of China's smart medical industry from 2017 to 2021, the annual compound growth rate of smart medical market reach 29.6% from 2015 to 2020, and the market scale will exceed 50 billion yuan in the future. The system architecture diagram is as follow:  Fast and correct data acquisition is the basis of the platform's efficient operation. The process of reading and transmitting data is a loop. The related knowledge base in  Figure 3  can be regarded as an expert system. The general technique route of the algorithm is as follows: Propose a comprehensive sorting algorithm, on the basis of semantic matching and a first-come-first-served principle. First, match the Word segmentation information by queued delivery with the taglib of the analysis system. If there are X words appearing in the taglib and N-X words not appearing in it, the matching degree of the information is (N-X) / N * 100%. The general algorithm of this part is as follows: Define priority function: f x, y ax by ε, with a basic definition as follows: x: semantic matching degree, 0≤x≤1 y: a request arriving at the serial position, 0<y≤1 a is defined as the weight of semantic matching degree, 0<a<1 b is defined as the weight of serial position of Request arrives, 0<b<1, a+b=1 ε is defined as the disturbance value, 0≤ε≤0.1, with the default as 0. With the definitions above, f x, y can turn to be: f x, y ax 1 a y ε , namely f x, y y a x y ε . Generally, a is close to µ= 1 2 , formula_0 Therefore, the process of queuing algorithm based on Semantic matching is as follows: With the hypothesis that a= 1 2 (to take one half of the sum) x= 5 10 (there are 10 participles in semantic information, of which 5 are matched), y 1 (Before a new message arrives, assumes that the request queue has 10 participles waiting, then: f x, y y a x y ε =0.294, therefore, the priority value of the message is 0.294. An improved SVM calculation method is as follows: Definition 1 (conception): Define a tuple F= (X, Y, Z), in which x is the collection of all objects is the object property set, Z (X*Y), Z is a subset of X*Y. Definition 2: let {X1, Y1}, {X2, Y2} is concept form of {X, Y}, {X2, Y2b {X1, Y1}. Definition 3: The concept similarity is calculated as:  formula_1 Patients' health information involves their privacy. AES encryption process is shown in  Figure 4 : (3) column Mixed Operation The column mixed transformation is realized by matrix multiplication. The state matrix after row shift is multiplied by the fixed matrix to obtain the confused state matrix. The column mixed operation formula is shown as  Figure 6 : (4) key plus calculation Key plus calculation operates the 128 bit key is XOR bit by bit with the data in the state matrix. The process of key plus calculation can be regarded as the result of Bitwise XOR or byte level or bit level operation. The calculation is shown as  Figure 7 :   After logging in the system, enter the sign input interface and put on the sensor, the system will automatically input the relevant data collected, as shown in  Figure 8 : After inputting the data of relevant signs, the system will automatically generate diagnosis & treatment files and health assessment according to the relevant data. If the signs are abnormal, a warning will pop up, shown as  Figure 9 : The system can also display personal health data in the form of reports and trend charts, as shown in  Figure 10 :  Simulation and test show that the method adopted in this paper is correct. Data acquisition in  Figure 8  forms the health records in  Figure 9 . Through the big data platform, the system intuitively displays the backstage analysis data in the form of reports and trend charts as shown in  Figure 9 .
paper_38	 The study adopted the descriptive research design. Pearson Product Moment Correlation (PPMC) Coefficient and Multinomial Logistics Regression (MLR) were the statistics used to answer the four research questions used. It concluded that the use of OL and UTME as instruments is not enough to select candidates for admission and therefore recommended that other instruments such as senior secondary school mock examinations need to be included as part of the entry requirements in the admission criteria. Education is an essential issue regarding the development of any country in the world. It is a progressive development of knowledge and skills of examinees through stages of teaching and learning at various levels  [1] . The number of undergraduate population in Nigerian Universities has increased from 103 in 1948 to an estimated population of 600,000 in 2018  [4] . There have been various perspectives presented by researchers and psychologists about what is academic performance and its importance. Formula 1 is used for calculating the CGPA. formula_0 Most universities in Nigeria have been using 5.0 as their perfect CGPA score. The rest of this paper is organised as follows: section 2 is the review of related literature, section 3 presents the methodology, section 4 discusses the results obtained, and the last section presents the conclusion and recommendation. The objectives of this study are to: i. The following research questions directed the study: 1. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? What is the relationship between OL results, UTME scores and Post-UTME scores of students and their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? 4. How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? Performance as defined by  [7] , is an observable or measurable behaviour of a person or an animal in a particular or experimental situation in which the authors further explained that performance measures the behaviours within a specific period. The underlying assumption made in such selection is that those admitted by satisfying the admission criteria will be successful in the successive academic activities attached to their studies. Wide disparities have cited between UTME and PUTME scores and the progress/performance of students especially those with exceptionally high UTME scores. The authors tested their nine hypotheses using an independent samples t-test and two-way analysis of variance. The article recommended that JAMB should embark on a more realistic review of the content of the UTME to enhance its predictive validity. The author further recommended the need for the PUTME exercise to be strengthened to have a fruitful admission of candidates. One major shortcoming of virtually all the studies reviewed is their scope in time and spread/coverage. This section discussed in detail the methodology employed such as research design, sample of study, instruments used, the procedure for the collection of data and data analysis. The correlational design is also sub-divided into explanatory and predictive research designs. The Faculty of Science consists of nine undergraduate B. Sc. Full-time degree programmes: Biochemistry, Biological Sciences, Chemistry, Computer Science, Geography, Industrial Chemistry, Microbiology, Mathematical Sciences, and Physics. The sample distribution is as shown in  Table 1 . The UTME was wholly multiple-choice objective questions conducted via Computer-Based Tests (CBTs) by JAMB. The semester examinations were mostly essay type questions. The stanine grades in the OL results obtained at either NECO or WAEC were collected and coded as shown in  Table  2 . The total score for five relevant subjects in OL is then computed and coded together with the UTME and PUTME scores which are as shown in  Table 3 . The coding for the CGPA is also shown in  Table 4 . SPSS is an acronym for Statistical Package for Social Sciences, but now it can also be referred to as Statistical Product and Service Solutions. It was used in this research study. Since the focus of the study is to determine the predictive validity of OL results and CGPA scores (OL-CGPA), UTME and CGPA scores (UTME-CGPA), and PUTME and CGPA scores (PUTME-CGPA), the statistics employed on the extracted data were Multinominal Logistic Regression (MLR) and Pearson Product Moment Correlation (PPMC) coefficient. There are three types of linear relationship that may exist between these two variables namely positive linear correlation, negative linear correlation and no correlation. The results of the data analysis are presented in tables according to the research questions that guided the study in this section. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? For the Computer Science programme, as shown in  Table 5 , the correlation coefficient indicated a low negative correlation in UTME-CGPA (-0.092) and PUTME-CGPA (-0.001) and a low positive correlation for OL-CGPA (0.072). Similarly, there exists a low negative correlation in UTME-CGPA (-0.082) and PUTME-CGPA (-0.038) and a low positive relationship in OL-CGPA (0.089) for the Mathematics programme. In the Physics programme, there exists a low positive relationship in OL-CGPA (0.016), PUTME-CGPA (0.028) and a low negative relationship in UTME-CGPA (-0.031). How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? Likelihood Ratio Tests is a statistical test of the goodness-offit between two models. The traditional 0.05 criterion of statistical significance was employed for all tests in  Table 7 . PPMC was used to analyse the data for this research question. The summary of  Table 8  shows the correlations of OL and CGPA scores, UTME and CGPA scores and PUTME and CGPA scores for all the programmes used for the analysis. In the 2010/2011 academic session, the correlation coefficient indicated a low positive correlation in OL-CGPA (0.198), UTME-CGPA (0.189), and PUTME-CGPA (0.232). In 2011/2012, OL-CGPA (-0.114) and PUTME-CGPA (-0.043) display low negative correlation while UTME-CGPA (0.114) has low positive correlation. In 2012/2013, UTME-CGPA (-0.363) and PUTME-CGPA (-0.123) have low negative relationship whereas OL-CGPA (0.111) has a low positive relationship. In 2013/2014, there exist a low positive association for OL-CGPA (0.061), UTME-CGPA (0.056), and PUTME-CGPA (0.038). The OL-CGPA (-0.071), UTME-CGPA (-0.090), and PUTME-CGPA (-0.040) in the 2014/2015 session signifies a low negative correlation. How well do OL results, UTME scores and Post-UTME Sa'adatu Abdulkadir and Francisca Nonyelum Ogwueleka: Predicting Students' First-Year Academic Performance Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria scores of students predict their first-year CGPA across the academic session, 2010/2011 to 2014/2015? Table 10 , the traditional 0.05 criterion of statistical significance was also used. The authors in  [21] , in their study also recommended the need of potential researchers to compare the OL, UTME and Post UTME terms and scores across Nigerian Universities for standardisation and a model for educational development in the twenty first century.
paper_57	 The aim is to find the shortest path. The probability p ij (t) of choosing the particular arc (i, j) depends on pheromone τ ij  (t)  and the heuristic η ij values which are associated with the arc (4). Disadvantages of ACO algorithms are (i) many user parameters and (ii) the selection pressure. Genetic algorithms (GA) were proposed by  Holland (1975) . The four main components of GA are representation (i), mutation (ii), crossover (iii) and selection (vi) mechanism. In ACO adaptation the first and the last node is excluded from mutation. For feasibility reason the replacement node n r (new gene) is such a node from the node n i neighborhood N i , to which an arc from n i predecessor n p to n i successor n s exists  (Fig. 2) . If more such nodes occur, random selection is applied. 3 ). If more of such nodes exist, random selection is applied. At first mutation is applied. It is applied on random selected tour T k (t) in random selected node. If mutation is not feasible, another node is chosen. If more candidates by which the selected node can be replaced occur, the new node is random chosen from the candidates. If mutation fails on all nodes of the tour, another tour is chosen. Parent strings are random selected. If no tour has common node with the first selected tour, another first tour is selected and the random selection process is repeated. 4) . The task is to find the shortest path between start node n s = 1 and end node n e = 80. For each setting 500 trials were performed. The results received with GO are better almost in any case. For better results representation three graphs are provided. 5 -7) . 5 ). 7) . The highest value 13% was received with four mutation paths with three mutation operation per path. The results vary  (Fig. 9 ). Limit of crossover is 60% of crossover rate. Mutation operation causes better results than crossover operation. Without crossover operation distributed mutation operation has better performance, but with two crossover pairs concentrated mutation operation on less paths tends to perform better.
paper_78	 Each formation has its own composition, structure, distribution range and environment, and all of them should be protected effectively. Given a finite set of data matrix (species × plots), the method calculates a list of C cluster centres  formula_1 formula_2 Where i = 1, 2, …, N = the number of plots; j = 1, 2, …, C = the number of clusters; U = {U ij } = the matrix of membership values, U ij is the membership of plot i in cluster j; V is a matrix of cluster centers; m is fuzzifier which determines the level of cluster fuzziness (1 ≤ m < ∞). We can use U to identify the relationships among plots and communities directly. Based on a general survey of Glycyrrhiza uralensis and its community distribution, five study regions, Chifeng (in inner Mongolia), Hengjinqi (in inner Mongolia), Minqin (in Ganshu), Aletai (in Xinjing) and Kashi (in Xinjing), were selected as sampling sites  [7] . Glycyrrhiza uralensis + Stipa bungeana. Its disturbance intensity is medium and heavy. The community has a total cover of 75%, a shrub layer cover of 20% and an herb layer cover of 65%. spinosa, Lespedeza darurica, Pedicularis resupinata, Potentilla anserine, Saussurea epilobioides, Artemisia sacrorum, Artemisia mongolica, Cynanchum hancockianum, and Vicia amoena. Glycyrrhiza uralensis + Polygonum bistorta. Its disturbance intensity is medium and heavy. The community has a total cover of 70%, a shrub layer cover of 5% and an herb layer cover of 70%. The common species are Oxytropis myriophylla, Polygonum divaricatum, Adenophora gmeliniia, Potencilla acaulis, Suaeda prostrate, Astragalus melilotoides, Allium condensatum, Artemisia ordosica, and Oxytropis grandiflora. Glycyrrhiza uralensis + Ephedra przewalskii + Cancrinia discoidea. Its disturbance intensity is medium and heavy. The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 60%. The common species are Caragana korshinskii, Elaeagnus, mooceroftii, Suaeda prostrate, Artemisias phaerocephala, Saussurea laciniata, Saposhnikovia divariicata, Oxytropis glabra, and Artemisia ordosica. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 59500 ha -1 . Its disturbance intensity is medium and heavy. The community has a total cover of 65%, a shrub layer cover of 10% and an herb layer cover of 60%. The common species are Salicornia Bigelivii, Carex duriuscula, Stipa sareptana, Artemisias phaerocephala, Alopecurus pratensis, Saposhnikovia divariicata, and Carex pediformis. Its disturbance intensity is heavy. The community has a total cover of 60%, a shrub layer cover of 5% and an herb layer cover of 58%. The common species are Cleistogenes squarrosa, Caragana pygmaea, Hordeum brevisublatum, Ephedra sinica, Achnatherum sibiricum, Artemisia frigida, Viola tianschanica, Carex duriuscula, and Alopecurus pratensis. Glycyrrhiza uralensis + Astragalinae triloa + Stipa sareptana. Its disturbance intensity is heavy. The community has a total cover of 75%, a shrub layer cover of 10% and an herb layer cover of 70%. The common species are Artemisia scoparia, Kochia prostrate, Potencilla acaulis, Artemisia frigida, Ceratoides lates and Atraphaxis frutescus. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 6000 ha -1 . Its disturbance intensity is heavy. The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 65%. The common species are Caragana pygmaea, Astragalinae triloa, Stipa parpurea, Festuca logae, Artemisia kaschgarica, Polygonum viiiparum, Ephedra equisetina, Glycyrrhiza inflate, and Alyssum desertorum. Glycyrrhiza uralensis + Festuca logae + Stipa sareptana. Its disturbance intensity is heavy. The community has a total cover of 80%, a shrub layer cover of 10% and an herb layer cover of 75%. The common species are Artemisia parvula, Scorzonera divaricata, Roegneria kamoji, Potentilla bifurca, Carex duriuscula, and Ranunculus japonicas.
paper_96	 This humane technology innovation is the specific embodiment of environmental science and technology aesthetic theory in the field of scientific and technological innovation. It has a positive and promoting role to the development of transportation and blind-man welfare in China. During last summer, I went blind orphanage as a volunteer, and personally experienced the inconvenience and hardship of the blind when they travel. Especially when I heard the news that a blind man had been hit and killed when crossing the street just several days before because of his blindness, my compassion was once again inspired, and I was very eager to do my best to help the blind. The research on "warning system of embedded electronic guide dog avoiding obstacles" in this paper is an important part of intelligent electronic guide dog system, the study of which is mainly based on the embedded system, anti-collision warning system, and the USB technology is applied to warning system of embedded electronic guide dog avoiding obstacles. The design of obstacle-avoiding early warning system of embedded electronic guide dog is an inheritance and innovation, based on the design of traffic information collecting system, which is generally used in vehicle anti-collision at present. The overall design plan of obstacle-avoiding early warning system of embedded electronic guide dog mainly is the velocity and distance measurement module, the power circuit module, the LED screen show module, USBcommunication circuit module, and clock circuit module. The hardware of the obstacle-avoiding early warning system of embedded electronic guide dog can reference the Figure 1:  Figure 1 : Schematic diagram of hardware system structure  [4]   In the anti-collision early warning system, referenced in the design of obstacle-avoiding early warning system of embedded electronic guide dog, there mainly exits four steps, that is, information collection, information processing, information judgment, and warning information. The techniques of information collection are ultrasonic wave, laser, infrared ray, machine vision and interactive method. Presently, the types of the generally used anti-collision warning systems mainly are radar anti-collision warning system, ultrasonic anti-collision warning system, laser anti-collision warning system, infrared anti-collision warning system, machine vision anti-collision warning system, and interactive intelligent anti-collision warning system. The selection of anti-collision warning system for the design of obstacle-avoiding early warning system of embedded electronic guide dog should start from the characteristics of the highway network and the street network construction in our country, combing with the characteristics of the obstacle-avoiding early warning system of electronic guide dog, as well as the construction of our country's highway and street traffic integrated management system. The full name of ARM is Advanced RISC Machines. The ARM architecture follows the principle of reduced instruction set computer (RISC). It extensively uses the registers with a fast speed of instruction execution. Thereinto, the commercial RTOS includes WINCE and VxWotks; while the free RTOS has Linux (uCLinux and RT Linux included) and uC/OS -II. UCLinux aimed at the micro-control field, designing the Linux system, which is specially designed for the CPU without MMU (such as the S3C44B0X adopted in this project), and it has done a lot miniaturization work for the Linux kernel. These are the most remarkable features that uCLinux owned. The reference model is shown in  Figure 2 . This humanized technology innovation is the embodiment of environmental science and technology aesthetics theory in the field of science and technology innovation practice.
paper_134	 Data mining, also referred to as knowledge extraction from databases, is one of the most important analytical methods for identifying the relationships between the various elements of the information collected in order to discover the useful knowledge and support of strategic decision-making and sustainable development systems in various industries. Data mining takes advantage of the progress made in artificial intelligence and statistics. In recent years, nondestructive methods have been considered in purification. The purpose of the development of these methods is to estimate the quantitative and qualitative characteristics of the materials rapidly, non-destructively and reliably  [4] . Based on the dielectric method, when a material is placed in the alternating electric field, the positive and negative charged particles in it will constantly tend to move in the electric field. When in a country where milk is eaten up by 30 percent as a result of a rise in the price of a few hundred tons, what else would you expect from a 60 to 80 thousand Riels per liter consumption of lubricants  [9] ? The great benefit of mixing vegetable oils with lubricant makes it hard to find the type of product. The second pertain to data mining algorithms; third part related to samples and used methods in the article. Lizhi et al. The results of the PLS calibration model showed good results for the adulterated oils with soybean oil as a counterfeit oil. Reggie et al. (2006) predicted egg quality parameters using its capacitive properties. Experiments were carried out on the day of laying, on the third day, on the sixth day, on the ninth and twelfth days after the laying. Soltani et al. The correlation coefficients and the mean square error of the statistical indexes were 0.98 and 0.0006, respectively. 70% of the data was for network training, 15% for validation and 15% for the network testing. According to the researches that carried out in relation to the determination of the quality and content of agricultural products and food industries, it can be concluded that different methods have been developed to counteract the adulteration in these products. In this study, a recent study has been carried out to identify the authentication of olive oil. Samples of olive oil provided from Khorramshahr Oil Company and produced at Rudbar oil plant located in Manjil. The samples of sunflower oil, canola oil and corn oil which are known as adulterated oils were also obtained from national markets. The dielectric experiments started on the dielectric parameters of olive oil one day after the preparation of the sample. The device used consists of the Arduino board, ICL8083 and AD8302. (Figure 1 .) One of the new ideas in identifying and categorizing patterns is a backup vector machine or SVM. Backup machines have very valuable properties that make it suitable for pattern recognition. In particular, SVM does not have the problem of local optimization in its training, it builds the categorizer with maximum generalization, determines its structure and topology optimally, and functions nonlinear differentiation easily with low computing using It forms the concept of an inner product in the Hilbert space. In this study, coarse function was used to regression test data. Using this technique, the amount of RMSE for the samples were equal to 3.56, 2.45, 3.05 and the amount R 2 was equal to 0.92, 96 and 94, respectively. According to the results, Most R 2 related to Olive-Canola (0.90) and the least amount of R 2 pertain to Olive-Sunflower (0.86). The results were predicted and modeled using regression methods. A pair of matched logarithmic amplifiers provides the measurement, and their hard-limited outputs drive the phase detector. In this study, Quadratic function was used to regression test data. Figure 2  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn Using this technique, the amount of RMSE for the samples were equal to 3.65, 2.46, 3.31 and the amount R 2 was equal to 0.92, 96 and 93, respectively. Also device used can classify samples and use for other oils.
paper_139	 Much effort made towards in investigates mechanism with identical detection algorithms, still the retrieved web documents with outmodedlink. Using this KTMIN-JAK-MAXAM algorithm accessing of web pages with reduced time and space complication. Retrieving relevant information from web without redundancy is more challenge task nowadays where in web mining communities  [3, 4] . Utilizing customary data recovery  [5]  and information mining systems it get to the known and obscure data from the Web content. Web mining is categorized into three group Web Content mining  [6] , Web structure mining, Web usage mining. Traditional web mining algorithms handle with structured document  [7] [8] [9] [10] [11] [12] [13] [14] [15]  than the advanced methodology of mining algorithm can dealthe entire heterogeneous document comprises of images  [9] , graphs, videos  [16] , etc. Once the document is preprocessed, Normalization of tokens is generated to further process the web content document. Initially calculate measure for all the vertices and maintain the set U which contains a minimum and maximum degree for all vertices and isolated measure vertex. Repeatedly include the minimum measure in the set-up and each measured vertex included only once in the set-up. After applying the above steps the entire vertex without redundant information available in the set U. Pseudo Code forThe Proposed Algorithm KTMIN-JAK-MAXAM Step1: Compute degree measure for all vertices in the setup. Step2: Pick the minimum degree vertex 'v' in the set-up and include in the set U. Step3:While U doesn't include all vertices Step3A: Include the entire isolated vertex which is adjacent to the vertex 'v' to U. Step3B: Find the adjacent vertex 'u' to 'v' which is not in U and has maximumdegree. Update 'u' to U. Step3C: Update the value of degree for all adjacent vertices of 'u'. Iterate throughall adjacent vertices if possible. 4.2 Search the node B, the unvisited adjacent node is from B as 2 and 7. 4.3 Now travel around the node 2, the unvisited adjacent node is from 2 as 1 and 4. 4.4 Discover the node 1, the unvisited adjacent node is from 1 as only 3. Also we observed derived linked graph need not be unique but this approach will provide the optimized cost analysis report in future in data science field.
paper_145	 Among them 44.3 percent were overweight and obese. Overweight and obesity were more among urban residents compared to rural residents and they were thirty two percent more exposed to overweight and obesity. The major contribution of obesity is to lead the increase in the prevalence of chronic diseases and cancers  [7] [8] [9] [10] . The most common medical morbidities associated with obesity include impaired glucose tolerance and metabolic syndrome  [11, 12] . It had been observed in some research findings that youth who do not meet guidelines for dietary behavior, physical activity and sedentary behavior have greater insulin resistance than those who do meet guideline  [12] . For this reasons, World Health Organization considers the epidemic a worldwide problem which requires public health intervention  [13]  that act on different factors associated with overweight and obesity as well as technological changes that have lowered the cost of living of the people so that people can avail sufficient food with required protein. Efforts are needed to improve the economic, political, social and environmental conditions so that congenial atmosphere prevails in the society for maintaining healthy life of the people. Even government and public health planners remain largely unaware of the current prevalence of obesity which is the cause of many diseases  [5] . The data were collected through a pre-designed and pretested questionnaire during the months of May and June, 2015 by some undergraduate and post graduate students of American International University-Bangladesh, most of whom were doctors and nurses, of the department of Public Health and they were associated with public health services. The questionnaire contained questions related to sociodemographic characters of each person. Questionnaire also contained questions related to the stage and type of diabetes, treatment stage of disease, pre-cautions against the disease and the stage of complications due to the disease. The analysis was done by using SPSS [version 20.0]. The level of obesity was measured by BMI [weight in kg /(height in m)  2  ] and it is a most commonly used measure of level of obesity  [18] . The respondents were classified as underweight  [BMI < 20] , normal [BMI,  20 -25] , overweight [BMI < 30] and obese [BMI 30+]. Factor analysis was done to identify the factors for obesity. Obesity was observed among 15.3 percent people and overweight respondents were 34.0 percent. There were 58.9 percent males among the respondents and 47.2 percent of them were normal. The corresponding figure among females were 37.3 percent. But more Muslim respondents (43.8%) were normal compared to Non-Muslim respondents (38.8%). The lower income (< 20,000.00 Taka) group of people were more (34.2%) and 48.4% of them were normal [  Table  8 ]. Obese group was also more (30.3% 0) among them. Significant association was noted between the level of obesity and the level of income [P(χ2 ≥ 64.994) = 0.00]. Again, those who were not doing any physical labor (23. Among the smokers 47.2 percent were normal and 37.2 percent were overweight. Table 11  Among the smokers 47.2 percent were normal and 37.2 percent were overweight. This was done by factor analysis. The inclusion of the variables in the analysis was justified as communality of no variable was less than 0.4  [22] . The significant multiple regression analysis using one of the included variable as dependent variable and others as explanatory variables also justified the inclusion of the variables for factor analysis. The results related to the justification of inclusion of variables were presented in  Table  12 . So, the inclusion of variables were satisfactory. These three variables were more important for the variation in the level of obesity. The factor analysis extracted 5 components as these components explained 73.309 percent variation in the observations of obesity. The coefficients of the components were presented in  Table 13 . From the factor analysis it was noted that the coefficients of the variables gender was highest followed by occupation, education and type of work. These coefficients were observed from the first component. This information were noted from the characteristic roots of the correlation matrix of the variables, where the roots were 2.573, 1.616, 1.086, 1.053, and 1.003. The selection procedure was a convenient sampling plan. Around 50 percent respondents were overweight and obese. Around 50.6 percent people of urban area were overweight and obese. They were classified as underweight (BMI <20), normal (BMI, 20 -<25), overweight (BMI, 25 -< 30) and obese (BMI = 30+). Most the respondents were in normal and overweight groups. Among the respondents 84.1 percent were of the age 40 years and above and among them 42.8 percent were overweight and obese. Again, prevalence of diabetes was more among these groups. This finding is similar to that observed in both home and abroad  [23] [24] [25] . The factor analysis showed that sex, occupation, education and type of work were more important to explain the variation in the level of obesity. The incidence of obesity cannot be avoided, but its prevalence can be reduced by implementing appropriate action plan. These are: a) Halt the rise in body weight by encouraging people so that they can take healthy home made food and avoid restaurant food / first food. This is for the in service, private or government, people. People may be advised to walk daily for at least half-an-hour. The public health authority can play a decisive role for the above steps.
paper_212	 Among other countries in the world, Kenya is among the twenty two that account for 90% of expectant women living with HIV. Among the expectant women there are 13,000 new HIV infections among children. The number of those that died account for 7% of the global total. Sex workers have the highest percentage of HIV prevalence at 29.3% according to the Kenya HIV estimates report-National Aids Control Council. There are various questions still left unanswered to date on the HIV epidemic. This model approximated that there would be 6000000 cases of HIV and 400000 cases of AIDS in China if there were no forms of interventions implemented. The group most affected would be the 31-40 years group. The group comprising of the largest individuals predicted to be living with HIV/AIDS was approximately 650000. There is need now more than ever to develop the SIR model since its application is going beyond epidemiological application such as how cues influence behaviour in a social setting and the spread of ideas  [5] . In several concentrated epidemics, HIV prevalence estimates do not match reported cases and mortality estimates do not match reported deaths, even after adjusting. There are issues estimating prevalence in high risk groups and the size of high risk groups. They considered and analyzed a two stage SI model that allowed for random variation in the demographic structure of the population with the population size changing at different times which had an exponentially distributed rate of infection. The parameter depended on the varying population size N. This meant that both the population size varied as well as the transmission/contact rate. The initial conditions changed over time and demographics not being included such that change over time was described as; (1) formula_0 The Kermack-McKendrick theory was later developed to a version where they tackled the problem of endemics  [14] ,  [15] . This transformed the theory to the basic SIR model such that when demographics were included becomes (4) (5)  (6)  where N denotes the total host population. denotes the birth rate and death rate denotes the infection rate denotes the recovery rate t denotes time point Model development The Gillespie algorithm was used to simulate a statistically correct trajectory given initial SIR conditions. The model explored how altering transmission dynamics affected the model as a whole. The death rates were distinguished such that one death event led an individual out of the model while the other death event led an individual into a different classes. All these aspects determine the quality of the inference drawn. denotes the rate of birth denotes the rate of non-AIDS death denotes the rate of infection denotes AIDS death rate denotes model's time step Gillespie's procedure The Gillespie simulation procedure was developed to produce a statistically correct course for finite well-mixed populations  [16, 17] . The compartments consist of initial state values S(t 0 ), I(t 0 ) and R(t 0 ) are contained in a vector and described at initial time t 0 . , , This Gillespie's algorithm based stochastic SIR model generates a statistically correct trajectory from the initial vector as , , where i=s, i, r S+I+R=N i denotes the population size of the state at time t denotes a function characterized by two quantities as a state change vector and a propensity function. , the state change vector defined as , , where is the change in state i caused by one event. Assuming that the resulting state is . A Markov chain model is one where the probability of the next event depends on the probability of the present state. A Markov chain is interpreted here then, as a stochastic discrete-valued model with the Markov property that future states of a process depend on the current state. The transition probabilities assigned are defined on an open interval (t, t + ), such that the probability an individual moves from the susceptible compartment to the infectives compartment is  [ 1 ] . The data was obtained from NACC for HIV/AIDS cases. Considering the hypotheses, 4 = If the mean and variance of the simulated and natural data are equal, the simulated mean does not fit the data. 4 5 = If the mean and variance of the simulated and natural data are not equal, the simulated data fits the natural data. A stochastic SIR model was simulated with a mean step size of 0.006336446. Variables in the model were S = 3507162, I = 45820, R = 4597, parameters in the model are crude birth rate of 0.06, non-AIDS death rate of 0.025, transition rate of 0.1 and AIDS death rate of 0.48. Curves produced are illustrated below. Therefore, the conclusion is that the simulated data model fits the natural data model. The simulated curves were compared to HIV/AIDS data. Recommendation for research would be to explore other variations of the SIR model such as SI, SEIR under Gillespie's algorithm. The SIR model as well as Gillespie algorithm could continue to be applied other areas such as viral marketing and behavioural science as has already been done successfully.
paper_214	 It is the only method that can be applied to unstructured problems. Quantitative mathematical models are embedded in base models, managed by subsystem a management model that requires separate users from the physical aspects of data processing and storage that extract, create, delete and modify models. Evaluation of search results depends on the method of presenting results and depends on the facilities of component dialog with users that provide inputs. Solving the result of communication between all stakeholders, sharing the general manager responsibilities both at decisionmaking levels and the corresponding subproblems defined. For example, after the first phase, the scope may relate to excessive spending decisions of a functional department, inventories too high or adoption of a draft research and evaluation on the introduction of computers  [3, 4] . Modeling takes expression of reality by means of abstract entities possessing quantitative and qualitative attributes. The assessment in turn depends on the search method. Methods called heuristics, based on a thorough analysis of the issue. Issues raised by the communication solution, accepting the decision or the additional costs of implementation are sluggish, and the decision-maker plays the important role of mediator. Data external economic information circulated nationally and internationally and usually come from the industrial sector of which the company, legal regulations. Personal data is data that relates to the behavioral aspects of decisionmakers in making decisions. The data source, internal or external, data is extracted and managed by a management database. The management of the database depends on the organization of data. In most cases there is SGBS transactional relational data system and a management database for multidimensional data warehouses created. The data dictionary is a catalog of all data from the database. It contains data definitions, data sources and their intrinsic significance. The subsystem management model consists of the following components: base models, the management models, dictionary and processor execution models and integration patterns Base models contain the set of models that make it possible to analyze the facts and the choice of options in terms required by the user. The models are domain-specific and models can be classified into strategic, tactical and operational models models. The models are used currently in operational and transactional system that aims of the organization. Database management system allows creating new models models using programming languages, update and modify existing models, establish interdependencies between models. In building a data warehouse is based on the analysis of data. Design is the stage where the data warehouse model is chosen, depending on the complexity of the system real user requirements and data structure existing in the company (databases, Excel spreadsheets...) Building a data warehouse there are three models: type star, snowflake patterns type and constellation type models. In such a model is highlighted: quantitative data centralized called measures of activity quantitative criteria for centralized aggregation  ,  referred sizes  relational table that stores the measures identified by   the facts dimensions is called table  Tables where aggregation criteria has explicit codes,  called type tables list. Facts associated table. The star is the type of aggregation criteria when codes are explained in type tables list. Data warehouse star The eastern type constellation when several schemes that use the same type star catalogs. The advantages of using databases for Decision Support Systems in businesses involves creating numerous queries by the decision maker and also allow to create many scenarious and variants to choose from. The databases permits flexibility in creating queries by choosing columns from dimension tables and measures from the fact table and so the decision maker may choose what economical indicators is useful to analyse and also permits to create different types of graphs such as columns, pies, scatter points or webs. By simply analysing these reports and graphs helps the decision maker to create different scenarious by changing target indicators and corelate with the economical indicators that can't be changed  [2, 5] .
paper_216	 The Black-Scholes model is a well-known model for hedging and pricing derivative securities. A number of studies have attempted to reduce these biases in different ways. Amongst the non-parametric approaches used to improve the accuracy of the model in this study is the Wavelet-based pricing model. The MSE and RMSE of Wavelet model is 0.208546 and 0.456669 respectively which is much lower than that of Black-Scholes model and therefore in conclusion, Wavelet model outperforms the other model. The financial contracts or instruments which derive their value from some other variables are called Derivatives. The ones traded on the exchange are standardized and regulated. European-style options can be exercised at the maturity date only while American options can be exercised at any time prior to or on the maturity date. American options need more complex pricing methodology due to the extra feature of early exercising. In the Kenyan market, derivatives are yet to be developed. The derivatives market has been inactive due to some of the factors such as low level of investor awareness and sophistication, inadequate risk management, lack of commodities on large scale and inadequate liquidity. The remainder of the paper is structured as follows: Section 2 discusses the existing literature; Section 3 presents an overview of the Black-Scholes model and Wavelet based pricing model. Section 4 describes the data, shows the empirical results and performance measures of the models. A lot of improvements have been done to the original Black-Scholes formula since the paper of  [2] . For out-of-money puts, there is high implicit volatility relative to the at-the-money calls and puts  [3] . In comparison between the risk-neutral MGF and the implied risk-neutral PDF, the risk-neutral MGF has a number of advantages even though between them, there is a one to one relationship. Also, wavelets can be used to improve analysis of volatility since they are a preprocessing denoising tool  [10] . The de-noising ability of wavelets was also recognized in  [7] . Evidence have been provided to support that wavelet based local linear approximation  [11] . Algorithm has a superior performance in de-noising financial data with high frequency by using the first principal component to de-noise frequency variations of a business cycle with wavelets  [12] . Lastly, wavelets can be used to estimate parameters of the models which are unknown using wavelets in pricing of an American derivative security by levy process  [13] . Another example is the application of the wavelets in nonparametrically estimation of a diffusion function  [15] . Where;  formula_0 From the Black-Scholes formula above, we see that the strike price (K), risk free interest rate (r), time to maturity (T), the volatility (") and the current price of the stock (S0) are required. is the cumulative distribution function. ʆ ! ( is the bilateral inverse Laplace transform. The underlying asset dynamics and investor expectation in option prices is captured by the MGF Θ !$ + of the logarithmic returns /0 7 . This needs to be approximated by wavelets. In order to approximate the implied MGF using the wavelet method, one has to choose a particular wavelet from a large family of wavelets. On this criterion, the Franklin hat function performs very well because it is symmetric, smooth and piecewise continuous. This function also emulates the probability density function of asset returns. The focus on this study was on pricing of a European Call option using two models, one parametric model (BS Model) and one non-parametric models (Wavelet Pricing Model). The superiority of the wavelet method comes from the ability of the wavelets to estimate the risk neutral MGF. Moreover, hedging performance may be evaluated with respect to each of the two models.
paper_219	 The QDS attribute brings distinction to the resultant data quality of the network's quality of service. A movement to shift from "platform-centric warfare" to "network-centric warfare" was initiated in the final years of the 20 th century  [1] . Successful policy implementation amongst other factors requires system analysis methods that assist acquisition agencies in targeting limited developmental resources to areas of greatest impact to the overall mission objectives. Traditional QoS expects prioritization has occurred prior to data entering the network. Not all data is the same; some is more relevant to the users' needs when compared across all the data. Section 2 of this paper provides a brief overview of the origins of net-centric and net-ready policies. Admiral Jay L. Johnson, Chief of Naval Operations, stated in 1997 that the military is undergoing "a fundamental shift from what we call platform-centric warfare to what we call network-centric warfare"  [1] . Vice Admiral Arthur K. Cebrowski, U.S. Navy, and John J. Garstka proposed that adoption of network-centric operations across the military enterprise would result in benefits at the strategic, operational, and structural levels and bring forth "a much faster and more effective warfighting style"  [2] . [2] . The access process should be via the "network" using "commonly supported access methods"  [6] . Net-centric operational tasks are those that "produce information, products, or services for or consume information, products, or services from external IT"  [3] . Fig. The most commonly used subjective rating in standards and in conjunction with QoE is the mean opinion score (MOS) where score of 1 is bad, 2 is poor, 3 is fair, 4 is good, and 5 is excellent. Sampling users is the preferred and direct method for measuring QoE. Where an accurate model exists for QoE as a function of that attribute's objective measures then it's used instead of sampling users' opinions. The question arises how to get the users' opinions. But the arithmetic mean assumes that all user groups are of equal size which could lead to biased estimates of the MOS. For a sample of size n consider v strata with sample s k in each stratum, i.e. ∑ i=1 to v n i = n and s = U k=1 to v s k . where n ≤ N and s = {1, 2,…, n}. Where it is not easy to design a stratified sampling plan a stratification post collection of n samples without replacement can be constructed. The sample is organized into their various strata with the number of elements in each total number of strata k, in the total population N. The HT estimator for stratification becomes formula_2 and estimate of the stratified mean is formula_3 where m k = ∑ i in s k y i /n is the arithmetic mean of strata k. With each attribute having its MOS value using the stratified estimate in (3). Traditionally these QoE MOS ratings were undertaken by panels of experts. But cost and time to use panels of experts to assess MOS has resulted in seeking alternative approaches. To produce a subjective rating MOS, or QoE, the ITU-T P.862.1  [13]  is used to map raw PESQ to the final rating. In  [17]  and  [15]  there are equations to estimate the interpretability for still imagery and video, respectively. Over the last several decades there have been many papers on the topic of QoS. But just for completeness a brief explanation of QoS is given here. However there are a number of challenges to QoE discussed in  [19]  and  [20] . 1 ). 2 . overhead, side-view, rear-view, distant, near). Temporal properties of the data pertain to the time of data collection relative to the actions and conditions of the target of interest (e.g. target while in port, target while in open ocean, target when first detected, target after engagement). The processing of tagged relevance involves the review and analysis of the data product to assess its key features followed by the application of specific metadata values. (2) A high user satisfaction for the enterprise data quality can only occur when all three attributes have high QoEs. In game theory, the minimax value of a player is the smallest value that other players can force without knowing the player's actions. The same value is also the largest guaranteed value for that player with knowledge of the other player actions. Formal minimax definition  [24]  is formula_4 where a(i) denotes the actions of the i-th player of n players, a(-i) is actions of all other players except the i-th, and v i is the value function of player i. The cells in  Table 1  consist of a left value which is the games' pay-off for player one and a right value which is the games' pay-off for player two. Rows in  Table 1  represent actions of player one and columns in  Table I  represent actions of player two. formula_5 Thus, for each d in D, starting with d=QDS, a state vector s max (d) is found that produces max s in S(d) f d (s) where f d (s) is a discrete function formed by a predictive model for QoE or in the absence of a model formed from direct user sampled MOS value calculated using (3), i.e. f d (s) = u HT . The x-axis of the chart in  Fig.
paper_241	 Transformers are the key equipment in electrical power transmission. So the use of transformers protection in electric power systems is very crucial and critical as they are required for efficient transportation of electricity to consumers for long term. The system is efficient in transformer protection, gives better isolation, has accurate fault detection and quick response time to clearing faults The demand of electricity in our modern day as led to advancement in electrical power systems, which is reflected in the development of all the power system device generators, different sizes of transformers, transmission lines and the protection equipment. In A. C transmission, power transformer is one of the most important equipment. It is expensive uninterrupted and desired to be kept in good condition always to have supply. Due to advancement in technology and daily use of electrical devices by industries, organizations and individuals, there is an increase in electricity demand which most likely results systems overload, reducing its efficiency and can cause damage to the transformer  [1] . Typically, Power systems are built to allow continuous generation, transmission and consumption of energy. differential current) and can be prevented using differential protection and microcontroller based relay protection. The protective relay techniques provide accurate reproduction of normal and abnormal conditions for correct sensing and operation. The protection techniques employed differential relay mechanism with Arduino. If it finds any error then it sends commands to the circuit breakers to trip the main potential transformer and the buzzer gives an alert. The current sensor acs712x series was used in the project as the interfacing instrument between the power transformer and the pic16f690 microcontroller. The step-down transformer is used to step down the primary or source voltage from 220 to 12V. This 12V is regulated to 5V (using L7805CV voltage regulator) which is the required voltage for the Arduino. Variac is introduced in the system to vary and show the characteristics of the differential protection scheme when differential current is below zero and above zero respectively. The LCD display is used to display the differential current values of the transformer and to display the voltage and current level of the system as well. The following table shows the result from the conditions of the system From the test carried out, it was observed that the current values gotten from the primary and secondary side of the system in Faulty conditions were larger than that gotten in Normal conditions and also the current difference were also larger for Faulty than Normal condition. For the load with both the 200W and 60W bulbs the current values and difference were larger than with each connected separately. At No load the secondary current is very close to zero as a result of open circuit at the secondary.
paper_251	 However different users have different requirements of computational power and application and systems software. Hence demand of users is heterogeneous in nature so that varieties of application (hardware & software) have been developed to achieve the highest user satisfaction. and software tools. frequently not used. According to the Lewis Chunningham  [20]  "Cloud computing is the internet to access someone else's software running on someone else's hardware in someone else's data center". They are able to communicate i.e. social in nature, mobile i.e. autonomous and distributive in nature. They are following with respective functionality in the proposed system 1. 2. 3. i.e. These are following 1. Service scheduling delay 2. Elasticity optimization 3. Better Provisioning of the SaaS 4. The main lacking point in the article  [1]  and  [2]  is validation of the proposed mechanism. Additionally the requirements for such fast provisioning of the cloud has been discuss in the recent year in the article  [3] . Following goals has been achieved or solved with integrating of the Mobile Agent to Cloud Computing service realization 1. 2. For SaaS development Codenvy has been subscribed. 4. Resources: Algorithm for Scheduling (influenced from Aneka) developed onto the Cloudbees Proposed Algorithm Step1: Register user login Step2: Sign in the user login Step3: Select multi monitoring agent in cloud environment (for each subc_request with QoS constraints) Step 4: Mapping of resources between cloud client, cloud agent and cloud service providers. Number of tasks submitted at instant i (Ni) 2. Time to execute the task 3. Availability 4. Hence proposed system performs better than  [2]  and any other methods as shown in figure 3. Proposed mechanism has influences from the working of Aneka framework. The developed java web application (SaaS) has been developed with the help of codenvy SaaS developed platform. 1. Security enhancement using Agent for following attack internal attacks and DoS (Denial of Service) attack. 2. Develop a security perimeter based on anomaly detection using Application Process Management by integrating the mobile Agent on them for the cloud computing paradigm.
paper_272	 Three synchronous generators 6.6KV/8MW each have been connected to three power transformers, rating 6.6KV/1250KVA in refinery power plant. Moreover the switching process associated with power electronics offers many modern benefits as a static switching system when joints together by using a snubber circuits for damping chopping current and transient over voltages or other facilities  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] . This paper focuses on the understanding of both characteristics and load specification relative to vacuum circuit breaker to generate precisely parameters  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] . A more exact name would be metal vapor arc inside vacuum electroplate. J=2*10 6 (A/m 2 ) Joule heating as thermionic emission Where Ø is the thermionic work function. A high frequency current governed by the circuit parameters flows. However pre-striking transient over voltages is less severe than multiple re-ignitions occurring during load-dropping, first because the contact gap at the first prestrike is very small and second because the contact gap is rapidly decreasing rather than increasing with respect of time. Multiple repeated ignitions mean an over voltage magnitude is a straightforward concept: as the amplitude of any overvoltage increases, the probability of breakdown in vacuum or breakdown of solid insulation increases. The process of current chopping is the premature suppression of 50Hz or 60Hz circuit current before normal current zero due to instability of the arcs in a vacuum interrupter  [6] [7] . Time is required for magnetic energy to be transferred from the inductance "transform inductance loads", and for the magnetic field associated with stored energy to collapse. The second transient ends up in a negative loop of current that changes the polarity to positive at just about 480 µs time scale indicates in the figure. Notice that the negative polarity of the voltage loop and the negative direction of the two high frequency transients agree with the negative polarity of the last cycle of the current interrupted. We note that the only parameter involved is η. So that a family of generalized curves can be drawn from equation  4 for different values of η with dimensionless quantity -t`, as abscissa. Where η =0.5, the sine function changes from a circular to a hyperbolic function. However to gain familiarity with these curves, consider a specific example where the inductor current is required in a circuit in which the components have the following values: R=10 5 Ω. L =5 Henrys, C = 2X10 -8 F. These values are typical of unloaded transformer, where R represents the equivalent loss resistance. Suppose Vo =13.8√2 KV So. Zo = √L/C = 5X104 ohms η. = 2. 2. 4. Transient over-voltages (450us -480us-500us). 7. In brief, the majority of transients associated with vacuum interrupter are not just results of the switching device a lone but the interaction between the device and other components of the power system.
paper_294	 The cultural heritage of any people can be redressed through the preserved cultural values tangibly stored and preserved in media and forms retrievable and usable. Cassava has played an important role as a staple crop in the feeding of the Tiv people. Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. The cultural heritage of any people can be redressed through the preserved cultural values tangibly stored and preserved in media and forms retrievable and usable. Cassava has played an important role as a staple crop in the feeding of the Tiv people. The Tiv had different management strategies of postharvest losses of cassava (Akom or Alogo) that includes dried cassava chips (kpor), imyorun or imorun (gari) processing, fermented cassava (akpu) and production of base (mtuhem) among others, which were potent managing postharvest losses of cassava. Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. Specifically, the study sought to: 1. 1. What are the Tiv management strategies for postharvest losses of cassava? 3. They are not restricted to any class of persons in the community but freely available to all. This is their most crucial function of all. Benue State like other African Regions has farmers who cannot read nor write but who need information to develop their agricultural enclaves. Cassava (Manihot esculenta Crantz, Euphorbiaceae, Dicotyledons) is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world, mostly in the poorest tropical countries. Cassava plays an essential food security role because its matured edible roots can be left in the ground for up to 36 months. The crop therefore represents a household food bank that can be drawn on when adverse climatic conditions limit the availability of other foods. Postharvest priorities across the globe have evolved considerably over the past four decades, from being exclusively technical in their outlook, to being more responsive to consumer demand. Postharvest encompasses the conditions and situations surrounding the state of the food after separation from the medium and site of immediate growth or production of that food. The food need not be removed any great distance from the harvest site, but it must be separated from the medium that produced it by a deliberate human act with the intention of starting it on its way to the table. These include: 1. Peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips) 2. Peeling the cassava, slicing, sun drying it and grinding into flour known as Mwem ma Kpor (Cassava Flour) 3. Peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom 4. Slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base) and 5. Data were analyzed using mean and standard deviations. Data was collected using 4 point rating scale instrument. Importantly, on each research question, data were collected on related items in the instrument. The collected data were analyzed using mean and standard deviation. Apparently, any item of the instrument whose mean rating scores was 2.50 and above was considered significant and any item with the mean rating scores below 2.50 was not considered significant. What are the Tiv management strategies for postharvest losses of cassava? The collected data were analyzed and presented in  Table 1 . From  Table 1 , it can be seen that the mean rating scores of all the items are above 2.50. This means that peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips), peeling the cassava, slicing, sun drying it and grinding into flour known as Mwem ma Kpor (Cassava Flour), peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom, slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base), peeling the cassava, fermenting, sieving and squeezing it into lumps known as Akpu, peeling the cassava, grating, squeezing (demoisturizing) it, mixing with salt and other ingredients and frying with palm or groundnuts oil known as Kweesi or Akweesa Akom and peeling the cassava, grating, squeezing and sieving the starch and preparing it for food as Kamu Akom are the management strategies for postharvest losses of cassava. In order to answer the research question, data were collected relating to the research question, analyzed and presented in  Table 2 . The collected data were analyzed and presented in  Table 3 . Table 3 , it can be seen that the mean rating scores of all the items are above 2.50. 1. Based on the results in  Table 1 , the findings from the study revealed that processing cassava into Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom, Mtuhem (Base), Akpu, Kweesi or Akweesa Akom and Kamu Akom are the Tiv management strategies for postharvest losses of cassava. Table 2 , findings of the study revealed that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). 3. Findings from the study based on the result in  Table 3  showed that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. Management of public libraries should also ensure that initiatives on going from one community to another to record and shot films on indigenous knowledge are in place.
paper_298	 Four research questions guided the study. Data generated was analyzed using simple percentage and descriptive statistics. The library as a service oriented organization must provide the bibliographic resources and services channeled towards the fulfillment of its parent institution's goals and objectives. The library ensures that the resources acquired are well organized to allow easy access by the library users  [1] . Library catalogue is considered as an interface of information retrieval system which assists information searchers to access resources of libraries using several access points. Library catalogue exist in different form. Like the card catalogues, sheaf catalogues, books, machine readable catalogues (MARCs) and online public access catalogues (OPACs). OPAC has brought a changing relationship between libraries and their users in the changing ICT environment. Library OPACs first emerged in the late 1970s and early 1980s and have gone through several cycles of change and development all geared towards improvement. In this environment, the user is both the primary searcher of the system and the user of the information resources  [2] . Overall majority of respondents 80% satisfied with OPAC functionality  [4] . Although, much awareness of the retrieval tools may be created in the libraries, it does not necessarily mean its accessibility, not to speak of its use. 91% respondents used the title search approach and 83.04% used the author search approach, User also indicated that the information regarding the problem faced by the respondents while using the OPAC like 74.39% faced by the problem lack of proper guidance about OPAC followed by 67.47% lack of awareness, 36.33% satisfied with the OPAC and its services  [7] . Consequently, AmkpaandIman emphasized that the success or failure of students to locate resources in the library depends on the skill acquired though the library instruction progamme  [8] . This is important because, in spite of the benefits which students can derive from catalogue use, its use is still poor in Nigerian university libraries. The author attributed the reason for the poor usage to lack of user education programme  [9] . While this is a welcome development, it is important to occasionally assess the effectiveness of the library catalogue especially from the users' point of view. For any academic library to serve the purpose to which it is created as agent of information provider in educational environment. It must contain different types of materials, very rich in nature, comprehensive in coverage with adequate bibliographical tools describing the location of each item, which is significant to the whole concept of library and librarianship. b. d. To identify the constraints associated with the use of library catalogue. The stratification sampling technique was employed to sample the entire registered population of undergraduate library users from each level (100-500) in Federal University of Kashere respectively. This section presents questionnaire collected on the basis of gender of the research respondents. The gender breakdown is presented on  Table 1 . The question on the demographic information of the respondents was on gender presented in  Table 1 . It shows that majority of the respondents 188 (69%) were male, followed by females 84 (31%). This shows that majority of library users in the Federal University of Kashere Library were male. Librarians in discharging their responsibilities should inform library users by communicating the availability of new technology and the way it operates to them. This finding is a good development for the library because it shows that the methods they used for creating awareness of library yielded good results. Their responses were presented in the  Table 4  below. Table 5  reveals that 106 (40%) got their awareness through library staff, followed by those that knew it through the user education programme (GST) 88 (16%). Never  Total  F  %  F  %  F  %  100  20  40  28  56  2  4  50  200  14  32  20  45  10  23  44  300  24  44  20  37  10  19  54  400  42  75  12  21  2  4  56  500  58  100  0  0  0  0  58  Total  158  60  80  31  24  9  262  Key: F=Frequency, %= Percentage. Total  F  %  F  %  F  %  F  %  F  %  F  %  100  10  20  0  0  6  12  28  56  6  12  0  0  50  200  10  23  6  14  10  23  16  36  2  5  0  0  44  300  10  19  6  11  14  26  12  22  6  11  6  11  54  400  14  25  0  0  36  64  6  11  0  0  0  0  56  500  2  3  0  0  54  93  2  3  0  0  0  0  58  Total  46  18  12  5  120  46  64  24  14  5  6  2  262  The finding indicated poor use of OPAC 12 (5%) at the university library. Lack of skills could discourage users from using the catalogue. The results are presented in  Table 8 . Table 8  focused on the challenges associated with catalogue use for information retrieval. The difficult interface of OPAC poses a big challenge to undergraduates who are the target users of OPAC in any academic library. Therefore, proper orientation and awareness campaign should be done to address this ugly situation. Because most challenges associated with the use of the OPAC often have to do with lack of ICT skills on how to make use of it.
paper_305	 The implementation was based on the java netbeans development platform to create an interface that was used to train a model and its subsequent use in predicting credit decisions. The results obtained proved that such a mechanism can be applied to augment manual credit appraising processes, especially where large volumes of applications are to be processed within limited timeframes. The performance of loan contracts in good standing guarantees profitability and stability of a bank. Generally, bank loan officers rely on traditional methods to guide them in evaluating the worthiness of loan applications. A checklist of bank rules, conventional statistical methods and personal judgment are used to evaluate loan applications. Furthermore, a loan officer's credit decision or recommendation for loan worthiness is subjective. Given the absence of objectivity, such judgment is biased, ambiguous and nonlinear and humans have limited capabilities to discover useful relationships or patterns from a large volume of historical data. Generally, loan application evaluations are based on a loan officers' subjective assessment. Therefore, a knowledge discovery tool is needed to assist in decision making regarding the application. Further, the complexity of loan decision tools and variation between applications is an opportunity for the use of a machine learning tool to provide learning capability that does not exist in other technologies. These traditional methods often require a great deal of subjective input from underwriters, making them un-reliable and often lack empirical and scientific backing. The study has left such analysis to oversight procedures especially where the confidence level of the classifier does not meet a certain threshold. The ongoing changes in the banking industry, in the form of new credit regulations, the need for innovative marketing strategies, the ever increasing competition and the constant changes in customer borrowing patterns; call for frequent adjustments to credit management in order to remain competitive. Automated techniques have progressively become popular in contemporary loan appraisal processes. This is a classification problem and can easily be implemented using a classification algorithm; the output of which is Boolean or multi-valued. One such algorithm that effectively addresses these issues is the LogitBoost Meta classifier -based on the log of the odds ratio for the dependent variable  [7, 8] . The boosting method was developed around the Probably Approximately Correct (PAC) model that entails transforming 'weak learners' into 'strong learners'. It works as follows: 1. Looks at all possible thresholds for each attribute 2. Resulting classifier is a simple threshold on a single feature a) Outputs a +1 if the attribute is above a certain threshold b) Outputs a -1 if the attribute is below the threshold In this study, 'majority voting' was adopted for combining hypothesis from different learners. It can be proven (as discussed here-under), that under the assumption that all individual classifiers have the same prediction rate and that the distribution of the data correctly classified by each base classifier is independent and random, this is the best possible strategy. In our study, the variable k was bi-valued and took on either 'accept' or 'reject' values and K was set at 2. Logistic regression models these probabilities using linear functions in x while at the same time ensuring they sum to one and remain in [0,1]. Select the single best classifier at this stage iv. Learn all K classifiers again, select the best, combine, viii. The model was built using the training dataset and tested using three strategies. iii. The two files can be generated by portioning a given data set into two and saving them separately. This gives an accuracy of 19/20=95%  Class =Accept: The number of correctly classified instances is 12 and that of instances classified as belong to the class is 13. This gives a precision value of 12/13=0.92 Class =Reject: The number of correctly classified instances is 7 and that of instances classified as belong to the class is 7. This gives a precision value of 7/7=1 Class =Accept: The number of correctly classified instances is 12 and the number of instances belonging to the class is 12. Test Split ROC graph ROC was developed during the World War II to statistically model false positives and false negatives of radar detections. P(true | false) calculated as 1-d/c+d The ROC area has the following indicators: . Indicates a perfect prediction . Excellent prediction . Good prediction . 0.7. Mediocre prediction . 0.6. Poor prediction . 0.5. Random prediction . <0.5. The value converted to 1 decimal place, these values indicate a perfect classification After a successful implementation of the stated system, the following were the key outcomes:  Three options were investigated for training the algorithm namely: a) The use of single file both for training and testing the model through stratified cross validation. Further, the system can be improved by creating a web-based interface or porting it to a distributed architecture platform. Finally, as stated earlier in the introduction, it is not prudent to completely rely on an automated credit appraising as some cases might require subjective interpretation and personal judgment. This is a good basis for manually investigating such cases whose levels of confidence go below a certain threshold.
paper_310	 We are now living in the 21 st century. Android application based smart phones are becoming each time more powerful and equipped with several accessories that are useful for Robots. Here we use mainly Arduino UNO (ATMEGA 328P), Bluetooth module (HC-05). This project is more necessary to the modern society in context of spying and surveillance. In achieving the task the controller is loaded with a program written using Embedded 'C' language. Still there exists a requirement of a cost-effective automation system, which will be easy to implement. The design of the system is kept as simple as possible. Few things like cost-effectiveness and simplicity in design, lowprofile structure etc. have been kept in mind before designing the project. b) Develop a robot which will be helpful for travelling. The system consists of following parts: a) Arduino UNO (ATMEGA 328P) b) Bluetooth module (HC-05) c) Smart phone d) Motor driver (l293D) e) Arduino software (version 1.8.1) The basic building blocks of the project have been described below: Microcontroller will act as the brain of the robot. The microcontroller is programmed with the help of the Embedded C programming. Arduino has it own programming burnt in its Read Only Memory (ROM). Cprogram is very easy to implement for programming the Arduino UNO. Generally our transmitter will be smart-phone and receiver will be Bluetooth module (  Figure  2 ). The smart phone is the transmitter of this circuit. The novelty lies in the simplicity of the design and functioning. It is also interfaced with the microcontroller  (Figure 4 (a) ) and with circuit connections  (Figure 4 (b) ). Here we use programming language 'C' for coding. The program for executing this project has been written in C language. When signal data arrives the Arduino the pin which corresponds to the particular input is set to high. There are two steps of the programming. First set up section where we define all the variables. Second loop part where the program runs continuously. The working principle is kept as simple as possible. The working principle of the circuit has been elaborated with the help of a block diagram, of the system interconnection as shown in  Figure 6 . As seen from the  Figure 6 . The circuit diagram of this project is shown below: Here at first we construct the circuit as shown in  Figure 7 . This is indeed a cost-effective and efficient project. The novelty lies in the fact that it is a cost-effective project with a simple and easy to use interface compared to existing ones.
paper_333	 This study presented a general process model to classify a given Enset leaf image as normal or infected. Food security is a challenge in many developing countries like Ethiopia. Nations in our country are still struggling to make use of available resources so as to combat hunger. Around 80% to 85% of people in Ethiopia are dependent on agriculture; among these more than 20% of them depend on Enset crop production in the country. It is also the sector that is given an overriding focus in the government's plan for growth of the economy as a whole. This is commonly known as false banana, the Ethiopian banana or the Abyssinian banana. Cheesman. The remaining part of this paper is organized as follows. In Section III, we present the architecture of proposed system. Experimental results are reported in Section IV. Section V presents the conclusion and discusses of future works. So far a number of fungal, nematode, viral and bacterial diseases were reported to cause damage at different degrees of intensity that was mainly explored by Quimio research finding since 1992. Therefore, the damage inflicted by each disease also varied. Among various diseases, Enset bacterial wilt and Fusarium Wilt is considered as the most dangers one that reduces Enset yield  [6, 7, 8] . In this work, disease identification was done by using characteristics of Enset plant. As it is mentioned in the above section, for the experiment two diseases and one normal of Enset were identified. From all those disease category a total of 460 Enset leaf images are collected from which 368 is used for training and 92 images are used for testing. B. Different results were found by using different multiclass support vector machine classifiers. We designed a system which can detect Enset disease automatically and this will help the farmers to detect the diseases in its early stage and to take relevant action. If there is a shortage of experts in the area of those disease identification the system is going to be a replacement for the experts. Better results is achieved if the system is used by large number of dataset.
paper_389	 Word probability, the paper explores the probability characteristic of word location. Experiments on the corpus show that the introduction of the word position probability feature has improved the accuracy, recall and the value of Fl. Zhou J built a hybrid method of Chinese word segmentation around CRF model. For example, "O" can be used to represent individual Chinese characters, "B" means Chinese characters appear in the head, "I" means Chinese characters appear in the middle or the end of the word. Therefore, Chinese word segmentation method has become a frequently used method to study word segmentation. Enter the sentence as "This is Wuhan." We first set three candidate marks "O", "B" and "I" for each Chinese character and add a start node "BOS" to the head, An end node "EOS". The feature that appears on each node is then calculated, using the feature weights to compute the most probable of all paths from "BOS" to "EOS". According to the meaning of "O", "B" and "I", the rules are summarized as follows: 1, the sentence of the first Chinese character tag can not be I, the last character of the mark can not be B. Because the mark B must be followed by a mark and only the mark I, and must be marked in front of the mark I, and marked as B or I. 2, marking O cannot appear behind the mark I, can only be O or B. 3, Mark B can only be followed by the mark I. Finally, according to the mark of each Chinese character, the mark result is "OOBIO", so the result is "这 / 是 / 武汉 /." The probability of the i-character of the position as the prefix: pY> 95% 昨狭抨第竞耽 3 Z The probability of the i-character as the suffix: pZ> 95% 丸役袄侣丸瞰 4 R The probability of the i-character individual word of the position: 85% <= pR <= 95% 吧枚刘磊躺却 5 U Position of the i-character as a prefix of the probability: formula_0 The probability of the i-character as the suffix: 85% <= pV<= 95% 貌型胁帘午岸 7 D The probability of the i-character position of the individual word: pD <= 5% 言辽改信申仪 8 E The probability of the i-character of the position as the prefix: pE <= 5% 业络王场姆杨 9 F The probability of the i-character as the suffix: pF<= 5% 增隐晋香浓伊 In order to deal with the long-distance information, this paper takes the context distance as 2. The probabilities of the position probabilities in  Table 3  are extracted from the training corpus and the probability of each position is calculated according to the following probability formula: P (individual word) = number of occurrences of the individual idiom of the word / total number of occurrences of the word * 100% (1) P (prefix) = number of occurrences of the word as a prefix / total number of occurrences of the word * 100% (2) P (suffix) = number of occurrences of the word as a suffix / total number of occurrences of the word * 100%  3 After many experiments and comparisons, the probability of more than 85% or probability of less than 5% of the word as a location probability feature, but also on the selected word for some filtering, the elimination of some unnecessary words, such as At the same time, select the probability of greater than or equal to 85% of the word is subdivided into the probability of 85% <p <95% and p> 95% of the two sets. Appropriate increase of p> 95% of the number of occurrences is in order to improve the characteristics of the sample expectations, and achieved good word effect. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. The standard corpus form used here dictates that each line in the corpus contains only one word, and the information associated with the word is followed by a tab stop followed by the word. The corpus is from 1950 to 2005, and the scale is 2564168000 sentences. The coding method is GB code. The corpus content mainly comes from newspaper news. The formula is as follows: Correct rate P = number of words correctly recognized / total number of system output words * 100% Recall rate IP correctly identify the number of words / test words in the total number of words * 100% F value F=*P*R/(P+R)*100％ The speed of word segmentation is another important index of word segmentation performance. The results of "+ feature template 3" model are obviously better than that of "+ feature template 2" model, that is, under the condition of adding feature template 3, F-score is 4.4% higher than that of feature template 3, Played a better effect.
paper_391	 Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The data for this study were the Tetanus Toxoid data set from the Ethiopian Demographic and Health Survey (EDHS) 2011, and analyzed using the Knowledge discovery process of Selection, Processing, Transforming, mining, and interpretation. The WEKA 3.6.1 tool was used for classification, clustering, association and attribute selection. Single data instance test using Naïve Bayesian was done by creating test 1, test 2, test 3, and test 4 data test instance, three of them are correctly predicted but one of them incorrectly classified. But, in the class attribute, it is 0.72. And the major cause of early infant deaths in many developing countries is often due to failure to observe hygienic procedures during delivery. Tetanus is caused by a toxin produced during the anaerobic growth of Clostridium tetani. Infection is acquired through environmental exposure of any broken skin or dead tissue such as a wound or when the umbilical cord is cut to the spores of the bacteria. Immunizing the mother prior to childbirth with TT protects both her and her newborn against tetanus and antenatal care is the main programmatic entry point for routine TT immunization. Five doses of TT can ensure protection throughout the reproductive years and even longer. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The information is rich and massive. In this study, we have used several data mining techniques; classification, clustering, association and outlier detection techniques over the nominated Tetanus Toxoid immunization dataset of the EDHS 11. This problem caused the loss of time and effort in the healthcare system environment and spending lots of efforts and costs without having evidence-based information for planning and intervention. The standards used are a percentage of accuracy and error rate of every classification techniques used. Selection phase generates the target data set from the whole data set of EDHS 2011. Those patterns that remain represent the discovered knowledge. The EDHS 2011 was conducted by the Central Statistical Agency (CSA) in collaboration with Measure DHS and ICF International under the auspices of the Ethiopian Ministry of Health (EMoH). For this particular study, the dataset was requested and accessed from DHS website https://dhsprogram.com after formal online registration and submission of project title and detail project description. It has a dimension of 7033 rows and 12 columns. Only 80% of the overall data is used for training and the rest 20% was used for testing the accuracy of the classification of the selected classification methods. csv" file formats and stored as an ". arff" file format. How does this classification work? Data classification has two steps; Firstly, consisting of a learning step that is, where a classification model is constructed. The approaches are; (a). Decision tree (J48) approach It is a flowchart-like a tree structure. Where each branch represents an outcome of the test, each internal node denotes a test on an attribute, and each leaf node holds a class label. And the topmost node in a tree is the root node. This approach uses divide and conquers algorithm to split a root node into a subset of two partitions till leaf node that occurs in a tree. The training tuples are described by m attributes. Each of the tuples represents a point in an m-dimensional space. (c). (d). Multilayer preceptor Multilayer preceptor is a simple two-layer neural network classifier with no hidden layers. Understanding them will make it easy to grasp the meaning of the various measures. Training and testing are performed k times. "How does the k-means ( ) algorithm work?" For each of the rest objects, an object is assigned to the cluster to which it is the most similar, based on the Euclidean distance between the object and the cluster mean. The k-means algorithm then iteratively boost the within-cluster variation. The InfoGainAttributeEval with Ranker T used with respect to the class attribute and visualization was done in the attribute selection. As explained in the    The literacy status of the mother has high information gain (0.046) and followed by the literacy status of the mother (0.041) and the least information gained was head of household (0.00000147). (Figure 3 ) In this study, the data mining tool and algorithm like (J48, k-nearest, and Bayes) is used for selecting the training and test data, for classification, and K-means approach for clustering, single-dimension association rule to identify the best association. Our work extends to utilize the implementation of the dataset for data mining tool present in all sections to achieve a better rate of accuracy and improve the efficiency when analyzing the large dataset.
paper_402	 This paper presents machine learning algorithms based on back-propagation neural network (BPNN) that employs sequential feature selection (SFS) for predicting the compressive strength of Ultra-High Performance Concrete (UHPC). Specifically, ANN has been used to solve a wide variety of civil engineering problems  [2] [3] [4] . Mainly, ANN was utilized to model the nonlinear behaviour of fatigue and creep of Reinforced Concrete (RC) members  [5] [6] [7] [8] . Recently, research interest has revolved around the development of ANN models to interpret the behaviour of structural materials such as steel, concrete, and composites  [9] [10] [11] [12] [13] [14] . The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete  [14] [15] [16] [17] . Afterwards, several authors began developing ANN models for the prediction of compressive strength of high performance concrete  [18] [19] [20] [21] . [24] . The objective of the study was to develop an ANN and SMD model to predict both the compressive strength and the consistency of UHPC with two different types of curing systems (steam curing and wet curing). Despite the statistical advantages of ANN, it has been long regarded as a black box that evaluates functions using input covariates and yielding outputs. In addition prediction of compressive strength of high strength and high performance concrete was addressed by other researchers  [20, 21] . This technique makes use of ANN's complex computation and allows the SFS tool to select and remove the influential and redundant parameters, respectively. There are two types of SFS classes -mainly filter method and wrapper method  [28] , where Zhou et al. Four sets of open human motion data and two types of machine learning algorithms were used. The total number of features was reduced rapidly, where this reduction helped the algorithm demonstrate better recognition accuracy than traditional methods. Four types of machine learning algorithms were used as wrappers for the SFS. Table 1  presents the initial input variables together with their range (maximum and minimum values) and symbols for identifying them in this experimental program. There are two types of ANN models: (1) feed forward; and (2) feed backward. The input neurons are responsible for containing the independent parameter presented by the user, the wires represent the randomly generated matrices called weights that manipulate the function's slope or steepness, the hidden neurons map the weights variables using an activation function, and the bias units control the output function's shift, either upward or downward. Equation  1 shows the linear combination of mapping weights from each input neuron, via wires, to the hidden neurons. SFS reduces the dimensionality of data by selecting only a subset of measured features to create a prediction model. There are two types of search algorithms: sequential forward selection and sequential backward selection. The increment started from one neuron and ended with 15 neurons, where the model was analyzed 10 times, for each increment, because the Levenberg-Marquardt algorithm locates the local, and not the global, minimum of a function. The SFS algorithm was run 200 times to capture all possible combinations of independent features when using ANN. Table 3  shows the statistical measurements calculated for both cases. It is observed from  Figure 6  that there is noticeable increase in the compressive strength of UHPC with the increase in Flay Ash and more noticeable with the increase in Silica Fume. This study was conducted to detect the correlation between the material constituents of UHPC and its compressive strength.
paper_418	 The decomposition models are Additive model: formula_0 Multiplicative model: formula_1 Mixed model: formula_2 where t M is the trend-cycle component, t S is the seasonal component and t e is the error. For equation  (1)  it is assumed that the error term t e is the Gaussian white noise ( )  For equation  (2)  it is equally assumed that the error term t e is the Gaussian white noise ( )  An additive model is based on the assumption that the sum of the components is equal to the unadjusted data. In particular, this means that the fluctuations overlapping the trend-cycle are not dependent on the series level. They do not depend on the level of the trend  [3] . The multiplicative model was adopted when the magnitude of the seasonal pattern in the data depends on the magnitude of the series. In other words, the magnitude of the seasonal pattern increases as the data value increases and decreases as the data value decreases level of the trend. The additive model was adopted when the magnitude of the seasonal pattern does not change as the series goes up and down while the additive model was adopted when the magnitude of the seasonal pattern does not change as the series goes up and down  [5] . Iwueze and Akpanta  [6]  pointed out that an additive model is appropriate when the seasonal standard deviations show no appreciable increase or decrease relative to any increase or decrease in the seasonal means while a multiplicative model is usually appropriate when the seasonal standard deviations show appreciable increase/decrease relative to any increase or decrease in the seasonal means. Sometimes the seasonal component is a proportion of the underlying trend value. This means that trend may be, has no effect on the seasonal and cyclical components nor do seasonal swings have any impact on cyclical variations. This is an improvement over what is in existence. The method adopted in this study is the Buys-Ballot procedure in descriptive time series. 3. We assume that the length of periodic interval is s For additive model, using the expression in table 1, we obtain  formula_6 when there is no trend. That is when 0 b =  (Table 1)  For mixed model, we obtain using the expression in  Table  1  ( ) ( )  formula_8 when there is no trend. formula_10 When there is no trend ( 0 b = ) we obtain from  (Table 1)  . . Table 3  that when there is no trend i.e. The data of 120 observations has been arranged in a Buys-Ballot table as monthly data (s = 12) and for 10 years (m = 10). The actual and transformed series are given in figures 3.1 and 3.2.
paper_428	 The present article examines the influence of thermal radiation on two-dimensional incompressible magnetohydrodynamic (MHD) mixed convective heat transfer flow of Williamson fluid flowing past a porous wedge. The solutions of the resulting nonlinear system are obtained numerically using the fifth order numerical scheme the Runge-Kutta-Fehlberg method. In Non-Newtonian fluids, the most frequently encountered fluids are pseudoplastic fluids, and Navier-stokes equations alone are insufficient to describe the rheological properties of these fluids, therefore, to overcome this defect, several rheological model like Ellis model, Power law model, Carreaus model and Cross model are presented, but little attention has been compensated to the Williamson fluid model and estimated to explain the rheological properties of pseudoplastic fluids. In this model both maximum viscosity μ (viscosity as shear rate tends to infinity) and minimum viscosity μ (viscosity as shear rate tends to zero) are to be taken. Williamson analyzed the flow of pseudoplastic materials and presented model to described the behavior of pseudoplastic material and explain convenient importance of plastic flows, and also recognized that viscous flows are very varied from plastic flows  [1] . Hayat et al. Nadeem et al. developed a model for the transport of Williamson fluid in an annular region  [10] . Khan et al. Hayat et al. Azimi et al. Srinivasacharya et al. Merkin et al. Jackson et al. Fu et al. Jafari et al. Bég et al. Porous wedge is a very important characteristic in science and engineering that can be illustrated as a material having a minute opening and the opening is almost filled with fluid. E 0 = E 6 , E′ 0 = 0, F 0 = 1 (9) E′ ∞ = 1, F ∞ = 0 Where E 6 is injection/suction parameter. The system of ordinary differential equations 7 and 8 subject to the boundary conditions 9 and 10 is first reduced to a system of first order ordinary differential equations using the substitutions E K =`, $ K = a, F K = b. This With the boundary conditions E 0 E 6 , $ 0 0, F 0 1, $ ∞ 1, F ∞ 0 (13) The resulting system in Eq. (11) (12) (13)  is solved numerically with the help of 5 th order Runge-Kutta-Fehlberg method. The transformed governing equations  (11) (12)  subjected to boundary conditions  (13)  are solved numerically by employing the fifth order Runge-Kutta-Fehlberg method. The influence of all pertinent parameters on flow and heat transfer are graphed and discussed in  Figures (1-8) . 1. Figures (1-2)  illustrate the influence of wedge angle parameter < with on velocity and temperature profile. Figure 6  drafts the non-dimensional velocity E′ for different values of suction parameter E 6 . Figures 7-8  illustrate the behavior of thermal radiation and Prandtl number on fluid flow region with M= E 6 = O = < =1. The important conclusions of the analysis are 1.
paper_432	 The mathematical analysis method used. We are motivated in part by the role of such action a groups of a symplectics of Hamiltonian systems with cotangent bundles phase spaces. We assume that is 1-periodic in time and grows a symptotically quadratically on each fiber. Generically, the corresponding Hamiltonian system `(%) =& H '%, % ). 1, for any 0 ∈ , the tangent space at 0, denoted , is the space of linear derivations on 1 2, , that vanish on 3 2, , . Observe that if = 06 ∘ 8, as  (,)  , and ( 1 2, (,) / 3 2, (,) ) ** described above, DC corresponds to the linear map in T 5 * (M) . Since  formula_3  formula_7 which is the change of variables formula for integrals. For the mapping.∅: & → | to have a lift ‹ satisfying the condition ( ‚ ) =J ‚ , where ‚ ∈ &,J ‚ ∈ H, 0 (J ‚ ) =d 0 =( ‚ ), it is necessary that∅ (π n (&, ‚ )) ⊆0 (a n (H,J ‚ )) (3. For cOEOE . 1)  Figure 2 . From which the required inclusions follow easily. 8 ' : K V → R s> ( K ), satisfying R o 8 ' u 0 ' and the equivariance property. 8 ' ( K , n, ' ) = 8 ' ( K, n ). ' , for K ∈ , n, ' ∈ . (n, ) = (n`n, ). these are easily seen to be free and proper. [n, ] = [n,`n, ] . Now consider a G on a manifold. In this part we study the symplectic geometry of cotangent -lifted action induced by a smooth proper action of a Lie group on a smooth manifold. Symplectic manifolds have their origin in the geometric for Hamilton ' s and Lagrange's equations of classical mechanics, where symmetries is the main tool that can be used to simplify the equations of motion. It is applications has been limited by the fact the proof is no constructive. ± • for some n ∈ .
paper_444	 Traditional resource constrained project scheduling problem research takes into account achieve management goal in certain environment. However, there are many uncertainties in practical projects due to the uncertain factors, which leads to the change of resource availability. The other constraint is the resource constraint in which the demand of resource shall not exceed the total supply of resource for each resource type at any time. However, the majority of the above studies focus on RCPSP in deterministic environment, and suppose that the resource availability is a real number. In real-word projects, there may be some uncertainty phenomena, for example, overdue materials, the decrease in the number of workers at Grain in Ear season, etc, which result in the resource availability may be changed. Ying  [8]  proposed a schedule model of flexible work-hour constraint, in which the human resource was dealt with a new constraint to the classical RCPSP and the increased quantities of human resource were real-value variables. Its goal was minimizing the expected weighted sum of the absolute deviations between the planned and the actually realized activity starting times. Its constrains were the resource and priority rules. When the indeterminacy does not behave neither randomness nor fuzziness, a new tool is required to deal with it. Uncertainty theory based on uncertain measure founded by Liu  [12] , and its a branch of axiomatic mathematics for modeling human uncertainty. (4) No interruption is allowed for each activity in progress. " For some time-intensive and heavy-duty projects, managers tend to be completed as quickly as possible and cost to be minimized. Managers usually increase resource availability to improve construction efficiency and shorten construction time, but it often makes the projects with high costs. As to a pair of activity and 2, activity 2 start after its predecessor activity is finished. [16]  Let G be an uncertain variable with regular uncertainty distribution H. If the excepted value exists, then +IGJ = K Φ MA (<) A = <. (2) For instance, let G be a linear uncertain variable, it has inverse uncertainty distribution H MA = (1 − <)N + <O. If the function Q(R A , R P , ⋯ , R ) ) is strictly increase with respect to R A , R P , ⋯ , R S and strictly decreaseing with respect to R STA , R STP , ⋯ , R ) , then formula_2 holds if and only if formula_3 To solve the model (1), according to the operational law of uncertain variables, we transformed it into its equivalent form, as shown below. Model (1) is equivalent to the following model. is formula_6 By Theorem 1, we know that the excepted value of ? is formula_7 Since ℳ6∑ ∈8 9 ≤ + ; ≥ < = , then, ℳ6∑ ∈8 9 − − ≤ 0; ≥ < = . By Theorem 2, we have ∑ ∈8 9 − − Φ MA (1 − <) ≤ 0，  (12)  i.e. formula_10 Therefore, the model (1) is equivalent to the model (2). In this section, we give a project example which is the decoration of engineering in the bonded areas data center to illustrate the reasonability of the model. The constrains are recourse constraint and precedence constraint. The information of the activities. Cost  Preceding activity  1  0  0  0  0  2  1  5  1200  0  3  5  24  28800  2  4  27  16  103680 3  5  30  24  172800 3  6  22  24  126720 4  7  19  9  41040  4  8  24  6  34560  3  9  12  4  11520  8  10  22  20  105600 7  11  7  8  13440  6  12  20  8  38400  7  13  24  17  97920  5  14  12  6  17280  4  15  16  4  15360  7  16  11  6  15840  6  17  10  8  19200  14  18  8  6  11520  2  19  24  11  63360  18  20  13  4  12480  6  21  9  4  8640  12/15  22  3  4  2880  21  23  4  4  3840  7  24  0  0  0  22  With the above demand, we can present the following model:  In the real-life project, due to the influence of uncertain environment, managers should consider the trade-off between the completion time and cost. In future research, We can also focus on more types of project scheduling problems based on uncertainty theory.
paper_462	 Since 1998, the postgraduate education for master's degree in clinical medicine has been piloted in China. Chongqing Medical University was founded in 1956. It was founded by the former Shanghai First Medical College (now Shanghai Medical College of Fudan University) and moved to Chongqing. Since 1979, it has enrolled doctoral and master's degree students. In 1981, it was approved as the first authorized unit of doctoral and master's degree in China. In 1998, it became the first batch of doctoral and master's degree in clinical medicine in China. Pilot units for professional degrees. Since 1998, the school has vigorously developed professional degree postgraduate education  [1] . Since 2009, the school has been focusing on reforming the training mode of clinical master degree postgraduates (hereinafter referred to as clinical master). In view of the main problems existing in clinical master education, the school has carried out a series of reforms and explorations on the organic link between clinical master education and standardized resident training (hereinafter referred to as standard training). To solve these problems, the Academic Degree Committee of the State Council officially launched the pilot work of clinical medicine professional degree in 1998. At present, China's postgraduate education has entered a critical period of structural adjustment. To vigorously develop professional degree postgraduate education and improve the quality of training high-level applied talents is the focus of the current degree and postgraduate education work. In order to effectively improve the quality of clinical master training and strengthen the management of clinical rotation process, the school has set up postgraduate management offices in clinical colleges, implemented the system of professional degree tutorial group, and established three-level management systems of schools (graduate schools), departments (graduate management offices) and clinical departments (tutorial groups)  [6] . To ensure the quality of clinical master's training, the school has set up a guidance group composed of the first tutor and rotating responsibility tutor. The tutor who applied for the postgraduate examination is the first tutor (defense tutor). Each clinical rotating Department of postgraduate clinical rotation appoints a teacher qualified as a professional master's tutor as rotating responsibility tutor to ensure that clinical master's is in clinical rotation. Effective management during the transition period. Requirements for the first stage of training. Students who have obtained the certificate of resident standardization training are directly transferred to specialist training (i.e. the second stage of standardization training), which not only saves the time for graduate students to carry out clinical rotation, but also ensures the quality of training, and achieves seamless docking in the rotation cycle. The main reason for this phenomenon is that since the trial implementation of clinical medical professional degree, the state has not evaluated the quality of the pilot units and lacked an effective monitoring mechanism. There are mainly "pre-job training", "centralized departure assessment", "stage assessment" and "annual assessment"  [13] . The Interim Measures for the Implementation of the Regulations on Academic Degrees of the People's Republic of China promulgated in 1981 stipulates that before the postgraduate's reply, the degree-granting unit shall employ one or two experts from the disciplines concerned with the thesis to review the thesis. If the reviewers are hired, some experts will evaluate the papers according to the requirements of scientific degree papers. In order to pass the papers evaluation smoothly, postgraduates will increase the difficulty of scientific research, which directly affects the students' clinical rotation. At the same time, the cost of training clinical masters has increased substantially. It stipulates that clinical master can apply for a degree only by publishing a review or case analysis. Fourth, direct link the workload of tutors' guidance to professional degree postgraduates with the promotion of their professional titles, so as to improve the enthusiasm of tutors' guidance to professional degree postgraduates. The new "5+3" training mode built by the school covers all aspects of training, such as curriculum system, assessment system, award system, award system, rotation system, tutor system and management mode. It fully guarantees the quality of clinical master training and brings up a large number of high-level applied medical talents for Chongqing and even the whole central and Western regions, which has produced remarkable economic and social effects. Benefit. The school has continuously innovated the training mechanism and formed a "modular" curriculum system and a quantitative assessment system for clinical competence. The school has reformed the single tutor system and explored the establishment of clinical master tutor group system. The main results are as follows: The reform breaks through the restrictions of relevant industry policies on the training objectives and modes of clinical master, and provides innovative modes for brothers to learn from. It has been highly valued by the Ministry of Education and the Health Planning Commission, and has the realistic conditions for its promotion and implementation throughout the country. The employment rate of graduates has been guaranteed to be 100% for a long time. The article "Construction and Practice of Quality Assurance System for Master of Clinical Medicine Degree Postgraduates" published in "Degree and Graduate Education" won the second prize for Excellent Thesis of "Degree and Graduate Education", and it is also the only prize-winning thesis in the field of medical science. There are 9694 open beds in each affiliated teaching hospital, 57 resident standardized training bases, covering all the specialty categories of clinical medicine. At present, five affiliated hospitals of our university have been appraised as model planning training bases of the State Health Planning Commission (8 hospitals in Chongqing). Professor Jin Xianqing and Professor Xie Peng were appointed members of the National Medical Graduate Education Steering Committee in 1998 and 2010 respectively. Professor Xie Peng, Professor Huang Ailong and Professor Wang Zhibiao were appointed experts of the Discipline Review Group of the Academic Degree Committee of the State Council. The "four certificates association" should be implemented, namely "licensed physician qualification certificate", "standard training qualification certificate", "diploma certificate" and "degree certificate"  [17] . Through a series of reforms, it has achieved relatively ideal results and accumulated rich experience in reform. It hopes that through continuous exploration and reform, it can play a role in attracting more brothers.
paper_476	 Under drastic competition, major express companies have increased their daily delivery frequency to improve customer satisfaction and market share. In addition, it is important for the logistics enterprises to consider the constraints such as delivery resources and consumer satisfaction to achieve the appropriate decision of delivery frequency. at various stages in the delivery system, which will more likely to result in uneconomical performance. Therefore, it is necessary to analyze the quantitative relationship among the above-mentioned factors to ensure that the express company can achieve high delivery economy while realizing multi-frequency and fast delivery to improve the customer satisfaction. Fan Xuemei et al. Jesus et al. It proposed an assessment framework for joint delivery. Some other researches tried to improve the delivery efficiency and reduce the delivery cost through delivery center location optimization  [6] [7] [8] , delivery vehicle route optimization and scheduling  [9] [10] [11] [12] , and delivery resource integration  [13] [14] [15] . The system dynamics has a good applicability in analyzing the delivery efficiency, and some researches have achieved a series of results. Hongtao Yang and Jianbang Du  [19] [20] [21]  uses system dynamics to analyze the problems of supply chain and delivery efficiency from the perspectives of transportation routes, drivers and cooperation agreements, and its sensitivity analysis provides some reference to this paper for the corporate in this case to improve the delivery efficiency. Lin Wanting [22]  considers the phenomenon of unfilled orders, information leakage, and product damage which exists in crowdsourcing delivery, then built system dynamics model from five dimensions of reliability, responsiveness, assurance, empathy and security, which provide decision-making advice for enterprise delivery efficiency management. First, it analyzes the boundary and causality of its delivery system. It provides reference for express companies to determine the delivery frequency. As the first step, this paper defines the research boundary of JDL delivery system. Secondly, it constructs subsystems of cost and resource operation efficiency for the delivery activities in the boundary. Therefore, the storage and ferry crossings are negligible in this paper., Only the section within the dotted line in  Figure 1  is considered in this paper. The delivery cost mainly includes the equipment usage costs and labor costs in the three stages of sorting, transportation and terminal delivery. The equipment usage costs can be divided into two parts: the fixed costs and the variable costs. The fixed costs occurred in the use of equipment in the three links. The variable costs occurred due to the volume of orders, which include operating cost of the sorting equipment, rental cost of the site, units fixed cost of transport vehicle, fuel costs, toll and so on. Labor costs are composed of fixed wages and performance wages for employees in the three links mentioned above. The number of employees is affected by factors such as order quantity, transportation efficiency, delivery frequency, and the number of transport vehicles. Performance wage is determined by the actual amount of work performed during the delivery process. Sorting staffs consist of sorters and on-site logistics personnel. The sorter is the person who operates on the sorting equipment. The on-site logistics personnel is the person who puts the sorted goods into a mail packages and places them on a pallet and then sends it to the transportation vehicles. The number of sorting staffs is influenced by factors such as the amount of cargo, the number of sorting equipment, the worker's efficiency, the sorting time requirements, the area of sorting venues, the number of logistics personnel in the yard, the delivery frequency and so on. The personnel load rate in the process is calculated by dividing the actual workload by the authorized workload. At the same time, the factors such as sorting time requirements, proportion of per-order batch, efficiency of the delivery personnel, and unit fuel consumption of the vehicle, will have a direct impact on the delivery frequency. Based on the surveys of JDL and interviews with related professionals, this paper summarizes 55 influencing factors on delivery frequency. These factors present a complex, nonlinear, and inverse relationship between each other. Therefore, this paper uses the causal loop method of system dynamics to analyze the relationship between the factors. Among them, the positive loop represents that there is a mutually reinforcing relationship between the factors, while the negative loop indicates that there is a balanced relationship between factors (Such as: Delivery frequency --> + Consumer demand response ability --> + Impact of demand response capacity --> + Consumer satisfaction --> + Order quantity due to change in satisfaction --> + Total daily order quantity --> + Per batch delivery order quantity-->+ Sorting center cargo volume-->+ Number of on-site logistics personnel--> -Onsite logistics personnel operating time-->+On-site logistics personnel costs-->+ Sorting personnel costs-->+ Sorting costs-->+ Total cost --> Total profit --> + Delivery facility input --> + Delivery frequency). This model passed the mechanical error checking and dimension consistency testing by VENSIM. On the other hand, the cost index was calculated based on the summed number of shipments as JDL adopts single-batch delivery. Second  Third  Fourth   1  60  30  40  20  2  30  30  20  20  3  10  30  30  20  4  -10  10  30  5  ---10  Table 4 . When the volume of orders increased from 1496 to 5300, the increased percentage of total delivery cost showed a downward trend followed by an upward trend. Regarding on the sorting efficiency equipment, the operation time of sorting equipment became longer and the sorting cost increased. The increased percentage behaved a negative growth trend followed by a positive growth trend as the order increased. The main reason is that the increase in the delivery frequency improved the demand for the number of delivery personnel and caused the delivery time of delivery personnel to be shorter in a single batch. As a result, the overall efficiency of the delivery staff was reduced. The reason is that the proportions of the sorting orders taken by the three sorting centers were different. This is because that the ratio between the order quantity and the actual number of delivery personnel at the delivery site was not equal. In the same delivery link the demand for delivery resources can be different due to different delivery operational capacities. Therefore, JDL should combine the simulation results with the actual situation before making the delivery frequency decision.
paper_479	 A forensic document examiner is saddled with the task of document authenticity. The L R theory takes its stance on odds-form from Bayes ' theorem. Let us presume that there are two opposing ideas. One proposition is 'The suspect is the author of the document in question'. A counter argument may be' The defendant is not the author of the document in question'. The propositions involved should be relevant and the latter case does not seem to be applicable. The oddity of the methodology depends on producing reproduced composing samples from an accumulation of composing samples acquired from a realized source to frame a database for evaluating the dissemination related with the numerator of a L R . While the utilization of probability proportions in the previous circumstance is currently rather entrenched spotlight on the insightful setting still remains rather past contemplations by and by. This method is genuinely direct for the score-based L R numerator, which involves creating a list of scores obtained by combining proof objects from the same source. Back Propagation Neural Network (BPNN) served the sake of supervised learning of the system. The rationale behind the choice of this algorithm is due to the fact that it is a supervised network and a supervised network will have a target, so the BPNN is a network that has a target. We target is set for each character in the handwriting. The target will help to know which handwriting is original and disguised. The neural network is a parametrized system because the weights are variable parameters. The weighted number of the inputs is the activation of the neuron. The activation signal is passed through the transfer feature to produce a single neuron output. Transfer function can add non-linearity to the network. Each character variable has a weight W i which shows its contribution in the training process. The feature information extracted from student handwriting through LBP was entered in NN through input layer and the participation of each class of character variables is calculated by the hidden layer of the network using: formula_1 net j is ith type of factor, n is the number of factors in net i , & ' ! weight-recognition factor with weight ! " . The weight from input points i and two hidden unit j is ! " and ! Weight from second hidden unit i and output unit j is ! ) . Weight of additional edge for each unit is bias− θ, where input unit and output vector from the hidden layer are expanded with a 1 component as seen in  Figure 2 . Step 2: Backpropagation to the output layer This research looked for the first partial derivatives 12 1 ! ) If L R value greater than 1 H p is true If L R value less than 1 H p is false. Table 1  shows the results of our Estimated Likelihood Ratio (ELR) for a writer against any other author in a collection of appropriate databases where the upper confidence interval (CiU) and the lower confidence interval (CiL) are indicated.
paper_492	 Based on known functions of neuroscience the neural network that performs serial parallel conversion and its inverse transformation is presented. This function is the behavior of so-called short-term memory. Furthermore retroactively, although there is a quantitative difference in each part between brain of ape who does not speak the language and our brain, but our brain is consisted of same material. In Chapter 2, the circuits by combining the basic functions of neural circuits for serial parallel conversion and inverse of the conversion are shown. Although for learning process Hebb rules is used on the circuits, operations such as back propagation and Markov process are not used. The nervous system related to short-term memory is activated in synchronization with environmental events, but the nervous system involved in long-term memory is highly layered to form an image corresponding to past and future events. Acceptance and generation of time series data of the nervous system of long-term memory is carried out in consistent with short-term memory. The same is true for the recognition process. Deductive logical development is desired. The divided subsequence is defined the basic subsequence. The neural circuit corresponding to each basic subsequence can be easily configured in the neural network, leading to the realization of processing by hardware of general time series data. The dividing is done by the following procedure. (1) The first element is the beginning of the first subsequence. When the following conditions are true, the element allocate to the top of new subsequence. (2) If the same element exists in already divided subsequence. In this example a6 is the concerned element. (3) If the maximum length of subsequence is defined, a new subsequence is allocated after the subsequence that reaches the maximum length, add new element to the concerned new subsequence. The subsequences divided by above procedure are defined as the basic subsequence. Figure 2  shows the affinity with the neural circuit. For the next data c 1 additionally activates the elements which has been activated by the first data c 0 . Because the elements activated by c 1 is randomly connected to input, not all elements activated by c 0 are additionally activated. By receiving c 2 , c 3 and so on the activated portions become narrow and narrow. The output of the element holding the activity when receiving the last c 4 of the time series data is the recognition result of the time series data c 0 c 1 c 2 c 3 c 4 . Its output is the result of the serial parallel conversion, it is the result of AND logic of the output of the activated elements. that can be said a conversion of parallel to serial triggered by the first data. On the flow direction of data, this parallel to serial conversion is upside down with the serial to parallel conversion described above, but the basis of neural network operation is the same. When the state transition diagram of the  Figure 3  is seen as a serial parallel conversion, the bottom is inputs connected to such sensory organs. On the other hand, when the state transition diagram of the  Figure 3  is seen as a parallel serial conversion, another waiting state is needed. The essence of the neural network which performs the parallel serial conversion and the neural network which performs the serial parallel conversion is the same except that the conversion result comes out downward or upward. On other words, both operations of the serial parallel conversion and the reverse conversion are essentially the same on the point both operations are triggered by the first data and proceed waiting for input state change. For each basic subsequence a neural network that accepts and generates the basic subsequence is considered. The movement will be mentioned in the next chapter. The nervous system which is involved in the imitating function is called mirror neurons. If there is a problem in the episodic memory, it causes difficulties in social activities. Figure 5  is an illustration for showing the state change of each part in the neural network. The upper part shows the part related to episodic memory, and the lower part shows the part related to the short-term memory. Stimulus from the sensory organs from the bottom becomes time series data and is transmitted to the upper part. The part shown in red is a part that is particularly activated, the lower red disk is the part activated by the visual data of the chocolate plate that placed in front, the upper red disk is the part that is activating by recalled episode about the chocolate plate. Next, we introduce a function called functor between different categories. The object and morphism that make up the source category are connected by a function to the object and morphism of the target category. The process of migration can be described mathematically using free functor or free construction functor. It has been vaguely thought that the nervous system responsible for long-term memory may be in a different place from the nervous system responsible for short-term memory. Consistency is required when the objects in which both memories are involved are the same. The process of taking the consistency between the two memories was explained using the idea of a free functor in category theory. From among the random connections, the necessary connections for the desired operation are selected and enhanced, and the target function is realized. This process is close to the rehabilitation process of the brain that has had a stroke.
paper_507	 Landslide is defined as a slow to rapid downward movement of instable rock and debris masses under the action of gravity. Landslides are one of the major natural hazards that account for hundreds of lives besides enormous damage to properties and blocking the communication links every year. They are the catastrophic phenomenon taken lives of many a hundred and destroyed the hard earned money, disrupting the communication facilities. As per the official figures of United Nations International strategy of disaster reduction (UN/ISDR) and Centre for Research on Epidemiology of Disasters CRED for the year 2006, landslide ranked 3 rd in terms of number of death among the top ten natural disasters  [2] . As far as Indian scenario is concerned approximately 0.49 million km  2  or 15% of land area of country is vulnerable to landslide hazard and 80% is spread over Himalayas, Nilgiris, Ranchi plateau and eastern ghats  (GSI 2006)    [3] . The traditional practice of Landslide prevention is enabling people with Landslide Hazard Zonation Maps. 2. 3. Uttarkashi falls under the physiographic division (s) Rohilkhand plains, Nepal Himalayas, Ganga, Yamuna daob, Siwalik range, Kumaun Himalaya, Dhaoladhar range. slope, aspect, lithology, rainfall, land cover etc. DEM (Digital elevation model) was obtained from BHUVAN. The ANN is a black box model is a multi-layered neural network, which consists of an input layer, hidden layers, and an output layer. Each node is a simple processing element that responds to the weighted inputs it receives from other nodes. The arrangement of the nodes is refer-red to as the network architecture (  figure 18 ). Figure 18 . Architecture of neural network (source:  (Lee, 2009) ). The dataset is categorized into 60% training and 40% validation. The data is categorized into training, testing and validation, and all the six factors namely soil depth, soil type, rock and a excel database is created. The dataset is categorized into 60% training and 40% validation. In these points we categorized them into two categories landslide prone and non-landslide prone. Since ANN does not understand the 'landslide prone' and 'non-landslide prone' region we explain it by giving value '1' and '0' respectively. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer (20), and between the hidden layer (20) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). For a new dataset the weights are unknown. Most of the training datasets met the 0.01 RMSE goal. slope, aspect, lithology, rainfall, land cover etc. Slope 2. Soil depth 3. Soil texture 4. Height 5. The dataset is categorized into 60% training and 40% validation. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer  (15) , and between the hidden layer (15) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). The regression performance was 2.03, the accuracy for the training data was 0.99409, for testing and validation are 0.41565 and 0.18369. The most contributing factor is slope carrying 93% and the least one is soil depth. The reliability of ANN is high over other methods.
